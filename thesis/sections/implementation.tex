{

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\section{Implementation}
\subsection{Dataset loading pipeline}

As mentioned in section 2.3, the dataset is split into 18 training and 20 test images. Each full scene is stored as a 16-bit unsigned integer .TIF file with four channels: RGB and NIR. \todo{ maybe add spectral range in nm} These images have a resolution of around 8000x8000 pixels and a size of approximately 130 MB. To reduce memory load and optimize training, validation and testing performance, the dataset is preprocessed by cropping each scene into smaller patches of 384x384 pixels.

Unlike typical RGB images used in computer vision, the channels are not combined. Instead, each channel is stored in its respective directory for both training and testing subsets. For visualization purposes, the full scene images are rendered using a standard false-color composite in which the NIR band is mapped to red, red to green, and green to blue. This mapping coresponds to the commonly used Color Infrared (CIR) visualization in remote sensing. CIR imagery enhances features such as vegetation and clouds, which exhibit distinct reflectance patterns across the visible and NIR spectrum.

However, these false-color visualizations were not used as input for the CNN during training, validation, or inference. They served solely as visual references for qualitative insprection by the human observer.

\todo{check patches and scenes to verify what bands were replaced}
\todo{provide 1 scene false color image and rgb nir gt patches as example images}

The following folder structure of the dataset was used:

\todo{provide folder structure}

Additionally .csv files containing lists of patch filenames and corresponding scene IDs were provided and leveraged for efficient dataset construction using TensorFlow data pipeline utilites.

One notable detail is that, due to the cropping and padding of border patches (in order to achieve standard patch size 384x384) and the tilted geometry of Landsat 8 imagery, some resulting patches contain no meaningful information – appearing completelly black across all four channels. These empty patches were excluded from training and validation to avoid introducing noise or misleading the model.

\subsubsection{Train and validation subsets}

To build a training and validation pipeline, the function BuildDS was implemented along with several supporting helper functions, all included in the load.py file. Later on, the function was extended to optionally load the test dataset as well – this will be discussed separately in !!!Test subset!!!

For efficient data handling and memory management, built-in TensorFlow utilities (referred to as tf in the following text and code) were used. Specifically, tf.Data.TextLineDataset() was used to read the .csv file line by line – each line representing the filename of a patch – which served as the foundation for the dataset pipeline.

The complete training subset originally contains 8400 patches. However, as noted earlier in !!!chapter!!!, only 5155 of them actually contain valid data and are therefore used for training and validation. The dataset is shuffled and then split into training and validation subsets. The ration between the two can be configured as needed.

Using the .map methon, each text line is first expanded into five full paths: the corresponding red, green, blue, NIR and ground truth patch filenames. Each of these file paths is then replaced by ist actual image content, loaded as tensor. This transformation is implemented in the helper function loadDS, which itself calls another utility, loadTIF. At this stage, each dataset element is a tuple of TensorFlow tensors representing the input image and its corresponding ground truth.

The loadDS together with loadTIF function are loading each .TIF image and performing the necessary conversions:

\begin{itemize}
    \item RGB and NIR patches, originally stored as 16-bit unsigned integers (range 0 - 65535), are cast to float32 and normalized to the range 0-1
    \item Ground truth masks, which originally contain values of either 0 or 255 (as uint8), are binarizes to values of 0 and 1, and also cast to float32
\end{itemize}

An additional feature of the loadDS function allows optional resizing (downsampling) of the input images, if a target image size is provided. Importantly, the image loading and transormation pipeline ensures no information loss up to the point of resizing, where a reduction in resolution is intentional and controlled.

As final preparation steps, the dataset is: shuffled, batched, prefetched, and set to repeat indefinitely (cycled).

\todo{handle all references and formattings}
\todo{maybe add code snippets}

\subsubsection{Test subset}

The buildDS funciton additionally provides functionality to load and construct the test dataset used for model evaluation. There are two available modes for buildung the test pipeline:

\begin{itemize}
    \item Using all 9201 patches cropped from the 20 test scenes, or
    \item Selecting patches from a single scene --- either by specifying its sceneID or allowing the function to randomly select one --- based on pre-generated .csv files that map patch names to sceneIDs.
\end{itemize}

After loading the filenames, the pipeline follows a similar flow to the training and validation datasets. However, ground truth masks are not available for individual test patches. Instead, each test dataset element is represented as a 384x384x4 (or predefined image size) float32 TensorFlow tensor containing the RGB and NIR channels.

Unlike training and validation datasets, the test dataset is not shuffled. This is necessary to preserve the spatial order of patches for later reconstruction. The dataset is batched and prefetched for efficient processing.

Since per-patch ground truth annotations are not provided, evaluation requires stitching the patches back into their original scene layout. This is done using the stitchPatches function, which reconstructs the entire scene by aligning the patches along their original positions. The function supports stitching either a single selected scene or all scenes in the test set.

\subsection{Model architecture implementation}

To facilitate flexible experimentation with different model architectures, the core building blocks and utility functions are organized in model.py. The CNN models are constructed using TensorFlow’s built-in APIs, allowing for modular and reusable design. To simplify architecture changes and streamline the process of constructing and loading different models, common patterns --- such as convolutional blocks --- are implemented as helper functions.

During development, it was necessary to test multiple model architectures and sizes. Therefore, the implementation supports fast and compatible switching between different models.

Additionally, where standard TensorFlow loss functions are insufficient, custom loss functions such as Soft Jaccard Loss are implemented withing this module. This also includes custom metrics, for example, the Dice Coefficient, which are particularly relevant for evaluating segmentation tasks.

By centralizing model definition and auxiliary functionality in model.py, the workflow supports rapid iteration and clear separation between architecture design and the rest of the implementation pipeline.

\todo{Put in Concept and cross reference quantization aware training, model architectures, maybe checkpoints, lr, early stopping. Batch normalization.
Maybe this todo is relevant for the next immediate chapter}

\subsection{Converting the model}

In order to convert the model after training to the Edge TPU format and to perform PTQ with calibration, several helper functions are defined in convert.py. As mentioned in [], the Edge TPU only supports models with a batch size of one. Howerver, for effective training, it is necessary to use and experiment with different batch sizes. Therefore, as the first step in the conversion process, the asBatchOne function transfers the trained weights to a new model with the exact same architecture but with an input batch size of one.

Next, using TensorFlow's TFLiteConverter, the model is converted to the .tflite format, which is specifically desinged for compact deployment on mobile or edge devices. During this conversion, calibration <TODO explain in concept> is performed using a representative dataset, typically consisting of numerous patches from the training dataset. Furthermore, the inputs and outputs of the converted model are set to int8 for quantized inference.

As the final step, the model is compiled for Edge TPU inference using the provided edgetpu-compiler (link). After successful compilation, all ops (explain what it is) in the model are ideally converted into a single Edge TPU custon op ready for deployment. If the compiler cannot map a certain op to the Edge TPU, it will be mapped to the CPU instead, which can significantly increase inference time. The goal of this thesis is to meet all model requirements for Edge TPU [chapter with requirements] and ensure that all ops are mapped to it. The aspired final model conversion result is illustrated below.

\todo{TODO put netron.app with before and after compile. Describe it in text. Mention netron! Look out for other todos in text above!}

}