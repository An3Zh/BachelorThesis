{

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\chapter{Evaluation}
\label{chapter:evaluation}

A flexible pipeline enables convenient training, conversion, and deployment of diverse model configurations for the \gls{devboard}.
It supports fast switching between model architecture, training, and deployment settings --- for example, changing the number of input channels,
the spatial resolution of the input tensor, or the composition of the training/validation/testing subsets.
Tailoring these configurations to user needs requires only minor code changes.

During development, numerous model architectures were designed and trained, most serving as prototypes for debugging.
For instance, the \code{simple} model produced the first inference results (see \secshortref{sec:deployment}).
After the implementation phase, five models were selected for evaluation.
These were trained for an appropriate number of epochs, their inference time measured on the \gls{devboard}, and their metrics computed via the evaluation pipeline.
One of these models was the first one to yield qualitatively high results: a simplified, edge adapted \code{Cloud-Net} architecture \cite{CloudNet2019},
referred to here as \code{earlyCloudEdgeQ}.
The remaining four models share an improved, identical architecture \code{improvedCloudEdgeQ},
and differ only in (i) input channels (\gls{rgb}+\gls{nir} vs. \gls{rgb}) and (ii) input patch size (\(192\times192\) vs. \(384\times384\)).

Note that all five models follow the classical \code{U-Net} structure (see \secshortref{subsec:stateoftheart}), comprising an encoder,
bottleneck, and decoder, with skip connections between corresponding encoder and decoder blocks.

Detailed descriptions of both architectures follow:

\begin{itemize}[itemsep=0.5\baselineskip]
    \item \textbf{earlyCloudEdgeQ}: The model hosts three encoder blocks, a bottleneck block, and three decoder blocks.
    Each encoder block consists of two \code{Conv2D} layers and one \code{MaxPooling2D} layer,
    connected sequentially. The bottleneck consists of two sequential \code{Conv2D} layers.
    Each decoder block contains a \code{Conv2DTranspose} layer, a \code{Concatenate} layer, and two \code{Conv2D} layers,
    connected sequentially. The output layer is a \code{Conv2D} layer.
    The base number of filters is 32 and is doubled in each encoder block and halved in each decoder block,
    resulting in 256 filters at the bottleneck.
    \gls{bn} is applied after each \code{Conv2D} layer and before its activation; it is not applied in the output layer.
    All suitable \glspl{tfop} are \gls{qat}-annotated. \gls{bce} is used as the loss function and \code{Adam} \cite{adam} as the optimizer.

    \item \textbf{improvedCloudEdgeQ}: This model follows the previous architecture with the following refinements.
    Between the input and the first encoder block, a \code{Conv2D} layer with 16 filters is inserted to improve feature representation at the input stage,
    enabling the encoder to capture more consistent low-level patterns before deeper processing.
    The model comprises four encoder blocks, a bottleneck, and four decoder blocks.
    The bottleneck design is adapted from \code{Cloud-Net}, where a residual connection with a \(1\times1\) projection ensures feature-dimension alignment and efficient gradient flow.
    Dropout regularization is added to reduce overfitting and improve robustness at the network's deepest layer.
    To meet the \gls{edgetpu}'s limited on-chip static \gls{ram} capacity, the decoder blocks were simplified by removing one \code{Conv2D} layer from each block,
    except for the final decoder block before the output layer, which retains two \code{Conv2D} layers to preserve finer spatial detail.
    Additionally, using \gls{bce} combined with \(\,0.5 \cdot \text{DiceLoss}\,\) as the loss function balances pixel-wise accuracy with overlap-based region matching,
    leading to better segmentation performance than plain \gls{bce} \cite{bcedice1, bcedice2}.
\end{itemize}

\code{improvedCloudEdgeQ} can be summarized as an adjusted \code{Cloud-Net} model tailored to the constraints of embedded \gls{edgetpu} development.
Necessary simplifications were made.
Both \code{earlyCloudEdgeQ} and \code{improvedCloudEdgeQ} can be examined in more detail in \code{model.py} and compared with the original \code{Cloud-Net} model
architecture.\footnote{\url{https://github.com/SorourMo/Cloud-Net-A-semantic-segmentation-CNN-for-cloud-detection/blob/master/Cloud-Net/cloud_net_model.py}}

\autoref{tab:model_comparison} details the differences between the models, reporting the number of input channels (CH),
input-patch size (In size), number of trainable parameters in millions (Params (M)),
number of training epochs (Epochs, suffixed with \code{eS} if \code{EarlyStopping} was triggered),
estimated arithmetic operations in billions (Ops (B)), on-chip/off-chip memory used to cache model parameters (On-chip (MB) / Off-chip (KB)),
and the average per-batch inference time on the \gls{devboard} (Inf. Time (ms)).

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.3}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{CH} & \textbf{In size} & \textbf{Params (M)} & \textbf{Epochs} & \textbf{Ops (B)} & \textbf{On-chip (MB)} & \textbf{Off-chip (KB)} & \textbf{Inf. Time (ms)} \\ \hline
early      & 4 & 192 & 1.9 & 100  & 10.4 & 1.9 & 46  & 506 \\ \hline
improved~1 & 3 & 192 & 6.0 & 54 eS   & 11.6 & 5.7 & 342 & 514 \\ \hline
improved~2 & 3 & 384 & 6.0 & 46 eS   & 46.3 & 6.0 & 349 & 2435 \\ \hline
improved~3 & 4 & 192 & 6.0 & 51 eS   & 11.6 & 5.7 & 342 & 515 \\ \hline
improved~4 & 4 & 384 & 6.0 & 45 eS   & 46.4 & 6.0 & 349 & 2440 \\ \hline
\end{tabular}%
}
\caption{Model properties and deployment metrics.}
\label{tab:model_comparison}
\end{table}

Models were evaluated on a test set containing 20 scenes.
The evaluation metrics defined in \secshortref{subsec:evalmetrics}, IoU, Dice coefficient, Precision, Recall,
and Accuracy, were computed per scene and then averaged. This averaging does not bias the results because all scenes are approximately the same size.

For all models except \code{early}, the binarization threshold was calibrated immediately after training by selecting the \code{bestF1} on the validation set,
yielding an inference-ready model with its own threshold for mask binarization. For \code{early}, no threshold was computed because the feature had not yet been implemented.
Reproducible dataset shuffling was also not available at that time, precluding a comparable PRC evaluation on the same validation set.

In addition to the validation threshold, a per-scene threshold was computed for each of the 20 test scenes and then averaged.
This procedure is even more optimistic than using a single global test threshold, since it adapts to each scene and thus overestimates true generalization performance.
Nevertheless, reporting these results is useful:
they illustrate an upper bound under ideal threshold tuning and underscore the strong influence of threshold choice on segmentation quality.

\clearpage
The results are shown in \autoref{fig:evalresults} below.
The second and third panels are derived from the first by splitting results by validation threshold versus test threshold for clearer visual comparison.
\code{improvedCloudEdgeQ} models are labeled by configuration (number of input channels and input patch size), and \code{earlyCloudEdgeQ} is additionally marked as \code{early}.

\begin{figure}[H]
  \centering
  
  % First: full width
  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{files/evalRes.pdf}
  \end{subfigure}

  % Second row: two half-widths, no gap
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{files/valSetThr.pdf}
  \end{subfigure}%
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \includegraphics[width=\textwidth]{files/testSetThr.pdf}
  \end{subfigure}

  \caption{Evaluation Results}
  \label{fig:evalresults}
\end{figure}


By analyzing these results, the following key insights emerge:
\begin{itemize}
    \item A general upward performance trend is evident with increasing input patch resolution and with the addition of the \gls{nir} channel.
    This is expected, as the model benefits from additional spectral information.
    \item \code{improvedCloudEdgeQ} generally outperforms \code{earlyCloudEdgeQ}, consistent with its architectural refinements.
    \item At \ensuremath{192\times192} input size, adding \gls{nir} did not improve recall and in fact led to a slight reduction (when using the validation-derived threshold).
    A likely cause is that the extra spectral channel increases input dimensionality without providing sufficient spatial context at the lower resolution:
    the model becomes more conservative at cloud boundaries and thin structures, reducing false positives but slightly increasing missed cloud pixels (lower recall).
    \item Accuracy is consistently very high, but it should not be treated as a reliable metric for cloud segmentation.
    If non cloud pixels vastly outnumber cloud pixels,
    accuracy is dominated by true negatives, so even a model that misses many clouds can score highly: hence IoU, Dice, Precision, and Recall are preferred.
\end{itemize}

Considering the evaluation metrics together with inference time on the \gls{devboard},
the \code{improvedCloudEdgeQ} model with \gls{rgb}+\gls{nir} inputs and \ensuremath{192\times192} resolution is recommended as the most suitable choice in this work,
striking a balance between edge-device speed and segmentation quality.
However, if \gls{nir} channel is unavailable, the preferred model is \code{improvedCloudEdgeQ} at \ensuremath{384\times384},
trading a significantly longer inference time for stronger performance.

}