{

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\chapter{Evaluation}
\label{chapter:evaluation}

A flexible pipeline enables convenient training, conversion, and deployment of diverse model configurations for the \gls{devboard}.
It supports fast switching between model architecture, training, and deployment settings --- for example, changing the number of input channels,
the spatial resolution of the input tensor, or the composition of the training/validation/testing subsets.
Tailoring these configurations to user needs requires only minor code changes.

During development, numerous model architectures were designed and trained, most serving as prototypes for debugging.
For instance, the \code{simple} model produced the first inference results (see \secshortref{sec:deployment}).
After the implementation phase, five models were selected for evaluation.
These were trained for an appropriate number of epochs, their inference time measured on the \gls{devboard}, and their metrics computed via the evaluation pipeline.
One of these models was the first one to yield qualitatively high results: a simplified, edge adapted \code{Cloud-Net} architecture \cite{CloudNet2019},
referred to here as \code{earlyCloudEdgeQ}.
The remaining four models share an improved, identical architecture \code{improvedCloudEdgeQ},
and differ only in (i) input channels (\gls{rgb}+\gls{nir} vs. \gls{rgb}) and (ii) input patch size (\(192\times192\) vs. \(384\times384\)).

Note that all five models follow the classical \code{U-Net} structure (see \secshortref{subsec:stateoftheart}), comprising an encoder,
bottleneck, and decoder, with skip connections between corresponding encoder and decoder blocks.

Detailed descriptions of both architectures follow:

\begin{itemize}[itemsep=0.5\baselineskip]
    \item \textbf{earlyCloudEdgeQ}: The model hosts three encoder blocks, a bottleneck block, and three decoder blocks.
    Each encoder block consists of two \code{Conv2D} layers and one \code{MaxPooling2D} layer,
    connected sequentially. The bottleneck consists of two sequential \code{Conv2D} layers.
    Each decoder block contains a \code{Conv2DTranspose} layer, a \code{Concatenate} layer, and two \code{Conv2D} layers,
    connected sequentially. The output layer is a \code{Conv2D} layer.
    The base number of filters is 32 and is doubled in each encoder block and halved in each decoder block,
    resulting in 256 filters at the bottleneck.
    \gls{bn} is applied after each \code{Conv2D} layer and before its activation; it is not applied in the output layer.
    All suitable \glspl{tfop} are \gls{qat}-annotated. \gls{bce} is used as the loss function and \code{Adam} \cite{adam} as the optimizer.

    \item \textbf{improvedCloudEdgeQ}: This model follows the previous architecture with the following refinements.
    Between the input and the first encoder block, a \code{Conv2D} layer with 16 filters is inserted to improve feature representation at the input stage,
    enabling the encoder to capture more consistent low-level patterns before deeper processing.
    The model comprises four encoder blocks, a bottleneck, and four decoder blocks.
    The bottleneck design is adapted from \code{Cloud-Net}, where a residual connection with a \(1\times1\) projection ensures feature-dimension alignment and efficient gradient flow.
    Dropout regularization is added to reduce overfitting and improve robustness at the network's deepest layer.
    To meet the \gls{edgetpu}'s limited on-chip static \gls{ram} capacity, the decoder blocks were simplified by removing one \code{Conv2D} layer from each block,
    except for the final decoder block before the output layer, which retains two \code{Conv2D} layers to preserve finer spatial detail.
    Additionally, using \gls{bce} combined with \(\,0.5 \cdot \text{DiceLoss}\,\) as the loss function balances pixel-wise accuracy with overlap-based region matching,
    leading to better segmentation performance than plain \gls{bce} \cite{bcedice1, bcedice2}.
\end{itemize}

\code{improvedCloudEdgeQ} can be summarized as an adjusted \code{Cloud-Net} model tailored to the constraints of embedded \gls{edgetpu} development.
Necessary simplifications were made.
Both \code{earlyCloudEdgeQ} and \code{improvedCloudEdgeQ} can be examined in more detail in \code{model.py} and compared with the original \code{Cloud-Net} model
architecture.\footnote{\url{https://github.com/SorourMo/Cloud-Net-A-semantic-segmentation-CNN-for-cloud-detection/blob/master/Cloud-Net/cloud_net_model.py}}

The following table details the differences between the models, reporting the number of input channels (CH),
input-patch size (In size), number of trainable parameters in millions (Params (M)), number of training epochs (Epochs),
estimated arithmetic operations in billions (Ops (B)), on-chip/off-chip memory used to cache model parameters (On-chip (MB) / Off-chip (KB)),
and the per-batch inference time on the \gls{devboard} (Inf. Time (ms)).

\begin{table}[h!]
\centering
\label{tab:model_comparison}
\renewcommand{\arraystretch}{1.3}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{CH} & \textbf{In size} & \textbf{Params (M)} & \textbf{Epochs} & \textbf{Ops (B)} & \textbf{On-chip (MB)} & \textbf{Off-chip (KB)} & \textbf{Inf. Time (ms)} \\ \hline
early      & 4 & 192 & 1.9 & 100  & 10.4 & 1.9 & 46  & 15.2 \\ \hline
improved~1 & 3 & 192 & 6.0 & 54   & 11.6 & 5.7 & 342 & 20.5 \\ \hline
improved~2 & 3 & 384 & 6.0 & 46   & 46.3 & 6.0 & 349 & 12.7 \\ \hline
improved~3 & 4 & 192 & 6.0 & 51   & 11.6 & 5.7 & 342 & 10.3 \\ \hline
improved~4 & 4 & 384 & 6.0 & 45   & 46.4 & 6.0 & 349 & 25.8 \\ \hline
\end{tabular}%
}
\caption{Comparison of model properties and deployment metrics}
\end{table}

\todo{inference time coral and add learnable params!}

Models were evaluated on a test set, containing 20 scenes. The evaluation metrics, outlined in \secshortref{subsec:evalmetrics}
were calculated per scene and then averaged:
IoU, Dice Coefficient, Precision, Recall, Accuracy. Averaging does not skew the results, because all scene are of approximately the same size.
Furthermore, for each model except \code{early} the best binarization threshold was calculated immediately after the training precess,
utilizing the bestF1 score, obtained on a validation set.
This improves model to be inference-ready with its own calibrated best threshold for immediate binarization of predicted masks.
For \code{early} model no threshold was calculated, since to the point of its training, the feature was not yet implemented.
Features for the reproducibility of shuffling the datasets were not implemented at the time too,
leaving no chance to perform \code{PRC} evaluation on the same validation set to make up for the best validation threshold.
In addition to determining the optimal threshold from the validation set,
a per-scene threshold was computed individually for each of the 20 test scenes and subsequently averaged.
This procedure is even more optimistic than selecting a single global test threshold,
since it implicitly adapts to each test scene and therefore overestimates true generalization performance.
Nonetheless, presenting these results is useful, as it illustrates the upper bound of performance
achievable under ideal threshold tuning and underscores the strong influence of threshold choice on segmentation quality.
The final evaluation results are represented on three column chart below,
whereas second and third charts are derived from the first one, splitting between validation set threshold and test set threshold
for better visual comparison possibility.
\code{improvedCloudEdgeQ} models are represented only by their parameters configuration, such as number of input channels and input patch' size,
\code{earlyCloudEdgeQ} is highlighted additionally as \code{early} model. 

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{files/evalRes.pdf}
  \caption{The basic workflow to create a model for the Edge TPU}
  \label{fig:evalres}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=0.45\textheight,keepaspectratio]{files/valSetThr.pdf}
    \caption{Links}
  \end{subfigure}
  \vspace{0.8em}
  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=0.45\textheight,keepaspectratio]{files/testSetThr.pdf}
    \caption{Rechts}
  \end{subfigure}
  \caption{Zwei Diagramme Ã¼bereinander (als PDF).}
\end{figure}

By analizing these results, following key insights can be made:
\begin{itemize}
    \item A general upward performance trend can clearly be seen with increasing input patch resolution and with addition of \gls{nir} channel.
    This is expected, as the model benifits from additional input information sources.
    \item \code{improvedCloudEdgeQ} architecture performs generally better than \code{earlyCloudEdgeQ} given the architecture improvements.
    \item At input size \ensuremath{192\times192}, adding the \gls{nir} channel did not improve recall and in fact led to a slight reduction,
    when analizing the validation threshold. This effect likely arises because the additional spectral information increases input dimensionality without
    providing sufficient spatial context at the lower resolution. As a result, the model becomes more conservative at cloud boundaries and thin structures,
    which reduces false positives but slightly increases the number of missed cloud pixels, leading to lower recall.
    \item The accuracy metric is constanltly at a very high level, but it has better not to be qualified as a reliable metric in the
    cloud segmentation case, as explained in \secshortref{subsec:evalmetrics}. <TODO EXPLAIN THERE>
\end{itemize}

Based on the evaluation metrics and taking the inference time on \gls{devboard} into consideration the \code{improvedCloudEdgeQ} model,
with \gls{rgb} and \code{nir} input channels as well as \ensuremath{192\times192} spatial resolution, is proposed as the best suitable model
out of all trained and evaluated models in the scope of this work. This model represents the golden middle between edge device inference speed
and performance metrics.
However, if the input data lacks \code{nir} channel, providing only \code{rgb} imagery, the best model is chosen to be
\code{improvedCloudEdgeQ} of \ensuremath{384\times384}, trading significantly increased inference time for the more than acceptable performance.

}