{

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\chapter{Implementation}
\label{chapter:implementation}

A detailed description of the software development process used to achieve the thesis objective is provided in this chapter.
Along the way, three major implementation milestones were reached. Despite several hurdles, success was achieved in
(i) training the model on powerful cloud machines, (ii) mapping its all components, fully quantized, to the \gls{edgetpu},
and (iii) performing C++ inference on the \gls{devboard}.
The technical obstacles and their resolution are presented in three “Technical Challenges” chapters inside
\nameref{sec:training}, \nameref{sec:conversion}, and \nameref{sec:deployment}.

Software developed in the scope of this work
is made available for free and unrestricted use on the author's GitHub and TU Berlin GitLab profiles:
\begin{itemize}
  \item \url{https://github.com/An3Zh/BachelorThesis}
  \item GitLab link
\end{itemize}

Details on relevant file locations, with references to this work's chapters,
as well as further information on the repository structure, are provided in \code{README.md}.

\section{Data}
\label{sec:data}

\subsection{Training \& Validation}

In order to construct the training and validation pipeline, the function \code{BuildDS} was implemented, along with several supporting helper functions, all located in the \code{load.py} file.
The function was later extended to optionally include the test dataset, which will be discussed in \secshortref{subsec:testing}.

Efficient data handling and memory management are achieved through the use of built-in \gls{tf} utilities.
In particular, the \code{tf.Data.TextLineDataset}\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset}} function
is employed to sequentionally read the \code{.csv} files,
which contain the patch filenames as outlined in \secshortref{subsec:dataset}.
This provides the foundation for the dataset pipeline.

The complete training set originally contains 8400 patches. However, as explained earlier in \secshortref{subsec:dataset}, some patches are entirely zero-valued.
Only 5155 of them contain valid data and are thus retained for subsequent use.
The dataset is shuffled and split into training and validation subsets, with the ratio configurable as needed.

Using the \code{.map} method, each text line is first expanded into five full paths to the corresponding \gls{rgb}, \gls{nir} and \gls{gt} mask patches.
Each filepath is then replaced by its image content, loaded as \code{tf.Tensor}\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/Tensor}}.
This transformation is handled by the helper function \code{loadDS}, which itself calls utility function \code{loadTIF}.
At this stage, each dataset element is a tuple of \gls{tf} tensors representing the four-channel input image and its corresponding \gls{gt} mask.

The \code{loadDS} and \code{loadTIF} functions perform the following operations:

\begin{itemize}
    \item RGB and NIR patches are cast to \gls{float32} and normalized to the range [0,1].
    \item \gls{gt} masks, originally in \gls{uint8}, are binarized to values of 0 and 1, and also cast to \gls{float32}.
\end{itemize}

An additional feature of the \code{loadDS} function allows for optional resizing of the input images if a target size is provided.
The image loading and transormation pipeline is designed to avoid information loss until the controlled resizing step, where a reduction in resolution is intentional.
In the case of resizing, \gls{gt} masks are resized using nearest-neighbor interpolation. This method copies the value of the closest original pixel for each target pixel,
thus preserving hard edges in the cloud segmentation mask and avoiding the introduction of intermediate values.
Conversely, the \gls{rgb} and \gls{nir} inputs are resized with bilinear interpolation,
which computes each new pixel value as a weighted average of the nearest $2\times2$ neighborhood. This results in smooth pixel transitions while maintaining important image details.
Nearest-neighbor interpolation is among the eraliest digital image resizing techniques, dating back to the origins of digital image processing in the 1960s,
whereas bilinear interpolation gained prominence in early computer graphics literature and is now standard in deep learning pipelines \cite{bilinearNearest1, bilinearNearest2}.

As a final preparation step, the dataset is: shuffled, batched, set to repeat indefinitely,
and prefetched to optimize data retrieval during training.
Because the datasets are repeated indefinitely, it is essential to define the number of steps required to complete one full pass through the training and validation subsets.
These step counts are used to ensure the correct number of iterations per epoch during training,
and they are computed as follows: \code{trainSteps(valSteps)~=~trainSubsetSize(valSubsetSize)~/~batchSize}.

\subsection{Testing}
\label{subsec:testing}

An additional capability of the \code{buildDS} is the construction of the test dataset.
As outlined in \secshortref{subsec:dataset}, only cropped \gls{rgb} and \gls{nir} test patches, together with complete scene \gls{gt} masks, are available for testing.
This arrangement necessitates two specific preparation steps prior to building the test pipeline:

\clearpage
\begin{enumerate}
    \item \textbf{Metadata collection:} To uniquely identify each scene, the \code{sceneID} is introduced, derived from the Landsat 8 metadata (path/row),
    and is unique within this test dataset. The total number of patches, as well as the number of rows and columns for each scene, is collected for every \code{sceneID}.
    The patch filenames, as outlined in \secshortref{subsec:dataset}, are used for this process.
    Additional \code{.csv} files were manually created --- each containing the ordered patch filenames corresponding to every full scene.
    Filenames in these \code{.csv} files are organized in the order resulting from cropping (left to right, top to bottom).
    Furthermore, the \code{fullTestDS.csv} file was generated, containing ordered patch filenames for all 20 full scenes.
    These \code{.csv} files, not provided in the original dataset,
    were developed in the scope of this thesis to support the test pipeline and are stored in the \code{additionalCSVs} folder within the testing subset.
    Based on this manually collected metadata, the \code{getSceneGridSizes} function was implemented in \code{load.py} file,
    returning a dictionary that maps each of the 20 \code{sceneID}s to the respective number of rows and columns in each scene.
    \item \textbf{Patch stitching:} The \code{stitchPatches} function was implemented in \code{load.py}.
    This function uses the \code{.csv} files and the \code{getSceneGridSizes} function to reconstruct entire scenes from the model's output patches for evaluation.
    The function can operate in two modes: it can either stitch together all scenes at once, saving all 20 full scenes,
    or stitch only a single specified scene. The single-scene mode is intended for debugging and verification, avoiding time-consuming processing of the full test subset.
\end{enumerate}

Following these preparatory steps, the \code{buildDS} function was extended with additional functionality to construct the test subset.
Using the optional boolean parameter \code{includeTestDS}, which indicates whether to include the test dataset, and the parameter \code{singleSceneID},
which allows the selection of a specific scene, the function now supports multiple modes for testing:

\begin{itemize}
    \item Utilizing all 9201 patches from the 20 test scenes, or
    \item Selecting patches from a single scene, either by specifying its \code{singleSceneID} or allowing the function to randomly select one.
    %based on pre-generated \code{.csv} files that map patch names to \code{sceneID}.
\end{itemize}

The dataset is then constructed using the same \gls{tf} utilities employed for the training and validation subsets.
It is important to emphasize that the test set is not shuffled, as preserving the initial patch order is essential for the subsequent stitching process.

Consequently, the \code{buildDS} function returns the training and validation subsets, along with their respective step counts based on the \code{batchSize}.
Optionally, it also returns test subset, which may contain either the entire test dataset or a single scene.
After inference, the model's output patches can be passed to the \code{stitchPatches} function, which reconstructs and saves the final scene images for further evaluation.

\clearpage
\section{Model}
\label{sec:model}

The core building blocks and utility functions for architecting the model are organized in \code{model.py}.
\gls{cnn} layers are constructed using \gls{tf}'s built-in \glspl{api}, allowing for a modular and reusable design.

In early development, the \code{simple} model architecture was implemented.
It served as a proof of concept and was used for initial debugging of the training and conversion pipelines, as well as to obtain the first inference results on the \gls{devboard}.
The model was not annotated for \gls{qat}.

Below in \autoref{fig:quantnotquantcomp} are schown the examples of layer diagrams for two models: the left is not annotated for \gls{qat}, the right is annotated.
These visualizations were produced with Netron\footnote{\url{https://netron.app/}}.
Following the concept outlined in \secshortref{subsubsec:qat}, the \code{QuantizeWrapperV2} and \code{QuantizeLayer} \glspl{tfop} are inserted into the architecture,
wrapping the base layers with quantize-dequantize operations.

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPart.pdf}
    \caption{model part not annotated for \protect\gls{qat}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPartQ.pdf}
    \caption{model part annotated for \protect\gls{qat}}
  \end{subfigure}
  \caption{Model before and after \protect\gls{qat} annotation}
  \label{fig:quantnotquantcomp}
\end{figure}

It is important to emphasize that \gls{bn} is applied to the output of the \code{Conv2D} \gls{tfop} and precedes the corresponding \code{Activation} function \cite{batchnormActivation}.

Subsequently, more complex \code{U-Net} architectures were implemented, with greater depth and skip connections.
Early versions of these models were not \gls{qat} annotated, instead, \gls{ptq} was applied at conversion time.

Where standard \gls{tf} loss functions are insufficient, custom losses such as \code{softJaccardLoss} and \code{diceLoss} are implemented in this module.
Custom metrics, particularly those used for segmentation evaluation and referenced in \autoref{subsec:evalmetrics}, are also provided within the \code{model.py} file.

\section{Training}
\label{sec:training}

The complete model training and conversion pipeline is implemented in the \code{main.py} file.

The process begins with configuration settings, which include:
\begin{itemize}
\item Batch size for training
\item Image size for both training and inference
\item Number of training epochs
\item Selected model architecture
\item Ratio between validation and training subsets
\item Number of batches used for the calibration dataset
\end{itemize}
Additional settings can be incorporated as needed.

The pipeline starts by loading the dataset using functions from \code{load.py}.
The selected model architecture is then compiled utilizing utilities from \code{model.py}.

Each training run is stored in a dedicated folder, named with a timestamp corresponding to the start of the run.
This folder contains all configuration files, training-related artifacts, and final results, including the compiled \gls{edgetpu} model.

Prior to training, the following configurations and callbacks are set up:

\begin{itemize}
\item \textbf{Model Checkpoints}: Model checkpoints are saved during training whenever the validation loss improves, preserving the best-performing model.
\item \textbf{Early Stopping}: If the validation loss stops improving, training continues for a predefined number of additional epochs before termination.
\item \textbf{Learning Rate Reduction}: The learning rate is reduced if the validation loss does not improve for a set number of epochs, helping to fine-tune convergence.
\end{itemize}

Once all configurations are saved, model training is initiated with the specified callbacks active.

Upon completion of training, the final model weights are saved and immediately used for model conversion.

\subsection*{Technical Challenges}

Training of initial proof-of-concept models was performed on a personal workstation equipped with an Intel Core i5-13400F \gls{cpu} and an NVIDIA GeForce RTX 4060 \gls{gpu}.
To enable \gls{gpu} acceleration for the \gls{tf} training process, the NVIDIA \gls{cuda} \gls{api} and the corresponding \gls{cudnn} library must be installed and configured.
The following versions were used: \gls{cuda} v11.2 and \gls{cudnn} v8.1.1.
These versions are not the latest available from NVIDIA at the time of this work, but were selected for compatibility with certain \glspl{tfop} and the \code{libedgetpu} runtime library.
Further details on these compatibility considerations are discussed in \secshortref{sec:conversion} and \secshortref{sec:deployment}.

Initial models containing approximately 300,000 parameters were trained without \gls{qat} for tens of epochs.
On the described setup and using the full training dataset, the time required for a single epoch was less than 15 seconds, which was deemed acceptable.

With the introduction of more complex models containing 2,000,000 and 30,000,000 (experimental, proven to be inefficient at edge inference) parameters,
and when employing \gls{qat}, training times increased significantly.
For these larger models, one epoch on the full training dataset required approximately 3 minutes and 12 minutes, respectively.
Since at least 100 epochs were necessary to achieve adequate model performance, more powerful hardware resources became essential.

Thunder Compute\footnote{\url{https://www.thundercompute.com/}}, an American startup with \gls{gpu} cloud resources for machine learning and data science,
provided solutions for large-scale experiments.
By using an instance with an NVIDIA A100XL \gls{gpu}, training times for the 2,000,000 parameter network were reduced to approximately 20 seconds per epoch,
and for the 30,000,000 parameter network to 40 seconds per epoch.
A \ensuremath{10\times} increase in \gls{gpu} memory, to 80GB on the A100XL compared to 8GB on the RTX 4060, also enabled larger batch sizes,
resulting in more effective training and improved model robustness.

However, significant effort was required to configure the cloud instance for compatibility.
The more expensive production mode\footnote{\url{https://www.thundercompute.com/docs/production-mode}} had to be used,
as the prototyping mode\footnote{\url{https://www.thundercompute.com/docs/prototyping-mode}} did not allow downgrading \gls{cuda} and \gls{cudnn} versions.
The \gls{cuda} and \gls{cudnn} libraries needed to be downgraded from their most recent platform versions to those compatible with an older \gls{tf} version.
This was challenging, as it required purging the latest \gls{cuda} and \gls{cudnn} files without removing the \gls{gpu} drivers required by the older versions.
Ultimately, \gls{cuda} v11.2 and \gls{cudnn} v8.1.0 were installed on the instance for subsequent training.
In addition, the Thunder Compute development team was contacted with a suggestion to allow selection of \gls{cuda} and \gls{cudnn} versions at instance creation.
As a result, an improvement is planned that will allow passing the desired \gls{cuda} and \gls{cudnn} version as an environment variable during instance startup.

\section{Conversion}
\label{sec:conversion}

The file \code{convert.py} contains all necessary functions and utilities for model conversion and \gls{edgetpu} compilation.
These functions are called from \code{main.py} immediately after training is complete.

\subsection{Weight Transfer}

As outlined in \secshortref{subsec:hardware}, the \gls{edgetpu} supports input tensors with at most three dimensions.
This necessitates the use of batch size one, since the input tensor shape is already \ensuremath{image~height~\times~image~width~\times~number~of~channels}.
During training, larger batch sizes are typically used for efficiency.
Therefore, after training, the function \code{asBatchOne} is used to transfer the trained weights to a model with identical architecture,
but with batch size set to one for the input, all intermediate computations, and the output.

\subsection{Post-Training Quantization}

The \code{tf.lite.TFLiteConverter}\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter}} utility is used to perform \gls{ptq} and convert the model into
the \code{.tflite} format.
A representative dataset, consisting of training \gls{rgb} and \gls{nir} patches, is generated using the \code{representativeDatasetGen} function.
The number of calibration batches used can be adjusted via \code{numCalBatches}, as defined in \secshortref{sec:training}.
The greater the amount of calibration data, the more likely the observed value range will closely match the inference data, improving quantization accuracy.

As a next step, all supported \glspl{tfop} as well as model inputs and outputs are quantized.
While various data types can be used for quantization in general case,
this work utilizes \gls{int8}, as required for \gls{edgetpu} compatibility in \secshortref{subsec:hardware}.

The following subchapter provides a deeper investigation into quantization of model parameters by analyzing a \gls{qat}-annotated \gls{float32} model and its already quantized version.
The Netron tool is used here as well for visualizing the model's parts.
This chapter is aimed at readers who wish to deepen their understanding of parameter quantization and gain insight into the general techniques
behind full-integer operation execution on edge devices such as the \gls{edgetpu}.

It should be noted that both \gls{qat} annotation and \gls{ptq} are already wrapped into convenient, ready-to-use \gls{tflite} commands.
Thus, this section is not intended to teach the implementation of quantized models from scratch. Instead, it aims to offer a deeper understanding that can be beneficial for debugging, designing custom (potentially more powerful) models, or simply appreciating the engineering trade-offs between speed, memory, and accuracy on constrained devices.

\subsection{Practical example of Model quantization}
\label{subsubsec:optquant}

Below, a \gls{qat} annotated part of the model is shown in the left panel of \autoref{fig:qatconvertedquant}.
On the right, the same part is shown after \gls{ptq}, yielding a fully \gls{int8} quantized model.

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPartQBeforePTQ.pdf}
    \caption{model part annotated for \protect\gls{qat}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPartQAfterPTQ.pdf}
    \caption{model part after full \protect\gls{int8} quantization}
  \end{subfigure}
  \caption{Model before and after quantization}
  \label{fig:qatconvertedquant}
\end{figure}

Netron also provides detailed information about graph nodes (model layers) and their connections (tensor passes).
For example, layer weights and biases can be directly inspected, and values can be examined.
In the tables below, quantization-relevant information is displayed for further analysis.

For both the \gls{qat} annotated model and the resulting quantized model, the relevant information for four layers is summarized.
All floating-point numbers are shown in full \gls{float32} precision.
Representative calculations are performed in full precision and displayed with extended decimals.
The goal is to illustrate, using a practical example, the error effects of quantization and the limits of \gls{float32} precision.

\clearpage
\subsubsection*{\gls{qat} Annotated Model (left in \autoref{fig:qatconvertedquant})}

\begin{layerbox}{Layer Type: QuantizeLayer}{Layer Name: Quantize Input}
  \begin{center}
    \textbf{Input value range:} \\[2pt]
    $\min = 0 \quad\quad \max = 1$
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: QuantizeWrapperV2}{Layer Name: Conv2D No. 1}
  \begin{center}
    \textbf{Kernel shape:} \\[2pt]
    $[3, 3, 4, 32]$ \\[6pt]
    \textbf{Kernel No. 1 value range:} \\[2pt]
    $\min = -0.544256329536438 \quad\quad \max = 0.544256329536438$ \\[6pt]
    \textbf{First element of Kernel No. 1 value:} \\[2pt]
    $0.032761260867118835$ \\[6pt]
    \textbf{Bias shape:} \\[2pt]
    $[32]$ \\[6pt]
    \textbf{Bias No. 1 value:} \\[2pt]
    $0.1983194351196289$ \\[6pt]
    \textbf{Activation value range:} \\[2pt]
    $\min = -2.4379329681396484 \quad\quad \max = 1.0039010047912598$
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: \glsxtrlong{bn}}{Layer Name: \gls{bn} No. 1}
  \begin{center}
    \textbf{Shape of $\gamma$, $\beta$, $\mu_{\text{moving}}$, $\sigma^2_{\text{moving}}$ parameters:} \\[2pt]
    $[32]$ \\[6pt]
    \textbf{Parameter $\gamma_1$ value:} \\[2pt]
    1.110548973083496 \\[6pt]
    \textbf{Parameter $\beta_1$ value:} \\[2pt]
    -0.11593257635831833 \\[6pt]
    \textbf{Parameter $\mu_{\text{moving},1}$ value:} \\[2pt]
    0.17263321578502655 \\[6pt]
    \textbf{Parameter $\sigma^2_{\text{moving},1}$ value:} \\[2pt]
    0.00221889466047287
  \end{center}
\end{layerbox}


\begin{layerbox}{Layer Type: QuantizeWrapperV2}{Layer Name: Activation No. 1}
  \begin{center}
    \textbf{Output value range:} \\[2pt]
    $\min = -3.696099329375535 \mathrm{e}{-11} \quad\quad \max = 18.816810607910156$
  \end{center}
\end{layerbox}

\clearpage
\subsubsection*{Model after quantization (right in \autoref{fig:qatconvertedquant})}

\begin{layerbox}{Layer Type: Conv2D}{Layer Name: Conv2D No. 1}
  \begin{center}
    \textbf{Input affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.003921568859368563 \times (q_{\text{x}} + 128)$ \\[6pt]
    \textbf{Kernel No. 1 symmetric dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.004285482689738274 \times q_{\text{x}}$ \\[6pt]
    \textbf{First element of Kernel No. 1 quantized value:} \\[2pt]
    $8$ \\[6pt]
    \textbf{Bias No. 1 symmetric dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.000016805815903353505 \times q_{\text{x}}$ \\[6pt]
    \textbf{Bias No. 1 quantized value:} \\[2pt]
    $11801$ \\[6pt]
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: Mul}{Layer Name: \gls{bn} No. 1 Scale}
  \begin{center}
    \textbf{Input affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.013497387990355492 \times (q_{\text{x}} - 53)$ \\[6pt]
    \textbf{Fused \gls{bn} multiplier $M_c$ affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.10698594152927399 \times (q_{\text{x}} + 128)$ \\[6pt]
    \textbf{Multiplier $M_1$ quantized value:} \\[2pt]
    $55$ \\[6pt]
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: Add}{Layer Name: \gls{bn} No. 1 Shift}
  \begin{center}
    \textbf{Input affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.17354172468185425 \times (q_{\text{x}} - 26)$ \\[6pt]
    \textbf{Fused \gls{bn} additive term $A_c$ affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.0339626707136631 \times (q_{\text{x}} + 10)$ \\[6pt]
    \textbf{Additive term $A_1$ quantized value:} \\[2pt]
    $-113$ \\[6pt]
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: ReLU}{Layer Name: Activation No. 1}
  \begin{center}
    \textbf{Output affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.07379141449928284 \times (q_{\text{x}} + 128)$ \\[6pt]
  \end{center}
\end{layerbox}

\clearpage
Before presenting the calculations, the Quantization Specification Table\footnote{\url{https://ai.google.dev/edge/litert/models/quantization_spec}} is reviewed.
An excerpt relevant to the \gls{tfop} \code{Conv2D} is shown below:

\begin{verbatim}
CONV_2D
  Input 0:
    data_type  : int8
    range      : [-128, 127]
    granularity: per-tensor
  Input 1 (Weight):
    data_type  : int8
    range      : [-127, 127]
    granularity: per-axis (dim = 0)
    restriction: zero_point = 0
  Input 2 (Bias):
    data_type  : int32
    range      : [int32_min, int32_max]
    granularity: per-axis
    restriction: (scale, zero_point) = (input0_scale*input1_scale[...], 0)
  Output 0:
    data_type  : int8
    range      : [-128, 127]
    granularity: per-tensor
\end{verbatim}

Using this information, quantization calculations can be retraced for a better understanding of model transformation.
The \code{input\_1} values, or the model's input tensors, are observed first from the \code{QuantizeLayer}.
As expected, these range from 0 to 1 due to normalized input patches.

The input scale and zero-point can be calculated using equations \ref{eq:scale} and \ref{eq:zeropoint} from \secshortref{subsec:quantization}:

\[
\begin{array}{rl}
\text{Input scale} &= \dfrac{1 - 0}{127 - (-128)} = 0.003921568627450980 \\[16pt]
\text{Input zero-point} &= \text{round}\!\left(-128 - \dfrac{0}{\text{Input scale}}\right) = -128
\end{array}
\]

The calculated values align with the "Input affine dequantization formula" for the \code{Conv2D} layer in the quantized model.
The precision difference at the 10th decimal place is expected due to \gls{float32} limits.
This confirms the use of affine (asymmetric) quantization for \code{Input 0},
consistent with the table's range [-128,127] and the absence of a zero-point restriction.

Moving to the \code{Conv2D} layer weights, the table specifies per-axis granularity with symmetric quantization (zero-point=0).
Inspection of the 32 kernel value ranges (only Kernel No. 1 shown here) confirms symmetry around zero.
This suggests that \gls{tf} enforces clipping of weight extremes to fit symmetric ranges.

Netron reveals that Kernel No. 1 has the largest max value among the 32 kernels (not represented here),
yet it still doesn't match the actual tensor extrema ($min=-0.4122757017612457, max=0.5442604422569275$),
indicating that \gls{tf} may apply additional optimization, likely outlier-aware clipping or mean adjustment,
since real values are not perfectly zero-mean ($mean=-0.005126346834471305$).
A detailed investigation of this effect lies outside the scope of this work.

Using the clipped extrema, the symmetric quantization scale is:

\[
\begin{array}{rl}
\text{Kernel No. 1 scale} &= \dfrac{0.544256329536438 - (-0.544256329536438)}{127 - (-127)}\\[8pt]
                          &= 0.004285482909735732
\end{array}
\]

This matches the Kernel No. 1 symmetric dequantization formula from the quantized model (within \gls{float32} precision limits).
Applying equation \ref{eq:dequantize}, the quantized value can be approximately dequantized back to the first element of Kernel No. 1:

\[
\begin{array}{rl}
\text{First element of Kernel No. 1} \approx 0.004285482689738274 \cdot 8 = 0.034283861517906192
\end{array}
\]

The resulting precision loss is small and can be quantified by absolute, relative, and \gls{lsb} errors:

\begin{itemize}[itemsep=0.4\baselineskip]
    \item $e_{\text{abs}} = 0.034283861517906192 - 0.032761260867118835 = 0.001522600650787357$
    \item $e_{\text{rel}} = \frac{0.001522600650787357}{0.032761260867118835} \times 100\% \approx 4.647\%$
    \item $e_{\text{lsb}} = \frac{0.001522600650787357}{0.004285482689738274} \approx 0.355,\ \Delta = 0.004285482689738274$
\end{itemize}

Here, the \gls{lsb} error $<0.5$ is a good indicator: it suggests that scale boundaries are not exceeded and quantization was applied without mismatches,
clipping errors, or encoding/decoding inconsistencies.

Looking at the bias restrictions in the Quantization Specification Table, an interesting point emerges:
the bias has no dedicated quantization scale derived from its own extrema.
Hence, no min/max values are logged by \code{QuantizeWrapperV2} for the \code{Conv2D} layer.
Instead, its scale is computed as the product of the layer's input scale and the per-channel weight scale,
yielding one bias scale per output channel (matching the weight scales per axis).
This engineering choice accelerates inference at the cost of a slight precision loss.

Multiplying input values by weights (each with its own scale) produces a new effective scale.
To add the bias to this product, the bias must be quantized using the same scale (with zero-point=0).
One could quantize the bias with its own scale and then rescale the product accordingly,
but this introduces extra operations that would significantly slow inference.
Instead, fast fixed-point arithmetic is employed, using a precomputed quantized multiplier and a bit-shift,
to reconcile the mixed scales that arise when multiplying quantized values.

\[
\begin{array}{rl}
\text{Bias No. 1 scale} &= 0.003921568859368563 \cdot 0.004285482689738274 \\
             &= 0.0000168058154634406445 \\[8pt]
\text{Bias No. 1 value} &\approx 0.000016805815903353505 \cdot 11801 \\
      &= 0.198325433475474713
\end{array}
\]

Analogous calculations can be performed for the Bias No. 1 to determine its scale and recover its approximate \gls{float32} value.

As can be seen in the Netron analysis, the \gls{bn} \gls{tfop} is decomposed into two primitive \glspl{tfop} after quantization: \code{Mul} and \code{Add}.
This follows from the affine form of batch normalization (and its folding during quantization), which can be expressed as:

\begin{equation}
y = \gamma \cdot \frac{x - \mu_{\text{moving}}}{\sqrt{\sigma^2_{\text{moving}} + \epsilon}} + \beta
\label{eq:bn}
\end{equation}

\begin{equation}
M_c = \frac{\gamma_c}{\sqrt{\sigma^2_{\text{moving},c} + \epsilon}}, 
\quad
A_c = \beta_c - \frac{\gamma_c \cdot \mu_{\text{moving},c}}{\sqrt{\sigma^2_{\text{moving},c} + \epsilon}}
      = \beta_c - M_c \cdot \mu_{\text{moving},c}
\label{eq:bnquant}
\end{equation}

\begin{itemize}
    \item \textbf{$x$}: input value to be normalized.
    \item \textbf{$y$}: output value after batch normalization.
    \item \textbf{$\mu_{\text{moving},c}$}: moving mean for channel $c$, computed during training as an exponential moving average of per-batch means; used in inference because batch statistics from a single sample are often unreliable.
    \item \textbf{$\sigma^2_{\text{moving},c}$}: moving variance for channel $c$, computed during training as an exponential moving average of per-batch variances; also used in inference for the same reason as the moving mean.
    \item \textbf{$\epsilon$}: small constant to avoid division by zero (default\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization}} \gls{tf} value is set to 0.001).
    \item \textbf{$\gamma_c$}: learnable scale parameter for channel $c$.
    \item \textbf{$\beta_c$}: learnable shift parameter for channel $c$.
    \item \textbf{$M_c$}: per-channel multiplier used in quantized inference.
    \item \textbf{$A_c$}: per-channel additive term used in quantized inference.
\end{itemize}

It should be noted that the affine dequantization of the \code{Mul} input is computed from the extrema of the outputs of \code{Conv2D} No.~1,
analogous to the previous calculations of scales and zero-points.

In quantized form, $\mu_{\text{moving}}$, $\sigma^2_{\text{moving}}$, $\gamma$, and $\beta$ are fused into per-channel multiplier $M_c$ and additive term $A_c$.
Reverse-engineering from the quantized values recovers the original \gls{float32} parameters with minimal error:

\begin{equation*}
\begin{gathered}
M_1 = 0.10698594152927399 \cdot (55 + 128) \approx 19.5784273 \\
\sqrt{\sigma^2_{\text{moving},1} + \epsilon} = \sqrt{0.00221889466047287 + 0.001} \approx 0.0567353 \\
\hat{\gamma}_1 = M_1 \cdot \sqrt{\sigma^2_{\text{moving},1} + \epsilon} \approx 19.5784273 \times 0.0567353 \approx 1.1107880 \\
\hat{\gamma}_1 = 1.1107880 \approx \gamma_1 = 1.110548973083496
\end{gathered}
\end{equation*}


\begin{equation*}
\begin{gathered}
A_1 = 0.0339626707136631 \cdot (-113 + 10) \approx -3.4981550 \\
\beta_1 = A_1 + M_1 \cdot \mu_{\text{moving},1} \approx -3.4981550 + 19.5784273 \times 0.1726332 \approx -0.1182682 \\
\beta_1 = -0.1182682 \approx \beta_{1,\text{float}} = -0.11593257635831833
\end{gathered}
\end{equation*}

Finally, the "Output affine dequantization formula" is derived from the extrema of the (ReLU) Activation No. 1 layer:

\[
\begin{array}{rl}
\text{Output scale} &= \frac{18.816810607910156 - (-3.696099329375535 \mathrm{e}{-11})}{127 - (-128)} \\
             &= 0.073791414148812224 \\[8pt]
\text{Output zero-point} &= \text{round}\left( -128 - \frac{-3.696099329375535 \mathrm{e}{-11}}{\text{Output scale}} \right) \\
                  &= -128
\end{array}
\]


All subsequent quantization steps for \glspl{tfop} in the model follow the same general procedure, adhering to predefined constraints and reusing common underlying methods.

From this deeper investigation, the following key quantization principles emerge:
\begin{itemize}
    \item After \gls{ptq}, the \code{QuantizeInput} layer defines the input scale and input zero-point based on observed extrema.
    Same calculation principles apply further.
    \item Weights are typically quantized per-axis (per-channel) with symmetric ranges.
    \item Bias scales are derived from input and weight scales, enabling fixed-point computation without extra rescaling.
    \item Certain operations, such as \gls{bn}, are decomposed into simpler arithmetic operations for inference efficiency.
    \item The precision loss is small, and \gls{lsb} error below $0.5$ indicates well-matched scales.
\end{itemize}

After quantized model is saved, the \gls{edgetpu} Compiler is invoked.

\clearpage
\subsection{Edge TPU Compiling}

Since training may be performed either on a local Windows machine or on a Linux-based cloud \gls{gpu} instance from Thunder Compute,
cross-platform automation of the pipeline is ensured by using helper functions and platform detection before invoking the \gls{edgetpu} compiler.
Because the \gls{edgetpu} compiler can only run on Debian-based Linux systems, the Windows Subsystem for Linux (WSL) is used when working on a Windows machine.

The \gls{edgetpu} Compiler parses the \code{.tflite} model, verifies that the restrictions are satisfied and that it is compatible with the \gls{edgetpu} runtime,
and then attempts to fuse all supported \glspl{tfop} into a single \code{edgetpu-custom-op}.
An example of the compiler's console output during compilation is shown below.

\lstdefinestyle{compilerlog}{
  basicstyle=\normalsize\ttfamily, % can adjust \fontsize{12pt}{13.5pt}
  captionpos=b,                    % caption at the bottom
  frame=single,                     % box around the log
  breaklines=true,
  showstringspaces=false,
  columns=fullflexible
}

\begin{lstlisting}[style=compilerlog,
    caption={Edge TPU compiler console output. Successful compilation},
    label={lst:edgetpu_compilation}]
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.

Model compiled successfully in 898 ms.

Input model: dev/results/run_20250719_170647/quant.tflite
Input size: 1.90MiB
Output model: dev/results/run_20250719_170647/quant_edgetpu.tflite
Output size: 2.07MiB
On-chip memory used for caching model parameters: 1.90MiB
On-chip memory remaining for caching model parameters: 3.82MiB
Off-chip memory used for streaming uncached model parameters: 46.00KiB
Number of Edge TPU subgraphs: 1
Total number of operations: 53
Operation log: dev/results/run_20250719_170647/quant_edgetpu.log
See the operation log file for individual operation details.
Compilation child process completed within timeout period.
Compilation succeeded!
\end{lstlisting}

The \gls{edgetpu} has \(\sim 8\,\mathrm{MiB}\) of static \gls{ram}, allowing model parameters to be cached.
It is further specified\footnote{\url{https://coral.ai/docs/edgetpu/compiler/\#parameter-data-caching}} that multiple models can be co-compiled and
stored in the \gls{edgetpu}'s on-chip memory, enabling fast switching and inference when different models are required on a single \gls{edgetpu}.
Conversely, a model can be partitioned for pipelining across multiple \glspl{edgetpu}, allowing parallel inference or acceleration of larger models.
In the scope of this thesis, however, only a single \gls{edgetpu} is used.
Therefore, the on-chip static \gls{ram} should ideally not be exceeded to preserve inference speed.
Falling back to external memory can degrade performance due to parameter reloads.

From this log example, useful information about memory usage can be inferred.
The model executable occupies approximately \(8 - 1.90 - 3.82 \approx 2.28\,\mathrm{MiB}\) of the \gls{edgetpu} on-chip cache.
Its parameters occupy \(1.90\,\mathrm{MiB}\), leaving \(3.82\,\mathrm{MiB}\) free, thus, the entire model is stored in the \gls{edgetpu}'s static \gls{ram}.
In \secshortref{chapter:evaluation}, memory usage is also considered when comparing different models.
For this particular model, however, \(46.00\,\mathrm{KiB}\) of external memory was still utilized despite the remaining free on-chip memory.
The reason is not documented officially and likely requires deeper investigation of the compilation process, which lies outside the scope of this thesis.

After successful compilation, a \code{.log} file is saved alongside the \gls{edgetpu} model.
This log reports, for each \gls{tfop}, whether mapping to the \gls{edgetpu} succeeded (and, if not, why),
as illustrated by the \code{.log} excerpt shown right in the \autoref{fig:successcompile}.

One way to confirm that the entire model compiles for the \gls{edgetpu} is to visualize the compiled model in Netron.
An example demonstrating that every \gls{tfop} was successfully mapped to the \gls{edgetpu} is shown left in the \autoref{fig:successcompile}.
In contrast to the pre-compilation graphs (for example, see the \autoref{fig:qatconvertedquant}), no additional structural information is visible:
all supported \glspl{tfop} have been fused into a single \code{edgetpu-custom-op} that executes exclusively on the \gls{edgetpu}.
As shown, the input and output tensor shapes are readable, and the associated quantization scales and zero points can be inspected,
as in \secshortref{subsubsec:optquant}; no further details are exposed.

\begin{figure}[htbp]
\begin{minipage}[t]{0.26\textwidth}\vspace{0pt} % check line 552
    \centering
    % Scale the image to fit half page width while keeping aspect ratio
    \includegraphics[width=\linewidth,keepaspectratio]{files/EdgeTPUCompiledModel.pdf}
\end{minipage}%
\hspace{0.5cm plus 1fill} % if I use \hfill the overfull disappears 
\begin{minipage}[t]{0.74\textwidth}\vspace{0pt}
    \vspace*{\fill}        % push down from top
    \raggedright           % left-align text inside this block
    \small

    Edge TPU Compiler version 16.0.384591198\\
    Input: dev/results/run\_20250719\_170647/quant.tflite\\
    Output: dev/results/run\_20250719\_170647/quant\_edgetpu.tflite\\

    \vspace{0.5em}
    \begin{tabular}{@{}lrl@{}}
      Operator & Count & Status \\[0.5em]  % extra space after header row
      CONCATENATION & 3  & Mapped to Edge TPU \\
      LOGISTIC      & 1  & Mapped to Edge TPU \\
      ADD           & 14 & Mapped to Edge TPU \\
      MAX\_POOL\_2D & 3  & Mapped to Edge TPU \\
      MUL           & 14 & Mapped to Edge TPU \\
      CONV\_2D      & 15 & Mapped to Edge TPU \\
      TRANSPOSE\_CONV & 3 & Mapped to Edge TPU
    \end{tabular}
    \vspace*{\fill}        % push up from bottom -> vertical centering
\end{minipage}
\caption{\gls{edgetpu}-mapped model (left) and compiler log excerpt (right).}
\label{fig:successcompile}
\end{figure}

The final artifact is a \code{.tflite} model file ready for \gls{edgetpu} inference.

\subsection*{Technical Challenges}

The \gls{devboard} is running on a end of life version of Mendel Linux, which is the only one latest officialy supported software version for \gls{devboard}.
With the OS itself comes already pre-installed runtime library \code{libedgetpu} of version v16.0 released on <verify version and maybe find date>, exposing a low-level C++ \gls{api}
for direct interaction with \gls{edgetpu}. Consequently, this library version is the latest officially available in Coral repository for this Mendel Linux release.
Building the newest one as of today from source requires more complex process using Bazel and potentially breaking dependencies and necessitates debugging,
which lies outside the scope of this thesis.
The newest as of today \gls{edgetpu} Compiler version v16.0 was, however, used,
requiring runtime minimum runtime version v14.0\footnote{\url{https://coral.ai/docs/edgetpu/compiler/\#compiler-and-runtime-versions}},
which was fully compatible with \code{libedgetpu} v16.0.

\todo{explain it inHardware, rename Hardware to Edge Device or something, because explain there Hardware + libedgetpu + Compiler + tflite 2.5 on device}

At a very early development stages, it was, however, not clear, which is the latest supported \gls{tf} version for the provided runtime library and compiler,
as the related information cannot be found in official documents.
Hence, heuristically the newest \gls{tf} version v2.19.0 (at the timepoint this work was conducted) was utilized for the first tries of dataset pipeline building,
model architecting, training and conversion. No \gls{qat} annotation was performed, solely relying on \gls{ptq} before compiling the model for \gls{edgetpu} trial inference.
At a compile time, the following error from \gls{edgetpu} Compiler was encountered:

\begin{lstlisting}
Edge TPU Compiler version 16.0.384591198
ERROR: Didn't find op for builtin opcode 'TRANSPOSE_CONV' version '4'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?
\end{lstlisting}

To avoid this error, the first idea was to completely omit the use of \code{'TRANSPOSE\_CONV'} \gls{tfop} in the model, relying on a more simple \gls{tfop} combination for deconvolution:
\code{'UpSampling2D'} + \code{'Conv2D'}, which will affect the model accuracy, but at least could yield acceptable, especially for edge device inference, results. The other idea, however,
was given a try: the \gls{tf} version was downgraded to v2.5.0 <check if true>, which corresponds to, as mentioned in \secshortref{subsec:hardware},
\gls{tflite} library version on \gls{devboard}. The rationale behind this decision is to try the best version compatibility combination, taking the edge device as starting point.
After the downgrade the first successfull compilation and and inference was performed, with the results presented in \secshortref{sec:evaluation}.

Upon introduction of \gls{qat} annotated model layers, the \gls{tfmot} package has to be installed for it. The version used needs to be compatible with \gls{tf} version,
which at that point was v2.5.0. This necessitated the installation of \gls{tfmot} v0.6.0. At a \gls{tflite} conversion step, during \gls{ptq} the following error keep occuring:

\begin{lstlisting}
error: 'tfl.max_pool_2d' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 840332.86281612806:-128> vs. !quant.uniform<i8:f32, 0.027852464160498452:-127>
\end{lstlisting}

\todo{improve looks}

This error reffers to a \code{restriction: Input and outputs must all have same scale/zero\_point} for \code{MAX\_POOL\_2D} layer from a \gls{tflite} specification table,
mentioned in \secshortref{subsubsec:optquant}. As part of the debugging, it was made sure, that the input and output minimum and maximum value ranges were essentially identical,
with only mean distributions changing, what is normal for differently distributed sets of batches:

\noindent
\begin{minipage}[t]{0.48\textwidth}
\begin{lstlisting}[basicstyle=\ttfamily\small\centering, columns=fullflexible, keepspaces=true, frame=single]
      ---- INPUT TO MAXPOOL ----
      Shape: (1, 384, 384, 16)
      min:   0.0
      max:   3.411252021789551
      mean:  1.003772497177124
\end{lstlisting}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.48\textwidth}
\begin{lstlisting}[basicstyle=\ttfamily\small\centering, columns=fullflexible, keepspaces=true, frame=single]
      ---- OUTPUT OF MAXPOOL ----
      Shape: (1, 192, 192, 16)
      min:   0.0
      max:   3.411252021789551
      mean:  1.0165772438049316
\end{lstlisting}
\end{minipage}


Furthermore, the sanity check of the value ranges of different input batches was performed, confirming expected values ranging from 0 to $\sim5$,
without extremely large or small numbers as outliers, hence providing no reason for big scale values, such as \code{840332.86281612806} as can be gathered from error message.
Activation ranges from the preceding \code{Conv2D} layer were investigated with the help of Netron, showing no conspicuousness as well.
These investigations are suggesting the internal issue of this specific \gls{tfmot} version, as the input and output data are contradicting the error's message scale calculation.

\todo{explain somewhere at the beginning the importance of Technical Challenges Chapter, saying that it is important for practical reproducibility,
but if the reader wants pure implementation logic, theyre optional}

During the debugging process was furthermore identified,
that the similar issue was encountered even on a more later \gls{tfmot} version\footnote{\url{https://github.com/tensorflow/model-optimization/issues/1053}}.
This information combined with own investigations led to the idea of trying newer \gls{tfmot} version, where this bug was already fixed.
Concerns about its compatibility with the older \gls{tf} v2.5.0 were raised, since it was needed for \gls{tfop} v3 compatibility mentioned earlier.
It was decided to prceed, however, due to the absence of documentation of version incompatibility of newer \gls{tfmot} against older \gls{tf}.
The idea was to try multiple version combinations of these modules.

Picking a combination of newer \gls{tfmot} and \gls{tf} versions resulted in resolving the issue, without making architectural changes to the model.
This recursively confirmed the the error as internal in \gls{tf} or \gls{tfmot} implementation code, fixed in later releases.

The successfull conversion to \code{.tflite} model format was subsequently achieved. This process, executed internally by new versions of \gls{tf} and \gls{tfmot},
inserted, however, automatically the \code{Transpose} \gls{tfop} into model architecture. The \code{Transpose} \gls{tfop}, however, was used at no point in model architecting,
indicating automated intertnal optimization of \gls{qat} annotation by \gls{tfmot}. The reasons for this are unclear,
with assumption of necessity to transform at some point of the conversion the layer's outputs between two common shape conventions:
\code{[batch, H, W, C] (NHWC)} and \code{[batch, C, H, W] (NCHW)}. The inability of a certain party, involved in conversion process,
to handle either of these shapes may request the transformation,
which is handled with \code{Transpose} \gls{tfop} inserted before \code{TransposeConv} \gls{tfop}. Below the architecture of the same model is illustrated before (left)
and after \gls{edgetpu} compilation process (right).

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPartQuantEdgeTPUFail.pdf}
    \caption{Links}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPartCustomEdgeTPUFail.pdf}
    \caption{Rechts}
  \end{subfigure}
  \caption{Zwei SVGs nebeneinander (als PDF)}
\end{figure}

The crucial console output part as well as \code{.log} file are provided, indicating the failed attempt to fuse multiple \glspl{tfop} into aspired \code{edgetpu-custom-op}.

\begin{lstlisting}[
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  breaklines=false,      % don't wrap—preserves column layout
  frame=single,
  xleftmargin=0pt,
  linewidth=\textwidth,  % fill the page width
  caption={Edge TPU compiler output for the quantized model},
  label={lst:edgetpu_compilation1},
  captionpos=b
]

Model successfully compiled but not all operations are supported
by the Edge TPU. A percentage of the model will instead run on the CPU,
which is slower. If possible, consider updating your model to use only
operations supported by the Edge TPU.
For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 4
Number of operations that will run on CPU: 7
See the operation log file for individual operation details.

Logfile:
Edge TPU Compiler version 16.0.384591198
Input: quant.tflite
Output: quant_edgetpu.tflite

Operator             Count         Status
      
CONV_2D              2             More than one subgraph is not supported
CONV_2D              2             Mapped to Edge TPU
TRANSPOSE_CONV       1             Filter, bias, or other param is not
                                   constant at compile-time
LOGISTIC             1             More than one subgraph is not supported
ADD                  1             More than one subgraph is not supported
MAX_POOL_2D          1             Mapped to Edge TPU
CONCATENATION        1             More than one subgraph is not supported
TRANSPOSE            1             Tensor has unsupported rank
                                   (up to 3 innermost dimensions mapped)
QUANTIZE             1             Mapped to Edge TPU
\end{lstlisting}

\todo{maybe wrap it better, add or remove some info, ensure consistent style across all work}

This behaviour confirms the inability of multiple model's layers to meet the requirements defined in \secshortref{subsec:hardware},
as well as confirms the procedure of compilation process in official documentation\footnote{\url{https://coral.ai/docs/edgetpu/models-intro/\#compiling}}.
That states that the compiler will stop at a first unsopported by \gls{edgetpu} operation and delegate all subsequent operations for \gls{cpu} execution,
even if after unsopported \gls{tfop} come the supported for \gls{edgetpu} execution operations. This behaviour is represented on the image below.

\begin{figure}[htbp]
  \centering
  % Choose either a width OR a height, or both with keepaspectratio.
  % Example A: set width only (aspect ratio preserved automatically)
  \includegraphics[width=0.7\linewidth]{files/compileSequenceEdgeTPU.png}

  % Example B (recommended): limit both max width & height, keep aspect ratio
  %\includegraphics[
  %  width=0.9\linewidth,
  %  height=0.5\textheight,
  %  keepaspectratio
  %]{images/example.png}

  \caption{Descriptive caption for the image (what the figure shows).}
  \label{fig:compileSeqEdgeTPU}
\end{figure}

The delegation of tensor operations to \gls{cpu} will almost certainly drastically increase the inference time,
which is inappropriate in the scope of the work, aspiring to map the entire model on \gls{edgetpu}.

As the process of \code{Transpose} \gls{tfop} insertion is happening completely internally and automatically,
there is no possibility to affect this or to explicitly set the layer's parameters to be constant at compile-time and of shape <3, as required by \gls{edgetpu} Compiler.

There are again two ideas could be realised. Avoid the use of \code{TransposeConv} or to continue the search for compatible \gls{tf} and \gls{tfmot} versions.
By choosing the second idea, the one is caught between a rock and a hard place. On the one hand the \gls{tf} version has to be kept as low as possible for the best compatibility with
\gls{tflite} on the edge device and to avoid encountering the \code{TRANSPOSE\_CONV version 4} compiler error, described earlier. On the other hand the version of \gls{tf} and \gls{tfmot}
has to be high enough to contain the fixed bug of scale incompatibility, encountered earlier. With this in mind, the following approach was chosen: find the highest \gls{tf} version,
which is still utilizing the v3 of \code{TransposeConv} \gls{tfop}. This approach, however is highly relying on the assumption, that all other newer \glspl{tfop},
probably introduced in newer \gls{tf} version, will not encounter compiling problems, as well as be compatible with the older \gls{tflite} v2.5.0 on an inference device.
Because essentially, if the \gls{edgetpu} Compiler successfully fuses every \gls{tfop} into \code{edgetpu-custom-op},
the \gls{tflite} runtime v2.5.0 is required to only be able to parse and load the model until it is delegated over to \gls{edgetpu}.
So in this case, it is more relied on the compiler as the last instance to check the general compatibility before running inference.
Hence the certain compatibility unsureness is still existing, which presence has to be considered when using new (possibly custom) layer types.
But in the scope of this work to achieve the objective this assumption is deemed as sufficient.

Given the absence of the official documentation about such detailed information of \glspl{tfop},
the search for the compatible \code{TransposeConv} \gls{tfop} was only possible by investigating the contents of \code{register.cc} file,
located in \gls{tf} repository in the folder \code{tensorflow/tensorflow/lite/kernels}. It is important to mention,
that in different \gls{tf} versions this file can be present in a slightly different folders, but in a close vicinity to the provided path.
The newest \gls{tf} version with a \code{TransposeConv} v3 is found out to be v2.10.0. The corresponding lines of \code{register.cc} are:

\begin{lstlisting}
AddBuiltin(BuiltinOperator_TRANSPOSE_CONV, Register_TRANSPOSE_CONV(),
           /* min_version = */ 1,
           /* max_version = */ 3);
\end{lstlisting}

It turned out, that the newest \gls{tfmot} v0.8.0 is backward compatible with \gls{tf} v2.10.0, despite being tested against \gls{tf} 2.14.1,
as can be confirmed on official releases page\footnote{\url{https://github.com/tensorflow/model-optimization/releases}}.

The problem of such version incompatibilities is presumably stemming from introducing the newer version of \code{libedgetpu} runtime library,
while keeping the old \gls{tflite} version on a \gls{devboard}, without providing an official possibility to upgrade and even out the versions.
This, as proven here, could be misleading and confusing when the one faced with the choice of inter-compatibility of multiple required packages.
Especially when newer improved versions of some of these packages have become available in the meantime.

At the end, the version combination \gls{tf} v2.10.0 and \gls{tfmot} v0.8.0 resolved all previous errors and led to successfull compilation of model,
fully mapping all \glspl{tfop} to \gls{edgetpu}, as it is presented in <ref image here!>.
This setup was subsequently used later on during the full process of the pipeline utilization in the scope of this work.

\todo{requirements mention somewhere}

\section{Deployment}
\label{sec:deployment}

After training, conversion and compilation processes the \code{.tflite} model is deployed on \gls{edgetpu} to run inference.

At early development stage the most convenient deployment way with the help of python utilities was applied.
Provided by Google these libraries wrap all necessary boilerplate into a small amount of functions with a purpose to conveniently load the model into \gls{edgetpu},
pass the input tensor to it and collect the output tensor. A simplified example of full inference in python code is presented below for understanding the simplicity of
operating \gls{edgetpu} with provided heplers.

%\begin{minted}{python}
%from pycoral.utils.edgetpu import make_interpreter
%from pycoral.adapters import common
%
%# Load model
%interpreter = make_interpreter(model_path)
%interpreter.allocate_tensors()
%
%# Prepare input
%common.set_input(interpreter, input_tensor)
%
%# Inference
%interpreter.invoke()
%
%# Output
%output = common.output_tensor(interpreter, 0)
%\end{minted}

\todo{ENABLE Minted}

Utilizing the provided python modules the first inference results were obtained, they are presented along with the corresponding ground truth on the image below.

\begin{figure}[htbp]
    \centering
    % First image
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[height=5cm]{files/FIRSTInferenceResult.png}
        \caption{First image}
    \end{subfigure}
    \hfill
    % Second image
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[height=5cm]{files/FIRSTInferenceResultGT.png}
        \caption{Second image}
    \end{subfigure}
    \caption{Two images side by side}
    \label{fig:two-images}
\end{figure}

This single patch inference initial results became a milestone of this work and laid the foundation for the subsequent inference pipeline.

To simplify the operation of \gls{edgetpu} hardware so much, developers are providing two python libraries\footnote{\url{https://coral.ai/docs/edgetpu/tflite-python/}}:
PyCoral \gls{api} and \gls{tflite} \gls{api}, where PyCoral wraps the \gls{tflite} repetitive functions needed for inference into even simplier ones, as shown on the image above <REF>.

After successfull implementation of a single patch inference in python, it was decided to expand it into the full scene inference pipeline using C++.
The main rationale behind it lies in the extended control over inference process and its potential acceleration.
Furthermore python overhead functions to, for instance, read the \code{.TIF} images directly,
transform them and pass to \gls{edgetpu} in one go yields on one side teh convenience of implementing the complete pipeline inside one script.
On the other side, however, may lead to slowed process unwanted on embedded system inference. This logic applies to other necessary operations,
that potentially had to be performed in python. More common approach for embedded systems has to be utilized, transforming the input data
potentionally on other device, perform inference on edge device, transform output data on other device.
Additionall factor that led to implementation of C++ inference was the specific error,
encountered at the end of each and every Python inference. It was believed at the time,
that more control over inference sequence with the help of C++ functions will shed light on and help to get rid of that error.
More specific about it in \secshortref{subsec:tecchallDeployment}.

The \gls{devboard} Mendel Linux OS contains from installation almost every library needed for operating its \gls{edgetpu} module.
That holds true for Python libraries, such as PyCoral and \gls{tflite} \glspl{api}, as well as for hardware runtime library \code{libedgetpu}.
The essential \gls{tflite} C++ \gls{api} library is not included, though.
It is furthermore impossible to cross use the \gls{tflite} \gls{api} for Python by C++,
since the binary is compiled specifically for Python and in fact lacks the header files for C++.
On a chart below the general flow of running the model on \gls{edgetpu} on \gls{devboard} is represented,
highlighting the difference between inference implementation in Python and C++.

\begin{figure}[H]
\centering
\resizebox{\linewidth}{!}{%
\begin{tikzpicture}[
  >=Stealth, ->,
  box/.style={rectangle,draw,rounded corners,
              minimum width=1cm,minimum height=1cm,
              align=center,font=\normalsize}
]

% Shared start
\node[box] (model) {.tflite\\FlatBuffer Model};

% API branches
\node[box, right=0.5cm of model, yshift=2cm] (pyapi) {PyCoral \gls{api} (optional) or\\\gls{tflite} Python \gls{api}};
\node[box, right=0.5cm of model, yshift=-2cm] (cppapi) {\gls{tflite} C++ \gls{api}\\(headers \& library)};

% Convergence
\node[box, right=4cm of model] (interp) {\gls{tflite} Interpreter\\(C++ core)};

% Runtime + hardware
\node[box, right=1cm of interp] (runtime) {\gls{edgetpu} runtime driver\\\texttt{libedgetpu.so}};
\node[box, right=1cm of runtime] (hw) {\gls{edgetpu}\\Hardware Accelerator};

% Connections
\draw (model) -- (pyapi);
\draw (model) -- (cppapi);
\draw (pyapi) -- (interp);
\draw (cppapi) -- (interp);
\draw (interp) -- (runtime) -- (hw);

\end{tikzpicture}%
}
\caption{ADJUST}
\end{figure}

Since no pre-build \gls{tflite} C++ \gls{api} libraries can be found for \gls{devboard}, it had to be build from source.
The process is described in detail in \secshortref{subsec:tecchallDeployment}
and marks the third and the last technical implementation challenge milestone in the scope of this work.

After successfully incorporating the \gls{tflite} C++ \gls{api} library on \gls{devboard},
it was utilized to implement the first C++ inference inside the \code{inference.cpp} file. Following the success,
the folder reading feature was implemented on C++ to process multiple patches in a row.
These patches had to be stored as \code{.bin} files with known shape and type (\gls{float32}) of the data inside.
After inference the program writes output \code{.bin} files with predicted cloud masks in \gls{float32} format.
Quantization parameters of input and output are retrieved from model during the inference and used for quantized transformation of input and dequantization of the output tensor.
Later on, the stitching process was C++ implemented too.
By running the \code{stitch.cpp} file all \code{.bin} patches inside the input folder are gapless stitched into a single \code{.bim} scene.
This \code{.bin} scene can be then converted to \code{.png} file for visual inspection.
It is implemented to optionally be performed on \gls{devboard} with the help of Python script \code{convert.py}.
\todo{check all filenames}
It has to be mentioned, that for the sake of convenience the whole inference, stitching and conversion process could have been implemented as a single C++ executable.
It was kept, however, separate due to two reasons. Firstly allowing more fexibility and less overhead for later potentially performance critical implementation on the satellite.
Secondly, the persisting error, described in \secshortref{subsec:tecchallDeployment}, is not allowing to run any code immediately after inference process is done.

As final steps, concluding the inference implementation on \gls{devboard} were following improvements.
The SDCard with prepared \code{.bin} patches for each scene on it, is mounted on a \gls{devboard}. This allows to host the entire dataset of several GBs of size,
which would be impossible to do an a \gls{devboard}.
The stitching and conversion processes were unified under a single executable, with Pyhton function call for conversion after C++ \code{.bin} stitching.
During both inference and stitch-conversion pracesses the user is promted to pick one out of 20 test scenes for inference and/or conversion.
Furthermore, the Python process of conversion to \code{.png} is improved by eliminating the big output array being loaded in \gls{devboard}'s RAM memory,
by handling it in chunks, therefore enabling to convert bigger full resolution scenes.

The final version of inference, stitching and conversion implementation on a \gls{devboard} can be found in \code{git} under the tag \code{Deployment\_v1.0}.

\subsection*{Technical Challenges}
\label{subsec:tecchallDeployment}

The last significant implementation milestone in the scope of this work was to build the \gls{tflite} C++ \gls{api} library for the \gls{devboard} from source.
As can be seen on the chart <REF>, this library is crucial for implementation of pure C++ inference.

A guidance to carry out this task is very sparsely mentioned in the official documentation\footnote{\url{https://coral.ai/docs/edgetpu/tflite-cpp/\#build-your-project-with-libedgetpu}}.
Taking these recommendations into account it was clear, that the compatible version of \gls{tf} to a pre-installed on \gls{devboard} \code{libedgetpu} library had to be determined.
The version of installed \code{libedgetpu} was determined quickly as v16.0. But on the official releases website\footnote{\url{https://github.com/google-coral/libedgetpu/releases}},
can be found no mention of numerical verison convention, so the literal date when the symlink \code{libedgetpu.so.1} was modified was used to approximately determine the release version.
On the particular \gls{devboard} used in this work it was the 09.07.2021,
predating the official "Grouper" \code{libedgetpu} release\footnote{\url{https://github.com/google-coral/libedgetpu/tree/release-grouper}} on 26.07.2021 by two weeks.
Hence from the \code{workspace.bzl} of this exact release the exact \gls{tf} commit corresponding to the \gls{tf} release 2.5.0 as of 05.07.2021 was determined.

First build try was performed on a Windows machine, cross-compiling for \code{aarch64} Linux. After successfull compilation the wall was hit during the first inference test,
where it was run into low level incompatibility with \gls{devboard}'s atomic instructions. It became clear, that the library has to be build either by using Bazel with predefinition
of full \gls{devboard} configuration or directly on the \gls{devboard}. It was made a decision to stick with the second option. In order to do it, the SDCard was used to host large
\gls{tf} folder (of the specific v2.5.0 commit) and additionally provide RAM swap space, due to concerns,
that \gls{devboard} will run out of RAM memory during compilation process. It is important to mention,
that building was performed using the latest by the time of the work portable version of \code{CMake}. 

After successfully building the static \code{libtensorflow-lite.a} library, several additional necessary steps had to be performed and files gathered.
The unavoidable \code{edgetpu.h} header has to be downloaded\footnote{\url{https://github.com/google-coral/libedgetpu/blob/master/tflite/public/edgetpu.h}},
together with a compatible with the given build FlatBuffers library v1.12.0. Additionally every \gls{tflite} required static library had to be included.
The necessary include files are provided in git and can also be investigated in \code{MAKEFILE}.

After all compilator and linker issues were resolved and requirements satisfied, the successfull model loading and tensor allocation using C++ \gls{api}
marked the last technical milestone of this work.

\subsubsection*{Unresolved error}

In the scope of this work and regarding to the specific \gls{devboard} with its specific installed Mendel Linux OS and the modules an interesting
behaviour after inference could not remain unnoticed.
From the early development stages and up to the final improvements of C++ inference in the scope of this thesis, one of the two following errors persistendly
kept appearing always after (!) the inference was completed: "Segmentation fault" or "Bus error". First it was encountered during the simple proof-of-concept
Python inference, at the end of the program, after successfully saved inference results. Being disasterous for a programmer by its nature,
but strangely appearing at program's closing, these errors had no effect on obtaining the required inference results. Clearly stemming from C++ background code
and thus providing no way for debugging them in Python, these errors allowed further development of pipeline having no effect on the required results.
The idea of C++ inference implementation was, however, further fueled by the desire to get rid of these errors. It was assumed,
that more low-level C++ implementation could shine the light on them. But it was found out, that even after successfully obtaining first results with pure C++ inference,
these errors kept their alternating appearence at the \code{main()} program's exit. During the debugging process it was established,
that these errors indeed come from \code{libedgetpu} library, presumably during cleanup routine after the inference is completed. There was, however,
no possibility to trace the exact line of code causing these errors, since provided \code{libedgetpu} library was build without debugging information.
In order to trace these errors the library would have to be rebuild with the necessary debugging information included. This potentially encompasses difficuilties
with broken dependencies and other corresponding to the custom low-level library installation difficuilties.
This is certainly a very interesting and worthwhile investigative error, but it lies beyond the scope of this thesis.
It is assumed, that the error was fixed in later \code{libedgetpu} releases\footnote{\url{https://github.com/tensorflow/tensorflow/issues/62371}},
but with no official way to update the library on the given device the dive into custom build would have been inevitable.

It was decided to continue the work, since these error had no effects on performance and results.
It is possible to run multiple \code{Interpreter} invokations to process folders of files.
It is, however, not possible to run any code after fully completed inference, since the program is crashing at exit.
Therefore stitching and conversion processes had to be done as separate programs.

\section{Evaluation}
\label{sec:evaluation}

With the implementation of deployment pipeline on \gls{devboard}, the cloud prediction masks can be finally obtained.
They are visually verified by the human observer, qualitatively marking the success of implementation and achieving the thesis' goal.
Furthermore the quantitative performance metric was implemented in \code{inference.cpp} on \gls{devboard} to determine the inference time per precossed patch.
By summing up the inference time, the model's inference time for the whole scene is obtained.

The flexibility of training and deployment pipelines allow convenient modular improvement and feature expansion.
It can be tailored to the needs and requirements of model's development, intended to be deployed immediately on the satellite.
For this task additional model's performance quantitative evaluation is necessary.
It will ultimately be used in \secshortref{chapter:evaluation}, where the performance of several \gls{edgetpu} optimized models will be asessed.

The evaluation pipeline is designed as an additional segment after the training pipeline. It is designed to run on a machine used for training, not on \gls{devboard}.
The evaluation pipeline consists of two parts: inference and evaluation.

\subsection*{Inference}

The necessary functions to perform inference and to save the results are implemented in \code{inference.py} and \code{inferenceQ.py} files.
\code{inferenceQ.py} utilizes \gls{tflite} Interpreter and the corresponding fully \gls{int8} quantized \code{.tflite} model to perform inference,
while \code{inference.py} operates with \gls{tf} utilities and uses \code{.h5} saved model format.
It was noticed, that performing quantized model inference on a \gls{cpu} results in significantly slower inference time compared to \code{.h5} \gls{float32} model.
This slows down the evaluation process. It was furthermore noticed,
that execution of quantized model on a general use \gls{cpu} <specify CPU and mention that it was even slower!!!> yields similar inference time
compared to deploying the quantized model on a dedicated \gls{edgetpu}. This inherently epmphasizes the sophistication of a \gls{tpu} to perform
this type of operations compared to much more powerful for general tasks \gls{cpu}. These findings resulted in omiting the quantized inference during the evaluation process.
\code{inference.py} is therefore used for the rest of the evaluation.

Firstly, the \code{buildDS} function feature is used to construct the testing dataset.
As mentioned in \secshortref{subsec:testing},
the pre-builded \code{.csv} files are used to construct the dataset either for evaluation of a single scene (specified by its ID or left to be chosen randomly),
or for the full test dataset. In the later improved versions of \code{inference.py} the inference of full dataset is handled internally scenewise,
achieving reduced RAM memory usage and preventing crashes. Adherently has to be mentioned, that the \code{fullTestDS.csv} is not being used at all due to this improvement.
Then the path to the desired \code{run} folder is specified, where the model files (\code{.h5} and \code{.tflite}) are located.
In \code{inference.py} the \code{.h5} model is loaded. Additional custom \gls{tf} objects are specified as needed for model loading,
such as, for instance, \code{softJaccardLoss} or \code{diceCoefficient}.
If the model predictions are provided in reduced resolution (e.g. as \ensuremath{192\times192} patches),
the predictions are upsampled to full \ensuremath{384\times384} resolution with the intention to later match full scene \glspl{gt} during evaluation.
The upsampling is performed using the \code{bilinear} method,
which suits best for continuous probability distribution prediction masks <mention paper or section with explanation>.

Finally, the infernce results are saved per scene in \code{.npy} files to be used in evaluation process.
Optionally the masks can be saved as \code{.png} files for visual inspection.

\subsection*{Evaluation}

After obtaining inference results for the complete testing set (20 scenes), they have to be quantified with evaluation metrics.
These metrics, defined in \secshortref{subsec:evalmetrics}, are implemented in \code{evaluate.py} file.
By specifiying the \code{run} folder, containing subfolders \code{evaluation/inference},
the algorithm searches and processes every scene's corresponding \code{.npy} file generated there by inference process.

As mentioned in \secshortref{subsec:dataset}, the test input scenes were cropped and padded to match \ensuremath{384\times384} patch format.
This process additionally introduced the artificial size increase of border patches in the process. In order to match back the scene size of \gls{gt} masks,
zero-padding at the borders has to be removed.
This is performed by applying the center-cropping on the prediction masks and is implemented in the helper function \code{unpadToMatch}.
This is done in the same way as it was performed in the original, dataset making papers \cite{CloudNet2019, CloudDet2018} by analyzing their \code{unzeropad}
function, used in evaluation process\footnote{\url{https://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset/blob/master/evaluation/evaluation.m}}.

Next, the evaluation pipeline provides two ways of definition of a threshold to binarize the predicted masks.
Either the best threshold, calculated using the validation dataset, can be applied; or the best threshold can be defined for each test scene,
ultimately utilizing the test dataset \gls{gt} masks. The effects from this will be discussed in \secshortref{chapter:evaluation}.
This feature is implemented by passing the optional argument \code{fixedThreshold} in the evaluation function \code{evaluateAll}.
\todo{mention threshold searching feature in training pipeline, mention evaluatePRC function there and link its usage in here too, so I dont need to mention it here}

As final step the prediction masks are binarized and evaluation performed using \code{computeMetrics} function.
The final evaluation results are saved in either \code{metricsValThr.csv} of \code{metricsTestThr.csv} files, depending on threshold used for binarization.
Optionally, \code{.npy} format per scene results can be saved, together with \code{.png} format masks for visual evaluation by the human observer.

\vfill

This process concludes the Implementation chapter and therefore finalizes the software development within the scope of this work.

}