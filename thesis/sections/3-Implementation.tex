{

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\chapter{Implementation}
\label{chapter:implementation}

A detailed description of the software development process used to achieve the thesis objective is provided in this chapter.
Along the way, three major implementation milestones were reached. Despite several hurdles, success was achieved in
(i) training the model on powerful cloud machines, (ii) mapping all its components, fully quantized, to the \gls{edgetpu},
and (iii) performing C++ inference on the \gls{devboard}.
The technical obstacles and their resolution are presented in three “Technical Challenges” chapters inside
\nameref{sec:training}, \nameref{sec:conversion}, and \nameref{sec:deployment}.

Software developed in the scope of this work
is made available for free and unrestricted use on the author's GitHub profile:
\begin{itemize}
  \item \url{https://github.com/An3Zh/BachelorThesis}
\end{itemize}

Details on relevant file locations, with references to this work's chapters,
as well as further information on the repository structure, are provided in \code{README.md}.

\section{Data}
\label{sec:data}

\subsection{Training \& Validation}

In order to construct the training and validation pipeline, the function \code{BuildDS} was implemented, along with several supporting helper functions, all located in the \code{load.py} file.
The function was later extended to optionally include the test dataset, which will be discussed in \secshortref{subsec:testing}.

Efficient data handling and memory management are achieved through the use of built-in \gls{tf} utilities.
In particular, the \code{tf.Data.TextLineDataset}\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset}} function
is employed to sequentially read the \code{.csv} files,
which contain the patch filenames as outlined in \secshortref{subsec:dataset}.
This provides the foundation for the dataset pipeline.

The complete training set originally contains 8400 patches. However, as explained earlier in \secshortref{subsec:dataset}, some patches are entirely zero-valued.
Only 5155 of them contain valid data and are thus retained for subsequent use.
The dataset is shuffled and split into training and validation subsets, with the ratio configurable as needed.

Using the \code{.map} method, each text line is first expanded into five full paths to the corresponding \gls{rgb}, \gls{nir} and \gls{gt} mask patches.
Each filepath is then replaced by its image content, loaded as \code{tf.Tensor}\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/Tensor}}.
This transformation is handled by the helper function \code{loadDS}, which itself calls utility function \code{loadTIF}.
At this stage, each dataset element is a tuple of \gls{tf} tensors representing the four-channel input image and its corresponding \gls{gt} mask.

The \code{loadDS} and \code{loadTIF} functions perform the following operations:

\begin{itemize}
    \item RGB and NIR patches are cast to \gls{float32} and normalized to the range [0,1].
    \item \gls{gt} masks, originally in \gls{uint8}, are binarized to values of 0 and 1, and also cast to \gls{float32}.
\end{itemize}

An additional feature of the \code{loadDS} function allows for optional resizing of the input images if a target size is provided.
The image loading and transformation pipeline is designed to avoid information loss until the controlled resizing step, where a reduction in resolution is intentional.
In the case of resizing, \gls{gt} masks are resized using nearest-neighbor interpolation. This method copies the value of the closest original pixel for each target pixel,
thus preserving hard edges in the cloud segmentation mask and avoiding the introduction of intermediate values.
Conversely, the \gls{rgb} and \gls{nir} inputs are resized with bilinear interpolation,
which computes each new pixel value as a weighted average of the nearest $2\times2$ neighborhood. This results in smooth pixel transitions while maintaining important image details.
Nearest-neighbor interpolation is among the earliest digital image resizing techniques, dating back to the origins of digital image processing in the 1960s,
whereas bilinear interpolation gained prominence in early computer graphics literature and is now standard in deep learning pipelines \cite{bilinearNearest1, bilinearNearest2}.

As a final preparation step, the dataset is: shuffled, batched, set to repeat indefinitely,
and prefetched to optimize data retrieval during training.
Because the datasets are repeated indefinitely, it is essential to define the number of steps required to complete one full pass through the training and validation subsets.
These step counts are used to ensure the correct number of iterations per epoch during training,
and they are computed as follows: \code{trainSteps(valSteps)~=~trainSubsetSize(valSubsetSize)~/~batchSize}.

\subsection{Testing}
\label{subsec:testing}

An additional capability of the \code{buildDS} is the construction of the test dataset.
As outlined in \secshortref{subsec:dataset}, only cropped \gls{rgb} and \gls{nir} test patches, together with complete scene \gls{gt} masks, are available for testing.
This arrangement necessitates two specific preparation steps prior to building the test pipeline:

\clearpage
\begin{enumerate}
    \item \textbf{Metadata collection:} To uniquely identify each scene, the \code{sceneID} is introduced, derived from the Landsat 8 metadata (path/row),
    and is unique within this test dataset. The total number of patches, as well as the number of rows and columns for each scene, is collected for every \code{sceneID}.
    The patch filenames, as outlined in \secshortref{subsec:dataset}, are used for this process.
    Additional \code{.csv} files were manually created --- each containing the ordered patch filenames corresponding to every full scene.
    Filenames in these \code{.csv} files are organized in the order resulting from cropping (left to right, top to bottom).
    Furthermore, the \code{fullTestDS.csv} file was generated, containing ordered patch filenames for all 20 full scenes.
    These \code{.csv} files, not provided in the original dataset,
    were developed in the scope of this thesis to support the test pipeline and are stored in the \code{additionalCSVs} folder within the testing subset.
    Based on this manually collected metadata, the \code{getSceneGridSizes} function was implemented in \code{load.py} file,
    returning a dictionary that maps each of the 20 \code{sceneID}s to the respective number of rows and columns in each scene.
    \item \textbf{Patch stitching:} The \code{stitchPatches} function was implemented in \code{load.py}.
    This function uses the \code{.csv} files and the \code{getSceneGridSizes} function to reconstruct entire scenes from the model's output patches for evaluation.
    The function can operate in two modes: it can either stitch together all scenes at once, saving all 20 full scenes,
    or stitch only a single specified scene. The single-scene mode is intended for debugging and verification, avoiding time-consuming processing of the full test subset.
\end{enumerate}

Following these preparatory steps, the \code{buildDS} function was extended with additional functionality to construct the test subset.
Using the optional boolean parameter \code{includeTestDS}, which indicates whether to include the test dataset, and the parameter \code{singleSceneID},
which allows the selection of a specific scene, the function now supports multiple modes for testing:

\begin{itemize}
    \item Utilizing all 9201 patches from the 20 test scenes, or
    \item Selecting patches from a single scene, either by specifying its \code{singleSceneID} or allowing the function to randomly select one.
    %based on pre-generated \code{.csv} files that map patch names to \code{sceneID}.
\end{itemize}

The dataset is then constructed using the same \gls{tf} utilities employed for the training and validation subsets.
It is important to emphasize that the test set is not shuffled, as preserving the initial patch order is essential for the subsequent stitching process.

Consequently, the \code{buildDS} function returns the training and validation subsets, along with their respective step counts based on the \code{batchSize}.
Optionally, it also returns test subset, which may contain either the entire test dataset or a single scene.
After inference, the model's output patches can be passed to the \code{stitchPatches} function, which reconstructs and saves the final scene images for further evaluation.

\clearpage
\section{Model}
\label{sec:model}

The core building blocks and utility functions for architecting the model are organized in \code{model.py}.
\gls{cnn} layers are constructed using \gls{tf}'s built-in \glspl{api}, allowing for a modular and reusable design.

In early development, the \code{simple} model architecture was implemented.
It served as a proof of concept and was used for initial debugging of the training and conversion pipelines, as well as to obtain the first inference results on the \gls{devboard}.
The model was not annotated for \gls{qat}.

Below, in \autoref{fig:quantnotquantcomp} are shown the examples of layer diagrams for two models: the left is not annotated for \gls{qat}, the right is annotated.
These visualizations were produced with Netron\footnote{\url{https://netron.app/}}.
Following the concept outlined in \secshortref{subsubsec:qat}, the \code{QuantizeWrapperV2} and \code{QuantizeLayer} \glspl{tfop} are inserted into the architecture,
wrapping the base layers with quantize-dequantize operations.

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPart.pdf}
    \caption{model part not annotated for \protect\gls{qat}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPartQ.pdf}
    \caption{model part annotated for \protect\gls{qat}}
  \end{subfigure}
  \caption{Model before and after \protect\gls{qat} annotation}
  \label{fig:quantnotquantcomp}
\end{figure}

It is important to emphasize that \gls{bn} is applied to the output of the \code{Conv2D} \gls{tfop} and precedes the corresponding \code{Activation} function \cite{batchnormActivation}.

Subsequently, more complex \code{U-Net} architectures were implemented, with greater depth and skip connections.
Early versions of these models were not \gls{qat} annotated, instead, \gls{ptq} was applied at conversion time.

Where standard \gls{tf} loss functions are insufficient, custom losses such as \code{softJaccardLoss} and \code{diceLoss} are implemented in this module.
Custom metrics, particularly those used for segmentation evaluation and referenced in \autoref{subsec:evalmetrics}, are also provided within the \code{model.py} file.

\section{Training}
\label{sec:training}

The complete model training and conversion pipeline is implemented in the \code{main.py} file.

The process begins with configuration settings, which include:
\begin{itemize}
\item Batch size for training
\item Image size for both training and inference
\item Number of training epochs
\item Selected model architecture
\item Ratio between validation and training subsets
\item Number of batches used for the calibration dataset
\end{itemize}
Additional settings can be incorporated as needed.

The pipeline starts by loading the dataset using functions from \code{load.py}.
The selected model architecture is then compiled utilizing utilities from \code{model.py}.

Each training run is stored in a dedicated folder, named with a timestamp corresponding to the start of the run.
This folder contains all configuration files, training-related artifacts, and final results, including the compiled \gls{edgetpu} model.

Prior to training, the following configurations and callbacks are set up:

\begin{itemize}
\item \textbf{Model Checkpoints}: Model checkpoints are saved during training whenever the validation loss improves, preserving the best-performing model.
\item \textbf{Early Stopping}: If the validation loss stops improving, training continues for a predefined number of additional epochs before termination.
\item \textbf{Learning Rate Reduction}: The learning rate is reduced if the validation loss does not improve for a set number of epochs, helping to fine-tune convergence.
\end{itemize}

Once all configurations are saved, model training is initiated with the specified callbacks active.

Upon completion of training, the final model weights are saved and immediately used for model conversion.

\subsection*{Technical Challenges}

Training of initial proof-of-concept models was performed on a personal workstation equipped with an Intel Core i5-13400F \gls{cpu} and an NVIDIA GeForce RTX 4060 \gls{gpu}.
To enable \gls{gpu} acceleration for the \gls{tf} training process, the NVIDIA \gls{cuda} \gls{api} and the corresponding \gls{cudnn} library must be installed and configured.
The following versions were used: \gls{cuda} v11.2 and \gls{cudnn} v8.1.1.
These versions are not the latest available from NVIDIA at the time of this work, but were selected for compatibility with certain \glspl{tfop} and the \code{libedgetpu} runtime library.
Further details on these compatibility considerations are discussed in \secshortref{sec:conversion} and \secshortref{sec:deployment}.

Initial models containing approximately 300,000 parameters were trained without \gls{qat} for tens of epochs.
On the described setup and using the full training dataset, the time required for a single epoch was less than 15 seconds, which was deemed acceptable.

With the introduction of more complex models containing 2,000,000 and 30,000,000 (experimental, proven to be inefficient at edge inference) parameters,
and when employing \gls{qat}, training times increased significantly.
For these larger models, one epoch on the full training dataset required approximately 3 minutes and 12 minutes, respectively.
Since at least 100 epochs were necessary to achieve adequate model performance, more powerful hardware resources became essential.

Thunder Compute\footnote{\url{https://www.thundercompute.com/}}, an American startup with \gls{gpu} cloud resources for machine learning and data science,
provided solutions for large-scale experiments.
By using an instance with an NVIDIA A100XL \gls{gpu}, training times for the 2,000,000 parameter network were reduced to approximately 20 seconds per epoch,
and for the 30,000,000 parameter network to 40 seconds per epoch.
A \ensuremath{10\times} increase in \gls{gpu} memory, to 80GB on the A100XL compared to 8GB on the RTX 4060, also enabled larger batch sizes,
resulting in more effective training and improved model robustness.

However, significant effort was required to configure the cloud instance for compatibility.
The more expensive production mode\footnote{\url{https://www.thundercompute.com/docs/production-mode}} had to be used,
as the prototyping mode\footnote{\url{https://www.thundercompute.com/docs/prototyping-mode}} did not allow downgrading \gls{cuda} and \gls{cudnn} versions.
The \gls{cuda} and \gls{cudnn} libraries needed to be downgraded from their most recent platform versions to those compatible with an older \gls{tf} version.
This was challenging, as it required purging the latest \gls{cuda} and \gls{cudnn} files without removing the \gls{gpu} drivers required by the older versions.
Ultimately, \gls{cuda} v11.2 and \gls{cudnn} v8.1.0 were installed on the instance for subsequent training.
In addition, the Thunder Compute development team was contacted with a suggestion to allow selection of \gls{cuda} and \gls{cudnn} versions at instance creation.
As a result, an improvement is planned that will allow passing the desired \gls{cuda} and \gls{cudnn} version as an environment variable during instance startup.

\section{Conversion}
\label{sec:conversion}

The file \code{convert.py} contains all necessary functions and utilities for model conversion and \gls{edgetpu} compilation.
These functions are called from \code{main.py} immediately after training is complete.

\subsection{Weight Transfer}

As outlined in \secshortref{subsec:hardware}, the \gls{edgetpu} supports input tensors with at most three dimensions.
This necessitates the use of batch size one, since the input tensor shape is already \ensuremath{image~height~\times~image~width~\times~number~of~channels}.
During training, larger batch sizes are typically used for efficiency.
Therefore, after training, the function \code{asBatchOne} is used to transfer the trained weights to a model with identical architecture,
but with batch size set to one for the input, all intermediate computations, and the output.

\subsection{Post-Training Quantization}

The \code{tf.lite.TFLiteConverter}\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter}} utility is used to perform \gls{ptq} and convert the model into
the \code{.tflite} format.
A representative dataset, consisting of training \gls{rgb} and \gls{nir} patches, is generated using the \code{representativeDatasetGen} function.
The number of calibration batches used can be adjusted via \code{numCalBatches}, as defined in \secshortref{sec:training}.
The greater the amount of calibration data, the more likely the observed value range will closely match the inference data, improving quantization accuracy.

As a next step, all supported \glspl{tfop} as well as model inputs and outputs are quantized.
While various data types can be used for quantization in general case,
this work utilizes \gls{int8}, as required for \gls{edgetpu} compatibility in \secshortref{subsec:hardware}.

The following subchapter provides a deeper investigation into quantization of model parameters by analyzing a \gls{qat}-annotated \gls{float32} model and its already quantized version.
The Netron tool is used here as well for visualizing the model's parts.
This chapter is aimed at readers who wish to deepen their understanding of parameter quantization and gain insight into the general techniques
behind full-integer operation execution on edge devices such as the \gls{edgetpu}.

It should be noted that both \gls{qat} annotation and \gls{ptq} are already wrapped into convenient, ready-to-use \gls{tflite} commands.
Thus, this section is not intended to teach the implementation of quantized models from scratch. Instead, it aims to offer a deeper understanding that can be beneficial for debugging, designing custom (potentially more powerful) models, or simply appreciating the engineering trade-offs between speed, memory, and accuracy on constrained devices.

\subsection{Practical Example of Model Quantization}
\label{subsubsec:optquant}

Below, a \gls{qat} annotated part of the model is shown in the left panel of \autoref{fig:qatconvertedquant}.
On the right, the same part is shown after \gls{ptq}, yielding a fully \gls{int8} quantized model.

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPartQBeforePTQ.pdf}
    \caption{model part annotated for \protect\gls{qat}}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPartQAfterPTQ.pdf}
    \caption{model part after full \protect\gls{int8} quantization}
  \end{subfigure}
  \caption{Model before and after quantization}
  \label{fig:qatconvertedquant}
\end{figure}

Netron also provides detailed information about graph nodes (model layers) and their connections (tensor passes).
For example, layer weights and biases can be directly inspected, and values can be examined.
In the tables below, quantization-relevant information is displayed for further analysis.

For both the \gls{qat} annotated model and the resulting quantized model, the relevant information for four layers is summarized.
All floating-point numbers are shown in full \gls{float32} precision.
Representative calculations are performed in full precision and displayed with extended decimals.
The goal is to illustrate, using a practical example, the error effects of quantization and the limits of \gls{float32} precision.

\clearpage
\subsubsection*{\gls{qat} Annotated Model (left in \autoref{fig:qatconvertedquant})}

\begin{layerbox}{Layer Type: QuantizeLayer}{Layer Name: Quantize Input}
  \begin{center}
    \textbf{Input value range:} \\[2pt]
    $\min = 0 \quad\quad \max = 1$
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: QuantizeWrapperV2}{Layer Name: Conv2D No. 1}
  \begin{center}
    \textbf{Kernel shape:} \\[2pt]
    $[3, 3, 4, 32]$ \\[6pt]
    \textbf{Kernel No. 1 value range:} \\[2pt]
    $\min = -0.544256329536438 \quad\quad \max = 0.544256329536438$ \\[6pt]
    \textbf{First element of Kernel No. 1 value:} \\[2pt]
    $0.032761260867118835$ \\[6pt]
    \textbf{Bias shape:} \\[2pt]
    $[32]$ \\[6pt]
    \textbf{Bias No. 1 value:} \\[2pt]
    $0.1983194351196289$ \\[6pt]
    \textbf{Activation value range:} \\[2pt]
    $\min = -2.4379329681396484 \quad\quad \max = 1.0039010047912598$
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: \glsxtrlong{bn}}{Layer Name: \gls{bn} No. 1}
  \begin{center}
    \textbf{Shape of $\gamma$, $\beta$, $\mu_{\text{moving}}$, $\sigma^2_{\text{moving}}$ parameters:} \\[2pt]
    $[32]$ \\[6pt]
    \textbf{Parameter $\gamma_1$ value:} \\[2pt]
    1.110548973083496 \\[6pt]
    \textbf{Parameter $\beta_1$ value:} \\[2pt]
    -0.11593257635831833 \\[6pt]
    \textbf{Parameter $\mu_{\text{moving},1}$ value:} \\[2pt]
    0.17263321578502655 \\[6pt]
    \textbf{Parameter $\sigma^2_{\text{moving},1}$ value:} \\[2pt]
    0.00221889466047287
  \end{center}
\end{layerbox}


\begin{layerbox}{Layer Type: QuantizeWrapperV2}{Layer Name: Activation No. 1}
  \begin{center}
    \textbf{Output value range:} \\[2pt]
    $\min = -3.696099329375535 \mathrm{e}{-11} \quad\quad \max = 18.816810607910156$
  \end{center}
\end{layerbox}

\clearpage
\subsubsection*{Model After Quantization (right in \autoref{fig:qatconvertedquant})}

\begin{layerbox}{Layer Type: Conv2D}{Layer Name: Conv2D No. 1}
  \begin{center}
    \textbf{Input affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.003921568859368563 \times (q_{\text{x}} + 128)$ \\[6pt]
    \textbf{Kernel No. 1 symmetric dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.004285482689738274 \times q_{\text{x}}$ \\[6pt]
    \textbf{First element of Kernel No. 1 quantized value:} \\[2pt]
    $8$ \\[6pt]
    \textbf{Bias No. 1 symmetric dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.000016805815903353505 \times q_{\text{x}}$ \\[6pt]
    \textbf{Bias No. 1 quantized value:} \\[2pt]
    $11801$ \\[6pt]
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: Mul}{Layer Name: \gls{bn} No. 1 Scale}
  \begin{center}
    \textbf{Input affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.013497387990355492 \times (q_{\text{x}} - 53)$ \\[6pt]
    \textbf{Fused \gls{bn} multiplier $M_c$ affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.10698594152927399 \times (q_{\text{x}} + 128)$ \\[6pt]
    \textbf{Multiplier $M_1$ quantized value:} \\[2pt]
    $55$ \\[6pt]
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: Add}{Layer Name: \gls{bn} No. 1 Shift}
  \begin{center}
    \textbf{Input affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.17354172468185425 \times (q_{\text{x}} - 26)$ \\[6pt]
    \textbf{Fused \gls{bn} additive term $A_c$ affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.0339626707136631 \times (q_{\text{x}} + 10)$ \\[6pt]
    \textbf{Additive term $A_1$ quantized value:} \\[2pt]
    $-113$ \\[6pt]
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: ReLU}{Layer Name: Activation No. 1}
  \begin{center}
    \textbf{Output affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.07379141449928284 \times (q_{\text{x}} + 128)$ \\[6pt]
  \end{center}
\end{layerbox}

\clearpage
Before presenting the calculations, the Quantization Specification Table\footnote{\url{https://ai.google.dev/edge/litert/models/quantization_spec}} is reviewed.
An excerpt relevant to the \gls{tfop} \code{Conv2D} is shown below:

\begin{verbatim}
CONV_2D
  Input 0:
    data_type  : int8
    range      : [-128, 127]
    granularity: per-tensor
  Input 1 (Weight):
    data_type  : int8
    range      : [-127, 127]
    granularity: per-axis (dim = 0)
    restriction: zero_point = 0
  Input 2 (Bias):
    data_type  : int32
    range      : [int32_min, int32_max]
    granularity: per-axis
    restriction: (scale, zero_point) = (input0_scale*input1_scale[...], 0)
  Output 0:
    data_type  : int8
    range      : [-128, 127]
    granularity: per-tensor
\end{verbatim}

Using this information, quantization calculations can be retraced for a better understanding of model transformation.
The \code{input\_1} values, or the model's input tensors, are observed first from the \code{QuantizeLayer}.
As expected, these range from 0 to 1 due to normalized input patches.

The input scale and zero-point can be calculated using equations \ref{eq:scale} and \ref{eq:zeropoint} from \secshortref{subsec:quantization}:

\[
\begin{array}{rl}
\text{Input scale} &= \dfrac{1 - 0}{127 - (-128)} = 0.003921568627450980 \\[16pt]
\text{Input zero-point} &= \text{round}\!\left(-128 - \dfrac{0}{\text{Input scale}}\right) = -128
\end{array}
\]

The calculated values align with the "Input affine dequantization formula" for the \code{Conv2D} layer in the quantized model.
The precision difference at the 10th decimal place is expected due to \gls{float32} limits.
This confirms the use of affine (asymmetric) quantization for \code{Input 0},
consistent with the table's range [-128,127] and the absence of a zero-point restriction.

Moving to the \code{Conv2D} layer weights, the table specifies per-axis granularity with symmetric quantization (zero-point=0).
Inspection of the 32 kernel value ranges (only Kernel No. 1 shown here) confirms symmetry around zero.
This suggests that \gls{tf} enforces clipping of weight extremes to fit symmetric ranges.

Netron reveals that Kernel No. 1 has the largest max value among the 32 kernels (not represented here),
yet it still does not match the actual tensor extrema ($min=-0.4122757017612457, max=0.5442604422569275$),
indicating that \gls{tf} may apply additional optimization, likely outlier-aware clipping or mean adjustment,
since real values are not perfectly zero-mean ($mean=-0.005126346834471305$).
A detailed investigation of this effect lies outside the scope of this work.

Using the clipped extrema, the symmetric quantization scale is:

\[
\begin{array}{rl}
\text{Kernel No. 1 scale} &= \dfrac{0.544256329536438 - (-0.544256329536438)}{127 - (-127)}\\[8pt]
                          &= 0.004285482909735732
\end{array}
\]

This matches the Kernel No. 1 symmetric dequantization formula from the quantized model (within \gls{float32} precision limits).
Applying equation \ref{eq:dequantize}, the quantized value can be approximately dequantized back to the first element of Kernel No. 1:

\[
\begin{array}{rl}
\text{First element of Kernel No. 1} \approx 0.004285482689738274 \cdot 8 = 0.034283861517906192
\end{array}
\]

The resulting precision loss is small and can be quantified by absolute, relative, and \gls{lsb} errors:

\begin{itemize}[itemsep=0.4\baselineskip]
    \item $e_{\text{abs}} = 0.034283861517906192 - 0.032761260867118835 = 0.001522600650787357$
    \item $e_{\text{rel}} = \frac{0.001522600650787357}{0.032761260867118835} \times 100\% \approx 4.647\%$
    \item $e_{\text{lsb}} = \frac{0.001522600650787357}{0.004285482689738274} \approx 0.355,\ \Delta = 0.004285482689738274$
\end{itemize}

Here, the \gls{lsb} error $<0.5$ is a good indicator: it suggests that scale boundaries are not exceeded and quantization was applied without mismatches,
clipping errors, or encoding/decoding inconsistencies.

Looking at the bias restrictions in the Quantization Specification Table, an interesting point emerges:
the bias has no dedicated quantization scale derived from its own extrema.
Hence, no min/max values are logged by \code{QuantizeWrapperV2} for the \code{Conv2D} layer.
Instead, its scale is computed as the product of the layer's input scale and the per-channel weight scale,
yielding one bias scale per output channel (matching the weight scales per axis).
This engineering choice accelerates inference at the cost of a slight precision loss.

Multiplying input values by weights (each with its own scale) produces a new effective scale.
To add the bias to this product, the bias must be quantized using the same scale (with zero-point=0).
One could quantize the bias with its own scale and then rescale the product accordingly,
but this introduces extra operations that would significantly slow inference.
Instead, fast fixed-point arithmetic is employed, using a precomputed quantized multiplier and a bit-shift,
to reconcile the mixed scales that arise when multiplying quantized values.

\[
\begin{array}{rl}
\text{Bias No. 1 scale} &= 0.003921568859368563 \cdot 0.004285482689738274 \\
             &= 0.0000168058154634406445 \\[8pt]
\text{Bias No. 1 value} &\approx 0.000016805815903353505 \cdot 11801 \\
      &= 0.198325433475474713
\end{array}
\]

Analogous calculations can be performed for the Bias No. 1 to determine its scale and recover its approximate \gls{float32} value.

As can be seen in the Netron analysis, the \gls{bn} \gls{tfop} is decomposed into two primitive \glspl{tfop} after quantization: \code{Mul} and \code{Add}.
This follows from the affine form of batch normalization (and its folding during quantization), which can be expressed as:

\begin{equation}
y = \gamma \cdot \frac{x - \mu_{\text{moving}}}{\sqrt{\sigma^2_{\text{moving}} + \epsilon}} + \beta
\label{eq:bn}
\end{equation}

\begin{equation}
M_c = \frac{\gamma_c}{\sqrt{\sigma^2_{\text{moving},c} + \epsilon}}, 
\quad
A_c = \beta_c - \frac{\gamma_c \cdot \mu_{\text{moving},c}}{\sqrt{\sigma^2_{\text{moving},c} + \epsilon}}
      = \beta_c - M_c \cdot \mu_{\text{moving},c}
\label{eq:bnquant}
\end{equation}

\begin{itemize}
    \item \textbf{$x$}: input value to be normalized.
    \item \textbf{$y$}: output value after batch normalization.
    \item \textbf{$\mu_{\text{moving},c}$}: moving mean for channel $c$, computed during training as an exponential moving average of per-batch means; used in inference because batch statistics from a single sample are often unreliable.
    \item \textbf{$\sigma^2_{\text{moving},c}$}: moving variance for channel $c$, computed during training as an exponential moving average of per-batch variances; also used in inference for the same reason as the moving mean.
    \item \textbf{$\epsilon$}: small constant to avoid division by zero (default\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization}} \gls{tf} value is set to 0.001).
    \item \textbf{$\gamma_c$}: learnable scale parameter for channel $c$.
    \item \textbf{$\beta_c$}: learnable shift parameter for channel $c$.
    \item \textbf{$M_c$}: per-channel multiplier used in quantized inference.
    \item \textbf{$A_c$}: per-channel additive term used in quantized inference.
\end{itemize}

It should be noted that the affine dequantization of the \code{Mul} input is computed from the extrema of the outputs of \code{Conv2D} No.~1,
analogous to the previous calculations of scales and zero-points.

In quantized form, $\mu_{\text{moving}}$, $\sigma^2_{\text{moving}}$, $\gamma$, and $\beta$ are fused into per-channel multiplier $M_c$ and additive term $A_c$.
Reverse-engineering from the quantized values recovers the original \gls{float32} parameters with minimal error:

\begin{equation*}
\begin{gathered}
M_1 = 0.10698594152927399 \cdot (55 + 128) \approx 19.5784273 \\
\sqrt{\sigma^2_{\text{moving},1} + \epsilon} = \sqrt{0.00221889466047287 + 0.001} \approx 0.0567353 \\
\hat{\gamma}_1 = M_1 \cdot \sqrt{\sigma^2_{\text{moving},1} + \epsilon} \approx 19.5784273 \times 0.0567353 \approx 1.1107880 \\
\hat{\gamma}_1 = 1.1107880 \approx \gamma_1 = 1.110548973083496
\end{gathered}
\end{equation*}


\begin{equation*}
\begin{gathered}
A_1 = 0.0339626707136631 \cdot (-113 + 10) \approx -3.4981550 \\
\beta_1 = A_1 + M_1 \cdot \mu_{\text{moving},1} \approx -3.4981550 + 19.5784273 \times 0.1726332 \approx -0.1182682 \\
\beta_1 = -0.1182682 \approx \beta_{1,\text{float}} = -0.11593257635831833
\end{gathered}
\end{equation*}

Finally, the "Output affine dequantization formula" is derived from the extrema of the (ReLU) Activation No. 1 layer:

\[
\begin{array}{rl}
\text{Output scale} &= \frac{18.816810607910156 - (-3.696099329375535 \mathrm{e}{-11})}{127 - (-128)} \\
             &= 0.073791414148812224 \\[8pt]
\text{Output zero-point} &= \text{round}\left( -128 - \frac{-3.696099329375535 \mathrm{e}{-11}}{\text{Output scale}} \right) \\
                  &= -128
\end{array}
\]


All subsequent quantization steps for \glspl{tfop} in the model follow the same general procedure, adhering to predefined constraints and reusing common underlying methods.

From this deeper investigation, the following key quantization principles emerge:
\begin{itemize}
    \item After \gls{ptq}, the \code{QuantizeInput} layer defines the input scale and input zero-point based on observed extrema.
    Same calculation principles apply further.
    \item Weights are typically quantized per-axis (per-channel) with symmetric ranges.
    \item Bias scales are derived from input and weight scales, enabling fixed-point computation without extra rescaling.
    \item Certain operations, such as \gls{bn}, are decomposed into simpler arithmetic operations for inference efficiency.
    \item The precision loss is small, and \gls{lsb} error below $0.5$ indicates well-matched scales.
\end{itemize}

After quantized model is saved, the \gls{edgetpu} Compiler is invoked.

\clearpage
\subsection{Edge TPU Compiling}

Since training may be performed either on a local Windows machine or on a Linux-based cloud \gls{gpu} instance from Thunder Compute,
cross-platform automation of the pipeline is ensured by using helper functions and platform detection before invoking the \gls{edgetpu} compiler.
Because the \gls{edgetpu} compiler can only run on Debian-based Linux systems, the Windows Subsystem for Linux (WSL) is used when working on a Windows machine.

The \gls{edgetpu} Compiler parses the \code{.tflite} model, verifies that the restrictions are satisfied and that it is compatible with the \gls{edgetpu} runtime,
and then attempts to fuse all supported \glspl{tfop} into a single \code{edgetpu-custom-op}.
An example of the compiler's console output during compilation is shown below.

\lstdefinestyle{compilerlog}{
  basicstyle=\normalsize\ttfamily, % can adjust \fontsize{12pt}{13.5pt}
  captionpos=b,                    % caption at the bottom
  frame=single,                     % box around the log
  breaklines=true,
  showstringspaces=false,
  columns=fullflexible
}

\begin{lstlisting}[style=compilerlog,
    caption={Edge TPU compiler console output. Successful compilation},
    label={lst:edgetpu_compilation}]
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.

Model compiled successfully in 898 ms.

Input model: dev/results/run_20250719_170647/quant.tflite
Input size: 1.90MiB
Output model: dev/results/run_20250719_170647/quant_edgetpu.tflite
Output size: 2.07MiB
On-chip memory used for caching model parameters: 1.90MiB
On-chip memory remaining for caching model parameters: 3.82MiB
Off-chip memory used for streaming uncached model parameters: 46.00KiB
Number of Edge TPU subgraphs: 1
Total number of operations: 53
Operation log: dev/results/run_20250719_170647/quant_edgetpu.log
See the operation log file for individual operation details.
Compilation child process completed within timeout period.
Compilation succeeded!
\end{lstlisting}

The \gls{edgetpu} has \(\sim 8\,\mathrm{MiB}\) of static \gls{ram}, allowing model parameters to be cached.
It is further specified\footnote{\url{https://coral.ai/docs/edgetpu/compiler/\#parameter-data-caching}} that multiple models can be co-compiled and
stored in the \gls{edgetpu}'s on-chip memory, enabling fast switching and inference when different models are required on a single \gls{edgetpu}.
Conversely, a model can be partitioned for pipelining across multiple \glspl{edgetpu}, allowing parallel inference or acceleration of larger models.
In the scope of this thesis, however, only a single \gls{edgetpu} is used.
Therefore, the on-chip static \gls{ram} should ideally not be exceeded to preserve inference speed.
Falling back to external memory can degrade performance due to parameter reloads.

From the \autoref{lst:edgetpu_compilation} log example, useful information about memory usage can be inferred.
The model executable occupies approximately \(8 - 1.90 - 3.82 \approx 2.28\,\mathrm{MiB}\) of the \gls{edgetpu} on-chip cache.
Its parameters occupy \(1.90\,\mathrm{MiB}\), leaving \(3.82\,\mathrm{MiB}\) free, thus, the entire model is stored in the \gls{edgetpu}'s static \gls{ram}.
In \secshortref{chapter:evaluation}, memory usage is also considered when comparing different models.
For this particular model, however, \(46.00\,\mathrm{KiB}\) of external memory was still utilized despite the remaining free on-chip memory.
The reason is not documented officially and likely requires deeper investigation of the compilation process, which lies outside the scope of this thesis.

After successful compilation, a \code{.log} file is saved alongside the \gls{edgetpu} model.
This log reports, for each \gls{tfop}, whether mapping to the \gls{edgetpu} succeeded (and, if not, why),
as illustrated by the \code{.log} excerpt shown right in \autoref{fig:successcompile}.

One way to confirm that the entire model compiles for the \gls{edgetpu} is to visualize the compiled model in Netron.
An example demonstrating that every \gls{tfop} was successfully mapped to the \gls{edgetpu} is shown left in \autoref{fig:successcompile}.
In contrast to the pre-compilation graphs (for example, see \autoref{fig:qatconvertedquant}), no additional structural information is visible:
all supported \glspl{tfop} have been fused into a single \code{edgetpu-custom-op} that executes exclusively on the \gls{edgetpu}.
As shown, the input and output tensor shapes are readable, and the associated quantization scales and zero points can be inspected,
as in \secshortref{subsubsec:optquant}; no further details are exposed.

\begin{figure}[htbp]
\begin{minipage}[t]{0.26\textwidth}\vspace{0pt} % check line 552
    \centering
    % Scale the image to fit half page width while keeping aspect ratio
    \includegraphics[width=\linewidth,keepaspectratio]{files/EdgeTPUCompiledModel.pdf}
\end{minipage}%
\hspace{0.5cm plus 1fill} % if I use \hfill the overfull disappears 
\begin{minipage}[t]{0.74\textwidth}\vspace{0pt}
    \vspace*{\fill}        % push down from top
    \raggedright           % left-align text inside this block
    \small

    Edge TPU Compiler version 16.0.384591198\\
    Input: dev/results/run\_20250719\_170647/quant.tflite\\
    Output: dev/results/run\_20250719\_170647/quant\_edgetpu.tflite\\

    \vspace{0.5em}
    \begin{tabular}{@{}lrl@{}}
      Operator & Count & Status \\[0.5em]  % extra space after header row
      CONCATENATION & 3  & Mapped to Edge TPU \\
      LOGISTIC      & 1  & Mapped to Edge TPU \\
      ADD           & 14 & Mapped to Edge TPU \\
      MAX\_POOL\_2D & 3  & Mapped to Edge TPU \\
      MUL           & 14 & Mapped to Edge TPU \\
      CONV\_2D      & 15 & Mapped to Edge TPU \\
      TRANSPOSE\_CONV & 3 & Mapped to Edge TPU
    \end{tabular}
    \vspace*{\fill}        % push up from bottom -> vertical centering
\end{minipage}
\caption{\gls{edgetpu}-mapped model (left) and compiler log excerpt (right).}
\label{fig:successcompile}
\end{figure}

The final artifact is a \code{.tflite} model file ready for \gls{edgetpu} inference.

\subsection*{Technical Challenges}

The \gls{devboard} is running on an end of life version of Mendel Linux, which is the only one latest officially supported software version for \gls{devboard}.
With the OS itself comes already pre-installed runtime library \code{libedgetpu} of version v16.0, exposing a low-level C++ \gls{api}
for direct interaction with \gls{edgetpu}. Consequently, this library version is the latest officially available in Coral repository for this Mendel Linux release.
Building the latest version from source entails a more complex Bazel-based process that risks breaking dependencies and incurs a nontrivial debugging burden; 
this lies outside the scope of this thesis.

The newest as of today \gls{edgetpu} Compiler version v16.0 is used,
requiring minimum\footnote{\url{https://coral.ai/docs/edgetpu/compiler/\#compiler-and-runtime-versions}} runtime version v14.0,
which is fulfilled by \code{libedgetpu} v16.0.

At early development stages, it was unclear which \gls{tf} version was supported by the provided runtime library and compiler,
as this information was not available in the official documentation.
Consequently, the newest \gls{tf} version, v2.19.0 (at the time this work was conducted), was used heuristically for initial attempts at dataset pipeline construction,
model architecture, training, and conversion.
No \gls{qat} annotation was performed, relying solely on \gls{ptq} prior to compiling the model for trial inference on the \gls{edgetpu}.
At compile time, the following error from the \gls{edgetpu} Compiler was encountered:

\begin{lstlisting}
Edge TPU Compiler version 16.0.384591198
ERROR: Didn't find op for builtin opcode 'TRANSPOSE_CONV' version '4'.
An older version of this builtin might be supported.
Are you using an old TFLite binary with a newer model?
\end{lstlisting}

To avoid this error, the first approach was to omit the \code{TRANSPOSE\_CONV} \gls{tfop} while keeping the newest \gls{tf},
replacing deconvolution with the simpler combination \code{UpSampling2D} + \code{Conv2D}.
This may reduce accuracy but could still yield acceptable results for edge-device inference. Alternatively,
the \gls{tf} version was downgraded to v2.5.0, corresponding to the \gls{tflite} library on the \gls{devboard},
to maximize version compatibility with the target hardware. After the downgrade, the first successful compilation and inference were achieved.
Results are presented in \secshortref{sec:evaluation}.

With the introduction of \gls{qat} annotated model layers, the \gls{tfmot} package must be installed, with a version compatible with \gls{tf} v2.5.0.
This required installing \gls{tfmot} v0.6.0.
During the \gls{tflite} conversion step (while performing \gls{ptq}), the following error kept occurring:

\begin{lstlisting}
error: 'tfl.max_pool_2d' op quantization parameters violate the same
scale constraint: !quant.uniform<i8:f32, 840332.86281612806:-128>
vs. !quant.uniform<i8:f32, 0.027852464160498452:-127>
\end{lstlisting}

This error refers to the \gls{tflite} restriction (see \secshortref{subsubsec:optquant}) that the inputs and outputs of \code{MAX\_POOL\_2D} must share the same scale and \code{zero\_point},
as specified in the Quantization Specification Table\footnote{\url{https://ai.google.dev/edge/litert/models/quantization_spec}}.
As part of debugging, it was verified that the input and output minimum and maximum ranges were essentially identical. Only the mean distributions differed,
which is normal for batches with different compositions:

\noindent
\begin{minipage}[t]{0.48\textwidth}
\begin{lstlisting}[basicstyle=\ttfamily\small\centering, columns=fullflexible, keepspaces=true, frame=single]
      ---- INPUT TO MAXPOOL ----
      Shape: (1, 384, 384, 16)
      min:   0.0
      max:   3.411252021789551
      mean:  1.003772497177124
\end{lstlisting}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.48\textwidth}
\begin{lstlisting}[basicstyle=\ttfamily\small\centering, columns=fullflexible, keepspaces=true, frame=single]
      ---- OUTPUT OF MAXPOOL ----
      Shape: (1, 192, 192, 16)
      min:   0.0
      max:   3.411252021789551
      mean:  1.0165772438049316
\end{lstlisting}
\end{minipage}

Furthermore, a sanity check of the value ranges across different input batches was performed, confirming expected values from \(0\) to \(\sim 5\),
without extreme outliers, thus, there is no justification for very large scale \(840{\,}332.8\) as indicated by the error message.
Activation ranges of the preceding \code{Conv2D} layer were inspected in Netron and showed no anomalies.
These findings suggest an internal issue with this specific \gls{tfmot} version, as the observed input/output statistics contradict the scale values implied by the error message.

During debugging, it was further identified that a similar issue\footnote{\url{https://github.com/tensorflow/model-optimization/issues/1053}} had been reported for a later \gls{tfmot} version.
Combined with the observations above, this suggested trying a newer \gls{tfmot} release in which the bug might be fixed.
This raised concerns about compatibility with the older \gls{tf} v2.5.0 required for \gls{tflite} (\gls{tfop} v3) compatibility mentioned earlier.
It was decided to proceed, however, given the absence of documentation explicitly stating incompatibility of newer \gls{tfmot} with older \gls{tf}.
Multiple version combinations of these modules were evaluated.

Selecting a combination of newer \gls{tfmot} and \gls{tf} resolved the issue.
This, in turn, supports the conclusion that the error originated in the \gls{tf}/\gls{tfmot} implementation and was corrected in later releases.

Successful conversion to the \code{.tflite} format was subsequently achieved.
During conversion, the newer \gls{tf}/\gls{tfmot} toolchain automatically inserted a \code{Transpose} \gls{tfop} into the model.
This operator was not used during model design, indicating an internal optimization related to \gls{qat} handling.
A plausible explanation is the need to convert between common tensor layouts --- \code{[batch, H, W, C] (NHWC)} and \code{[batch, C, H, W] (NCHW)} --- at some point in the pipeline.
Limitations in one component's ability to process a given layout can trigger such a transformation, implemented as a \code{Transpose} placed before \code{TransposeConv}.
Below in \autoref{fig:transposeerror}, the same model architecture is shown before (left) and after (right) the \gls{edgetpu} compilation process.

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=11cm]{files/modelPartQuantEdgeTPUFail.pdf}
    \caption{Before compilation}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=11cm]{files/modelPartCustomEdgeTPUFail.pdf}
    \caption{After compilation}
  \end{subfigure}
  \caption{Inserted \code{Transpose} \gls{tfop} prevents fusion into a single \code{edgetpu-custom-op}.}
  \label{fig:transposeerror}
\end{figure}

Both the crucial console output and the \code{.log} file are provided in \autoref{lst:edgetpu_compilation1},
indicating the failed attempt to fuse multiple \glspl{tfop} into the intended \code{edgetpu-custom-op}.

\begin{lstlisting}[
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  breaklines=false,      % don't wrap—preserves column layout
  frame=single,
  xleftmargin=0pt,
  linewidth=\textwidth,  % fill the page width
  caption={Failed mapping to \gls{edgetpu}},
  label={lst:edgetpu_compilation1},
  captionpos=b
]
Model successfully compiled but not all operations are supported
by the Edge TPU. A percentage of the model will instead run on the CPU,
which is slower. If possible, consider updating your model to use only
operations supported by the Edge TPU.
For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 4
Number of operations that will run on CPU: 7
See the operation log file for individual operation details.

Logfile:
Edge TPU Compiler version 16.0.384591198
Input: quant.tflite
Output: quant_edgetpu.tflite

Operator             Count         Status
      
CONV_2D              2             More than one subgraph is not supported
CONV_2D              2             Mapped to Edge TPU
TRANSPOSE_CONV       1             Filter, bias, or other param is not
                                   constant at compile-time
LOGISTIC             1             More than one subgraph is not supported
ADD                  1             More than one subgraph is not supported
MAX_POOL_2D          1             Mapped to Edge TPU
CONCATENATION        1             More than one subgraph is not supported
TRANSPOSE            1             Tensor has unsupported rank
                                   (up to 3 innermost dimensions mapped)
QUANTIZE             1             Mapped to Edge TPU
\end{lstlisting}

This behaviour confirms that multiple layers of the model do not meet the requirements defined in \secshortref{subsec:hardware},
and it is consistent with the compilation procedure described in the official documentation\footnote{\url{https://coral.ai/docs/edgetpu/models-intro/\#compiling}}.
Specifically, the compiler stops at the first operation not supported by the \gls{edgetpu} and maps all subsequent operations to the \gls{cpu},
even if some of those later operations would otherwise be supported on the \gls{edgetpu}.
This behaviour is illustrated below in \autoref{fig:compileSeqEdgeTPU}.

\begin{figure}[htbp]
  \centering
  % Choose either a width OR a height, or both with keepaspectratio.
  % Example A: set width only (aspect ratio preserved automatically)
  \includegraphics[width=0.8\linewidth]{files/compileSequenceEdgeTPU.png}

  % Example B (recommended): limit both max width & height, keep aspect ratio
  %\includegraphics[
  %  width=0.9\linewidth,
  %  height=0.5\textheight,
  %  keepaspectratio
  %]{images/example.png}

  \caption{The compiler creates a single custom op for all \gls{edgetpu} compatible ops, until it encounters an unsupported op; the rest stays the same and runs on the \gls{cpu}}
  \label{fig:compileSeqEdgeTPU}
\end{figure}

Delegation of tensor operations to the \gls{cpu} will almost certainly increase inference time drastically, which is inappropriate here,
as the goal is to map the entire model to the \gls{edgetpu}.

Because insertion of the \code{Transpose} \gls{tfop} occurs entirely internally and automatically,
there is no practical way to influence it or to enforce layer parameters to be constant at compile time and of shape \(<3\), as required by the \gls{edgetpu} Compiler.

Two options were considered: (i) avoid \code{TransposeConv} altogether, or (ii) continue searching for compatible \gls{tf} and \gls{tfmot} versions.
Pursuing the second option places one between a rock and a hard place:
the \gls{tf} version should be kept as low as possible to maintain compatibility with the device \gls{tflite} runtime and
to avoid the previously observed \code{TRANSPOSE\_CONV} version 4 compiler error, yet sufficiently high to include the bug fix for the scale incompatibility encountered earlier.
Accordingly, the following approach was adopted: identify the highest \gls{tf} release that still uses \code{TransposeConv} version 3.
This approach assumes that any newer \glspl{tfop} introduced in later \gls{tf} versions neither cause compilation issues nor conflict with the older \gls{tflite} v2.5.0 on the inference device.
In effect, if the \gls{edgetpu} Compiler successfully fuses all \glspl{tfop} into a single \code{edgetpu-custom-op},
the \gls{tflite} runtime v2.5.0 only needs to parse and load the model before delegation to the \gls{edgetpu}.
Some compatibility uncertainty therefore remains, especially for newer (or custom) layer types, but for the purposes of this work, this assumption is considered sufficient.

In the absence of official documentation at the required level of detail for individual \glspl{tfop},
the compatible \code{TransposeConv} version was identified by inspecting the \code{register.cc} file in
the \gls{tf} repository under \code{tensorflow/tensorflow/lite/kernels} (noting that its exact location may vary slightly by version).
The newest \gls{tf} version found to register \code{TransposeConv} v3 was v2.10.0.
The corresponding \code{register.cc} lines are:

\begin{lstlisting}
AddBuiltin(BuiltinOperator_TRANSPOSE_CONV, Register_TRANSPOSE_CONV(),
           /* min_version = */ 1,
           /* max_version = */ 3);
\end{lstlisting}

It was found that the latest \gls{tfmot} v0.8.0 is backward compatible with \gls{tf} v2.10.0,
despite being tested against \gls{tf} v2.14.1, as noted on the official releases page\footnote{\url{https://github.com/tensorflow/model-optimization/releases}}.

The observed version incompatibilities presumably stem from introducing a newer \code{libedgetpu} runtime while retaining an older \gls{tflite} on the \gls{devboard},
without an official upgrade path to align versions. As demonstrated here, this can be misleading when selecting inter-compatible packages, especially when newer releases become available.

Ultimately, the combination of \gls{tf} v2.10.0 and \gls{tfmot} v0.8.0 resolved the earlier errors and enabled successful compilation,
fully mapping all \glspl{tfop} to the \gls{edgetpu}, as shown in \autoref{fig:successcompile}. This setup was used thereafter throughout the pipeline.

\section{Deployment}
\label{sec:deployment}

After training, conversion, and compilation, the \code{.tflite} model is deployed on the \gls{edgetpu} to run inference.

At early stages of development, deployment was most conveniently performed using Python utilities.
Provided by Google, these libraries encapsulate the necessary boilerplate into a small set of functions to load the model onto the \gls{edgetpu},
pass the input tensor, and retrieve the output tensor.
A simplified Python example is presented below to illustrate the ease of operating the \gls{edgetpu} with the provided helpers.

\begin{minted}{python}
from pycoral.utils.edgetpu import make_interpreter
from pycoral.adapters import common

# Load model
interpreter = make_interpreter(model_path)
interpreter.allocate_tensors()

# Prepare input
common.set_input(interpreter, input_tensor)

# Inference
interpreter.invoke()

# Output
output = common.output_tensor(interpreter, 0)
\end{minted}

Using the provided Python modules, the first inference results were obtained and are shown alongside the corresponding \gls{gt} in \autoref{fig:firstinferenceresult}.

\begin{figure}[htbp]
    \centering
    % First image
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[height=5cm]{files/FIRSTInferenceResult.png}
        \caption{Predicted cloud mask}
    \end{subfigure}
    \hfill
    % Second image
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[height=5cm]{files/FIRSTInferenceResultGT.png}
        \caption{Corresponding \gls{gt}}
    \end{subfigure}
    \caption{First \gls{edgetpu} inference results}
    \label{fig:firstinferenceresult}
\end{figure}

These initial single-patch inference results marked a milestone in this work and laid the foundation for the subsequent inference pipeline.

To simplify operation of the \gls{edgetpu} hardware, two Python libraries are provided\footnote{\url{https://coral.ai/docs/edgetpu/tflite-python/}}:
the PyCoral \gls{api} and the \gls{tflite} \gls{api}, with PyCoral wrapping repetitive \gls{tflite} inference steps into simpler helpers.

After successful implementation of single-patch inference in Python, expansion to a full-scene inference pipeline in C++ was pursued.
The main rationale was increased control over the inference process and potential acceleration.
Python helper functions can conveniently read \code{.TIF} images, transform them, and pass tensors to the \gls{edgetpu} in a single script.
However, this convenience may introduce overhead that is undesirable on an embedded system.
A more typical embedded approach was therefore adopted: transform input data on another device, perform inference on the edge device, and post-process outputs elsewhere.
An additional factor was a specific error encountered at the end of each Python inference.
Greater control over the inference sequence via C++ was expected to clarify and eliminate this issue (see \secshortref{subsec:tecchallDeployment}).

The \gls{devboard}'s Mendel Linux includes, by default, most libraries needed to operate the \gls{edgetpu} module:
the PyCoral and \gls{tflite} Python \glspl{api}, as well as the \code{libedgetpu} runtime.
The essential \gls{tflite} C++ \gls{api} is not included, however.
Moreover, the \gls{tflite} Python package cannot be reused from C++, as its binary targets Python and does not provide C++ headers.
\autoref{fig:runflow} summarizes the flow of running the model on the \gls{edgetpu} on the \gls{devboard}, highlighting the differences between Python and C++ based inference.

\begin{figure}[H]
\centering
\resizebox{\linewidth}{!}{%
\begin{tikzpicture}[
  >=Stealth, ->,
  box/.style={rectangle,draw,rounded corners,
              minimum width=1cm,minimum height=1cm,
              align=center,font=\normalsize}
]

% Shared start
\node[box] (model) {.tflite\\FlatBuffer Model};

% API branches
\node[box, right=0.5cm of model, yshift=2cm] (pyapi) {PyCoral \gls{api} (optional) or\\\gls{tflite} Python \gls{api}};
\node[box, right=0.5cm of model, yshift=-2cm] (cppapi) {\gls{tflite} C++ \gls{api}\\(headers \& library)};

% Convergence
\node[box, right=4cm of model] (interp) {\gls{tflite} Interpreter\\(C++ core)};

% Runtime + hardware
\node[box, right=1cm of interp] (runtime) {\gls{edgetpu} runtime driver\\\texttt{libedgetpu.so}};
\node[box, right=1cm of runtime] (hw) {\gls{edgetpu}\\Hardware Accelerator};

% Connections
\draw (model) -- (pyapi);
\draw (model) -- (cppapi);
\draw (pyapi) -- (interp);
\draw (cppapi) -- (interp);
\draw (interp) -- (runtime) -- (hw);

\end{tikzpicture}%
}
\caption{Running the model on \gls{edgetpu} with Python and C++}
\label{fig:runflow}
\end{figure}

Since no prebuilt \gls{tflite} C++ \gls{api} libraries were available for the \gls{devboard}, it had to be built from source.
The process is described in detail in \secshortref{subsec:tecchallDeployment}
and marks the third and final technical implementation challenge in this work.

After successfully incorporating the \gls{tflite} C++ \gls{api} on the \gls{devboard},
it was used to implement the first C++ inference in \code{inference.cpp}.
Following this, a folder-reading utility was implemented in C++ to process multiple patches sequentially.
These patches are stored as \code{.bin} files with known shape and data type (\gls{float32}).
After inference, the program writes \code{.bin} outputs containing the predicted cloud masks in \gls{float32}.
Quantization parameters for the input and output are retrieved from the model at runtime and used to quantize the input tensor and dequantize the output tensor.
Later, the stitching process was also implemented in C++.
By running \code{stitch.cpp}, all \code{.bin} patches in the input folder are stitched into a single \code{.bin} scene without gaps.
This \code{.bin} scene can then be converted to a \code{.png} for visual inspection.
This conversion can optionally be performed on the \gls{devboard} using the Python script \code{convert.py}.

Note that the entire inference, stitching, and conversion pipeline could have been consolidated into a single C++ executable.
It was intentionally kept separate for two reasons:
(i) to allow greater flexibility and lower overhead for a future performance-critical satellite implementation, and
(ii) because the persistent error described in \secshortref{subsec:tecchallDeployment} prevents running any code immediately after the inference step.

As final steps concluding the inference implementation on the \gls{devboard}, the following improvements were made.
An SD card containing prepared \code{.bin} patches for each scene is mounted on the \gls{devboard},
allowing the entire dataset (several~GB) to be hosted, otherwise infeasible on the \gls{devboard}'s internal storage.
The stitching and conversion processes were unified under a single executable: after C++ \code{.bin} stitching, a Python function is invoked to perform the conversion.
During both inference and stitch-conversion processes, the user is prompted to select one of the 20 test scenes for inference and/or conversion.
Furthermore, the Python \code{.png} conversion was improved by avoiding loading the entire output array into the \gls{devboard}'s \gls{ram}.
Instead, chunked processing is used, enabling conversion of larger full resolution scenes.

\subsection*{Technical Challenges}
\label{subsec:tecchallDeployment}

The final significant implementation milestone in this work was building the \gls{tflite} C++ \gls{api} library for the \gls{devboard} from source.
As shown in \autoref{fig:runflow}, this library is crucial for implementing pure C++ inference.

Guidance for this task is sparse in the official documentation\footnote{\url{https://coral.ai/docs/edgetpu/tflite-cpp/\#build-your-project-with-libedgetpu}}.
Taking those notes into account, a compatible \gls{tflite} version for the \code{libedgetpu} runtime preinstalled on the \gls{devboard} had to be determined.
The installed \code{libedgetpu} was identified as v16.0. However, the releases page\footnote{\url{https://github.com/google-coral/libedgetpu/releases}}
does not use a consistent numeric versioning convention, so the modification date of the \code{libedgetpu.so.1} symlink was used to approximate the release.
On the \gls{devboard} used here this date was 09.07.2021,
which predates the official “Grouper” \code{libedgetpu} release\footnote{\url{https://github.com/google-coral/libedgetpu/tree/release-grouper}} (26.07.2021) by two weeks.
From that release's \code{workspace.bzl}, the exact \gls{tf} commit corresponding to \gls{tf}~2.5.0 (05.07.2021) was identified.

The first build attempt was performed on a Windows machine via cross-compilation for \code{aarch64} Linux.
Although compilation succeeded, the first inference test failed due to low-level incompatibilities with the \gls{devboard}'s atomic instructions.
It was therefore concluded that the library must be built either with Bazel using a full \gls{devboard} configuration or directly on the device.
The latter was chosen. An SD card was used to host the large \gls{tf} source tree (specific v2.5.0 commit) and to provide swap space,
to avoid running out of \gls{ram} during compilation. The build was performed using the then-latest portable \code{CMake}.

After successfully building the static library \code{libtensorflow-lite.a}, several additional steps were required.
The \code{edgetpu.h} header\footnote{\url{https://github.com/google-coral/libedgetpu/blob/master/tflite/public/edgetpu.h}} and a FlatBuffers library (v1.12.0) compatible with the build were obtained,
and all required \gls{tflite} static libraries were included.
The necessary include paths and libraries are recorded in the repository and can also be reviewed in the \code{MAKEFILE}.

Once compiler and linker issues were resolved and all requirements satisfied, successful model loading and tensor allocation using the C++ \gls{api}
marked the final technical milestone of this work.

\subsubsection*{Open Issue}

In the scope of this work, and with regard to the specific \gls{devboard} with its installed Mendel Linux OS and modules, an unusual behaviour after inference was observed.
From early development through the final C++ inference improvements, one of two errors persistently appeared only after the inference had completed:
\code{Segmentation fault} or \code{Bus error}.
The phenomenon was first encountered during the simple proof-of-concept Python inference, at program termination after results had been successfully saved.
Although such errors are typically disastrous, their occurrence at shutdown had no effect on obtaining the required inference outputs.
As they clearly originated in C++ backend code, debugging from Python was not feasible; nonetheless, pipeline development proceeded unaffected.

The move to a pure C++ inference was motivated in part by the desire to eliminate these errors.
However, even after the first successful C++ results, the errors continued to alternate upon exiting \code{main()} in \code{inference.cpp}.
During debugging, the source was narrowed down to the \code{libedgetpu} library --- presumably during its cleanup routine after inference.
Exact lines could not be traced, as the provided \code{libedgetpu} was built without debugging symbols.
Diagnosing this would require rebuilding the library with debug information, a process likely to entail dependency issues and other challenges typical of custom low-level builds.
This appears to be a worthwhile line of investigation but lies beyond the scope of this thesis.
It is presumed that the issue was fixed in later \code{libedgetpu} releases\footnote{\url{https://github.com/tensorflow/tensorflow/issues/62371}},
but without an official upgrade path on the device, a custom rebuild would have been inevitable.

Work was therefore continued, as the errors had no impact on performance or results.
Multiple \code{Interpreter} invocations can be executed to process folders of files. However, no code can be run after inference completes, because the program crashes at exit.
Consequently, the stitching and conversion stages were implemented as separate programs.

\section{Evaluation Pipeline}
\label{sec:evaluation}

With the implementation of the deployment pipeline on the \gls{devboard}, cloud prediction masks can be obtained.
Visual inspection by a human observer is used to qualitatively confirm successful implementation and attainment of the thesis objective.
Furthermore, a quantitative performance metric was implemented in \code{inference.cpp} on the \gls{devboard} to determine the inference time per processed patch;
by summing these times, the model's per-scene inference time is obtained.

The flexibility of the training and deployment pipelines allows convenient, modular improvement and feature expansion.
They can be tailored to the needs and requirements of model development intended for immediate satellite deployment.
For this, additional quantitative evaluation of model performance is necessary. The results are ultimately used in \secshortref{chapter:evaluation},
where the performance of several \gls{edgetpu} optimized models is assessed.

The evaluation pipeline is implemented as an additional stage following the training pipeline and is run on the training machine, not on the \gls{devboard}.
The evaluation pipeline consists of two parts: inference and evaluation.

\subsection*{Inference}

The necessary functions to perform inference and save results are implemented in the files \code{inference.py} and \code{inferenceQ.py}.
\code{inferenceQ.py} uses the \gls{tflite} \code{Interpreter} with the fully \gls{int8} quantized \code{.tflite} model,
whereas \code{inference.py} relies on \gls{tf} utilities and loads the \code{.h5} saved-model format.
It was observed that running the quantized model on a \gls{cpu} results in significantly slower inference than the \gls{float32} \code{.h5} model, which slows down the evaluation.
Moreover, executing the quantized model on a general-purpose \gls{cpu} Intel Core i5-13400F yielded inference times even slower than deployment on the dedicated \gls{edgetpu}.
This inherently emphasizes the sophistication of the \gls{tpu} in performing these operations, compared to a general-purpose \gls{cpu} that is otherwise more powerful for general tasks.
Consequently, quantized inference was omitted during evaluation, and \code{inference.py} is used for the remainder.

First, the \code{buildDS} function is used to construct the testing dataset.
As mentioned in \secshortref{subsec:testing}, prebuilt \code{.csv} files are used to assemble either a single-scene dataset (specified by ID or chosen at random) or the full test set.
In later versions of \code{inference.py}, full-dataset inference is handled internally on a scene-wise basis, reducing \gls{ram} usage and preventing crashes.
Accordingly, \code{fullTestDS.csv} is no longer used.
Next, the path to the desired \code{run} folder (containing the \code{.h5} and \code{.tflite} files) is specified.
In \code{inference.py} the \code{.h5} model is loaded, providing any required custom \gls{tf} objects (e.g., \code{softJaccardLoss}, \code{diceCoefficient}).

If model predictions are produced at reduced resolution (e.g., \(\,192\times192\,\) patches),
they are upsampled to the full \(\,384\times384\,\) resolution to match full-scene \glspl{gt} during evaluation.
Upsampling is performed with \code{bilinear} interpolation (see also \secshortref{sec:data}),
which is appropriate for continuous probability maps.

Finally, the inference results are saved per scene as \code{.npy} files for use in the evaluation process.
Optionally, masks can also be saved as \code{.png} files for visual inspection.

\subsection*{Evaluation}

After obtaining inference results for the complete test set (20 scenes), they are quantified using the evaluation metrics defined in \secshortref{subsec:evalmetrics},
as implemented in \code{evaluate.py} file.
By specifying the \code{run} folder, that contains the \code{evaluation/inference} subfolder,
the algorithm locates and processes each scene's \code{.npy} file produced by the inference step.

As mentioned in \secshortref{subsec:dataset}, the test input scenes were cropped and padded to fit the \(\,384\times384\,\) patch format.
This introduces zero padding at the scene borders. To restore the original scene size of the \gls{gt} masks, the border padding must be removed.
This is performed by center-cropping the prediction masks via the helper function \code{unpadToMatch},
following the procedure used in the original dataset papers \cite{CloudNet2019, CloudDet2018} (cf. their \code{unzeropad} routine used
for evaluation\footnote{\url{https://github.com/SorourMo/38-Cloud-A-Cloud-Segmentation-Dataset/blob/master/evaluation/evaluation.m}}).

Next, the evaluation pipeline offers two ways to choose the threshold (see \secshortref{subsec:evalmetrics}) for binarizing the predicted masks:
(i) apply the best threshold estimated on the validation set, or
(ii) determine the best threshold per test scene using its \gls{gt} mask.
The effects of these choices are discussed in the next \secshortref{chapter:evaluation}.
This functionality is controlled via the optional \code{fixedThreshold} argument to \code{evaluateAll} function.

As a final step, the prediction masks are binarized and metrics are computed using \code{computeMetrics} function.
Results are saved to \code{metricsValThr.csv} or \code{metricsTestThr.csv}, depending on the thresholding variant.
Optionally, per-scene \code{.npy} results can be saved alongside \code{.png} masks for visual inspection.

This process concludes the \emph{Implementation} chapter and finalizes the software development conducted within the scope of this work.

}