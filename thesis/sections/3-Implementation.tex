{

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\chapter{Implementation}
\label{chapter:implementation}

The previous chapter outlined the design and rationale for each system component. The following sections describe the practical realization, with a focus on engineering decisions, code-level implementation, and hardware-specific challenges.

\todo{MILESTONES and difficuilties}

3 milestones.
1. Explain libedgtpu + tflite. How difficuilt it was to get the right TF version
2. Seting up thunder compute instances, installing the right cuda
3. C++ inference on coral board. Building the tflite library from source with the right TF version. 


\section{Data}
\label{sec:data}

\subsection{Training \& Validation}

In order to construct the training and validation pipeline, the function \code{BuildDS} was implemented, along with several supporting helper functions, all located in the \code{load.py} file.
The function was later extended to optionally include the test dataset, which will be discussed in \secshortref{subsec:testing}.

Efficient data handling and memory management are achieved through the use of built-in \gls{tf} utilities.
In particular, the \code{tf.Data.TextLineDataset} function~\cite{tfTextLineDataset} is employed to sequentionally read the \code{.csv} files,
which contain the patch filenames as outlined in \secshortref{subsec:dataset}.
This provides the foundation for the dataset pipeline.

The complete training set originally contains 8400 patches. However, as explained earlier in \secshortref{subsec:dataset}, some patches are entirely zero-valued.
Only 5155 of them contain valid data and are thus retained for subsequent use.
The dataset is shuffled and split into training and validation subsets, with the ratio configurable as needed.

Using the \code{.map} method, each text line is first expanded into five full paths to the corresponding \gls{rgb}, \gls{nir} and \gls{gt} mask patches.
Each filepath is then replaced by its image content, loaded as \code{tf.Tensor}~\cite{tfTensor}.
This transformation is handled by the helper function \code{loadDS}, which itself calls utility function \code{loadTIF}.
At this stage, each dataset element is a tuple of \gls{tf} tensors representing the four-channel input image and its corresponding \gls{gt} mask.

The \code{loadDS} and \code{loadTIF} functions perform the following operations:

\begin{itemize}
    \item RGB and NIR patches are cast to \gls{float32} and normalized to the range [0,1].
    \item \gls{gt} masks, originally in \gls{uint8}, are binarized to values of 0 and 1, and also cast to \gls{float32}.
\end{itemize}

An additional feature of the \code{loadDS} function allows for optional resizing of the input images if a target size is provided.
The image loading and transormation pipeline is designed to avoid information loss until the controlled resizing step, where a reduction in resolution is intentional.
In the case of resizing, \gls{gt} masks are resized using nearest-neighbor interpolation. This method copies the value of the closest original pixel for each target pixel,
thus preserving hard edges in the cloud segmentation mask and avoiding the introduction of intermediate values.
Conversely, the \gls{rgb} and \gls{nir} inputs are resized with bilinear interpolation,
which computes each new pixel value as a weighted average of the nearest $2\times2$ neighborhood. This results in smooth pixel transitions while maintaining important image details.
Nearest-neighbor interpolation is among the eraliest digital image resizing techniques, dating back to the origins of digital image processing in the 1960s,
whereas bilinear interpolation gained prominence in early computer graphics literature and is now standard in deep learning pipelines~\cite{bilinearNearest1, bilinearNearest2}.

As a final preparation step, the dataset is: shuffled, batched, set to repeat indefinitely,
and prefetched to optimize data retrieval during training.
Because the datasets are repeated indefinitely, it is essential to define the number of steps required to complete one full pass through the training and validation subsets.
These step counts are computed as follows: \ensuremath{trainSteps = trainSubsetSize / batchSize}, \ensuremath{valSteps = valSubsetSize / batchSize},
and are used to ensure the correct number of iterations per epoch during training.

\todo{adjust formula}
\todo{maybe add code snippets}

\subsection{Testing}
\label{subsec:testing}

An additional capability of the \code{buildDS} is the construction of the test dataset.
As outlined in \secshortref{subsec:dataset}, only cropped \gls{rgb} and \gls{nir} test patches, together with complete scene \gls{gt} masks, are available for testing.
This arrangement necessitates two specific preparation steps prior to building the test pipeline:

\begin{enumerate}
    \item \textbf{Metadata collection:} To uniquely identify each scene, the \code{sceneID} is introduced, derived from the Landsat 8 metadata (path/row),
    and is unique within this test dataset. The total number of patches, as well as the number of rows and columns for each scene, is collected for every \code{sceneID}.
    The patch filenames, as outlined in \secshortref{subsec:dataset}, are used for this process.
    Additional \code{.csv} files were manually created --- each containing the ordered patch filenames corresponding to every full scene.
    Filenames in these \code{.csv} files are organized in the order resulting from cropping (left to right, top to bottom).
    Furthermore, the \code{fullTestDS.csv} file was generated, containing ordered patch filenames for all 20 full scenes.
    These \code{.csv} files, not provided in the original dataset,
    were developed in the scope of this thesis to support the test pipeline and are stored in the \code{additionalCSVs} folder within the testing subset.
    Based on this manually collected metadata, the \code{getSceneGridSizes} function was implemented in \code{load.py} file,
    returning a dictionary that maps each of the 20 \code{sceneID}s to the respective number of rows and columns in each scene.
    \item \textbf{Patch stitching:} The \code{stitchPatches} function was implemented in \code{load.py}.
    This function uses the \code{.csv} files and the \code{getSceneGridSizes} function to reconstruct entire scenes from the model's output patches for evaluation.
    The function can operate in two modes: it can either stitch together all scenes at once, saving all 20 full scenes,
    or stitch only a single specified scene. The rationale behind this design choice will be explained in the following paragraph.
\end{enumerate}
\todo{maybe move some content to design, especially with stitching etc. Why whole DS and only 1 scene. Add additionalCSVs in folder structure,
mention it there and reference to that chapter or to design.}
\todo{explain maybe in design why whole DS and why only 1 scene}

Following these preparatory steps, the \code{buildDS} function was extended with additional functionality to construct the test subset.
Using the optional boolean parameter \code{includeTestDS}, which indicates whether to include the test dataset, and the parameter \code{singleSceneID},
which allows the selection of a specific scene, the function now supports multiple modes for testing:

\begin{itemize}
    \item Utilizing all 9201 patches from the 20 test scenes, or
    \item Selecting patches from a single scene, either by specifying its \code{singleSceneID} or allowing the function to randomly select one.
    %based on pre-generated \code{.csv} files that map patch names to \code{sceneID}.
\end{itemize}

The dataset is then constructed using the same \gls{tf} utilities employed for the training and validation subsets.
It is important to emphasize that the test set is not shuffled, as preserving the initial patch order is essential for the subsequent stitching process.

Consequently, the \code{buildDS} function returns the training and validation subsets, along with their respective step counts based on the \code{batchSize}.
Optionally, it also returns test subset, which may contain either the entire test dataset or a single scene.
After inference, the model's output patches can be passed to the \code{stitchPatches} function, which reconstruct and saves the final scene images for further evaluation.

\section{Model}
\label{sec:model}

The core building blocks and utility functions for architecting the model are organized in \code{model.py}.
\gls{cnn} layers are constructed using \gls{tf}'s built-in \glspl{api}, allowing for modular and reusable design.

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{l X l l}
\textbf{Model Name} & \textbf{Description} & \textbf{Parameters} & \textbf{Input Size} \\
\hline
BaseNet      & Minimal CNN for proof-of-concept 2 conv layers, 1 dense & 300,000 & 192×192×4 \\
MediumNet    & Deeper CNN; 4 conv, 2 dense, dropout, QAT-ready          & 2,000,000 & 192×192×4 \\
LargeNet     & U-Net variant; skip connections, batch norm, QAT         & 30,000,000 & 192×192×4 \\
\end{tabularx}
\caption{Overview of Model Architectures Used in This Work}
\label{tab:model-architectures}
\end{table}


At early development stages, the model architecture \code{simple} was implemented.
It served as proof-of-concept and was used for initial debugging of training and conversion pipeline as well as to get the first inference results on \gls{devboard}.
The model was not annotaded for \gls{qat}.

Below are examples of layers of two models, the left one is not annotaded for \gls{qat}, the right one is annotaded for \gls{qat}.
It is visualized using Netron\footnote{\url{https://netron.app/}}.
As outlined in \secshortref{subsubsec:qat}, the \code{QuantizeWrapperV2} and \code{QuantizeLayer} \glspl{tfop} are inserted in the architecture,
wrapping the base layers with quantization-dequantization operations.

\todo{explain why quantized layer after input. Explain why youre putting this example here, why its important to step from regular model to qat annotated model}

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPart.pdf}
    \caption{Links}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPartQ.pdf}
    \caption{Rechts}
  \end{subfigure}
  \caption{Zwei SVGs nebeneinander (als PDF)}
\end{figure}

By taking a closer look it has to be mentioned, that in \gls{qat} model the \code{QuantizeLayer} is insertded in order to quantize input tensors.
The function Conv2D is wrapped into quantizedWrapper ...\todo{explain all basics, to be continued in Conversion}
It is important to emphasize, that the \gls{bn} \gls{tfop} is inserted after \code{Conv2D} and before \code{Activation} \glspl{tfop} \cite{batchnormActivation}.

Later on, a more complex, already \code{uNet} architecture was implemented, already involving more, deeper layers and implementing skip connections.
Early versions of this model architecture were not \gls{qat} annotaded, only \gls{ptq} was used at conversion time.
Below 

\code{uNet} model already represents the deep \code{uNet} architecture with \dots, model layers, however, are not annotaded for \gls{qat}.

\todo{explain different architecture functions here, maybe even all of them. Mention \gls{qat} annotation} 

Additionally, where standard \gls{tf} loss functions are insufficient, custom loss functions such as \code{softJaccardLoss} or \code{diceLoss} are implemented in model architectures.
Custom metrics, particularly relevant for evaluation of segmentation tasks are also implemented within this module.
These metrics include\dots
\todo{describe metrics}

\todo{mention binary crossentropy compared to softJaccardLoss! Maybe this in evaluation.}

Below, are shown the examples of the parts of uNet \gls{cnn}

\section{Training}
\label{sec:training}

The complete model training and conversion pipeline is implemented in the \code{main.py} file.

The process begins with configuration settings, which include:
\begin{itemize}
\item Batch size for training
\item Image size for both training and inference
\item Number of training epochs
\item Selected model architecture
\item Ratio between validation and training subsets
\item Number of batches used for the calibration dataset
\end{itemize}
Additional settings can be incorporated as needed.

The pipeline starts by loading the dataset using functions from \code{load.py}.
The selected model architecture is then compiled utilizing utilities from \code{model.py}.

Each training run is stored in a dedicated folder, named with a timestamp corresponding to the start of the run.
This folder contains all configuration files, training-related artifacts, and final results, including the compiled \gls{edgetpu} model.

Prior to training, the following configurations and callbacks are set up:

\begin{itemize}
\item \textbf{Model Checkpoints}: Model checkpoints are saved during training whenever the validation loss improves, preserving the best-performing model.
\item \textbf{Early Stopping}: If the validation loss stops improving, training continues for a predefined number of additional epochs before termination.
\item \textbf{Learning Rate Reduction}: The learning rate is reduced if the validation loss does not improve for a set number of epochs, helping to fine-tune convergence.
\end{itemize}

Once all configurations are saved, model training is initiated with the specified callbacks active.

Upon completion of training, the final model weights are saved and immediately used for model conversion.

\subsection*{Technical Challenges}

Training of initial proof-of-concept models was performed on a personal workstation equipped with an Intel Core i5-13400F CPU and an NVIDIA GeForce RTX 4060 GPU.
To enable GPU acceleration for the \gls{tf} training process, the NVIDIA \gls{cuda} \gls{api} and the corresponding \gls{cudnn} library must be installed and configured.
The following versions were used: \gls{cuda} v11.2 and \gls{cudnn} v8.1.1.
These versions are not the latest available from NVIDIA at the time of this work, but were selected for compatibility with certain \glspl{tfop} and the \code{libedgetpu} runtime library.
Further details on these compatibility considerations are discussed in \secshortref{sec:conversion} and \secshortref{sec:deployment}.

Initial models containing approximately 300,000 parameters were trained without \gls{qat} for tens of epochs.
On the described setup and using the full training dataset, the time required for a single epoch was less than 15 seconds, which was deemed acceptable.

\todo{double check parameters number}

With the introduction of more complex models containing 2,000,000 and 30,000,000 parameters, and when employing \gls{qat}, training times increased significantly.
For these larger models, one epoch on the full training dataset required approximately 3 minutes and 12 minutes, respectively.
Since at least 100 epochs were necessary to achieve adequate model performance, more powerful hardware resources became essential.

Thunder Compute, an American startup providing GPU cloud resources for machine learning and data science, was utilized for large-scale experiments \cite{thundercompute}.
By using an instance with an NVIDIA A100XL GPU, training times for the 2,000,000 parameter network were reduced to approximately 20 seconds per epoch,
and for the 30,000,000 parameter network to 40 seconds per epoch.
A \ensuremath{10\times} increase in GPU memory, to 80GB on the A100XL compared to 8GB on the RTX 4060, also enabled larger batch sizes,
resulting in more effective training and improved model robustness.

However, significant effort was required to configure the cloud instance for compatibility.
The more expensive production mode had to be used, as the prototyping mode did not allow downgrading \gls{cuda} and \gls{cudnn} versions \cite{thundercomputeProtProd}.
The \gls{cuda} and \gls{cudnn} libraries needed to be downgraded from their most recent platform versions to those compatible with an older \gls{tf} version.
This was challenging, as it required purging the latest \gls{cuda} and \gls{cudnn} files without removing the GPU drivers required by the older versions.
Ultimately, \gls{cuda} v... and \gls{cudnn} v... were installed on the instance for subsequent training.
In addition, the Thunder Compute development team was contacted with a suggestion to allow selection of \gls{cuda} and \gls{cudnn} versions at instance creation.
As a result, an improvement is planned that will allow passing the desired \gls{cuda} and \gls{cudnn} version as an environment variable during instance startup.

\section{Conversion}
\label{sec:conversion}

The file \code{convert.py} contains all necessary functions and utilities for model conversion and \gls{edgetpu} compilation.
These functions are called from \code{main.py} immediately after training is complete.

\subsection*{Weight Transfer}

As outlined in \secshortref{subsec:hardware}, the \gls{edgetpu} supports input tensors with at most three dimensions.
This necessitates the use of batch size one, since the input tensor shape is already \ensuremath{image~height~\times~image~width~\times~number~of~channels}.
During training, larger batch sizes are typically used for efficiency.
Therefore, after training, the function \code{asBatchOne} is used to transfer the trained weights to a model with identical architecture,
but with batch size set to one for the input, all intermediate computations, and the output.

\subsection*{Post-Training Quantization}

The \code{tf.lite.TFLiteConverter}\footnote{\url{https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter}} utility is used to perform \gls{ptq} and convert the model to
the \code{.tflite} format.
A representative dataset is generated using the \code{representativeDatasetGen} function, consisting of training \gls{rgb} and \gls{nir} patches.
The number of calibration batches used can be adjusted via \code{numCalBatches}, as defined in \secshortref{sec:training}.
The greater the amount of calibration data, the more likely the observed value range will closely match the inference data, improving quantization accuracy.

Next, all supported \glspl{tfop}, as well as model inputs and outputs, are quantized.
While various data types can be used for quantization in general, this work uses only \gls{int8} or \gls{uint8}, as required for \gls{edgetpu} compatibility (see \secshortref{subsec:hardware}).

The following subchapter provides a deeper investigation into quantization of model parameters by analyzing a \gls{qat}-annotated \gls{float32} model and its quantized version, using the Netron tool\footnote{\url{https://netron.app/}}.
This chapter is aimed at readers who wish to deepen their understanding of parameter quantization and gain insight into the general techniques behind full-integer operation execution on edge devices such as the \gls{edgetpu}.

It should be noted that both \gls{qat} annotation and \gls{ptq} are already wrapped into convenient, ready-to-use \gls{tflite} commands. Thus, this section is not intended to teach the implementation of quantized models from scratch. Instead, it aims to offer a deeper understanding that can be beneficial for debugging, designing custom (potentially more powerful) models, or simply appreciating the engineering trade-offs between speed, memory, and accuracy on constrained devices.

\subsubsection*{Practical example of Model quantization}
\label{subsubsec:optquant}

Below, part of the architecture of a \code{.tflite} model after \gls{ptq} (right) is shown, alongside the same \gls{qat}-annotated model (left) before conversion.

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPartQBeforePTQ.pdf}
    \caption{Links}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPartQAfterPTQ.pdf}
    \caption{Rechts}
  \end{subfigure}
  \caption{Zwei SVGs nebeneinander (als PDF)}
\end{figure}

Netron also provides detailed information about graph nodes (model layers) and their connections (tensor passes). For example, layer weights and biases can be directly inspected, and values can be examined.
In the tables below, quantization-relevant information is displayed for further analysis. For both the \gls{qat} annotated model and the resulting quantized model, the relevant information for four layers is collected.

% Example usage
\subsubsection*{\gls{qat} Model (left)}

\begin{layerbox}{Layer Type: QuantizeLayer}{Layer Name: Quantize Input}
  \begin{center}
    \textbf{Input value range:} \\[2pt]
    $\min = 0 \quad\quad \max = 1$
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: QuantizeWrapperV2}{Layer Name: Conv2D No. 1}
  \begin{center}
    \textbf{Kernel shape:} \\[2pt]
    $[3, 3, 4, 32]$ \\[6pt]
    \textbf{Kernel No. 1 value range:} \\[2pt]
    $\min = -0.544256329536438 \quad\quad \max = 0.544256329536438$ \\[6pt]
    \textbf{First element of Kernel No. 1 value:} \\[2pt]
    $0.032761260867118835$ \\[6pt]
    \textbf{Bias shape:} \\[2pt]
    $[32]$ \\[6pt]
    \textbf{Bias No. 1 value:} \\[2pt]
    $0.1983194351196289$ \\[6pt]
    \textbf{Activation value range:} \\[2pt]
    $\min = -2.4379329681396484 \quad\quad \max = 1.0039010047912598$
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: \glsxtrlong{bn}}{Layer Name: \glsentryshort{batchnorm} No. 1}
  \begin{center}
    \textbf{Shape of $\gamma$, $\beta$, $\mu_{\text{moving}}$, $\sigma^2_{\text{moving}}$ parameters:} \\[2pt]
    $[32]$ \\[6pt]
    \textbf{Parameter $\gamma_1$ value:} \\[2pt]
    1.110548973083496 \\[6pt]
    \textbf{Parameter $\beta_1$ value:} \\[2pt]
    -0.11593257635831833 \\[6pt]
    \textbf{Parameter $\mu_{\text{moving},1}$ value:} \\[2pt]
    0.17263321578502655 \\[6pt]
    \textbf{Parameter $\sigma^2_{\text{moving},1}$ value:} \\[2pt]
    0.00221889466047287
  \end{center}
\end{layerbox}


\begin{layerbox}{Layer Type: QuantizeWrapperV2}{Layer Name: Activation No. 1}
  \begin{center}
    \textbf{Output value range:} \\[2pt]
    $\min = -3.696099329375535 \mathrm{e}{-11} \quad\quad \max = 18.816810607910156$
  \end{center}
\end{layerbox}

\bigskip
\subsubsection*{\gls{ptq} Model (right)}

\begin{layerbox}{Layer Type: Conv2D}{Layer Name: Conv2D No. 1}
  \begin{center}
    \textbf{Input affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.003921568859368563 \times (q_{\text{x}} + 128)$ \\[6pt]
    \textbf{Kernel No. 1 symmetric dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.004285482689738274 \times q_{\text{x}}$ \\[6pt]
    \textbf{First element of Kernel No. 1 quantized value:} \\[2pt]
    $8$ \\[6pt]
    \textbf{Bias No. 1 symmetric dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.000016805815903353505 \times q_{\text{x}}$ \\[6pt]
    \textbf{Bias No. 1 quantized value:} \\[2pt]
    $11801$ \\[6pt]
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: Mul}{Layer Name: \gls{bn} No. 1 Scale}
  \begin{center}
    \textbf{Input affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.013497387990355492 \times (q_{\text{x}} - 53)$ \\[6pt]
    \textbf{Fused \gls{bn} multiplier $M_c$ affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.10698594152927399 \times (q_{\text{x}} + 128)$ \\[6pt]
    \textbf{Multiplier $M_1$ quantized value:} \\[2pt]
    $55$ \\[6pt]
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: Add}{Layer Name: \gls{bn} No. 1 Shift}
  \begin{center}
    \textbf{Input affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.17354172468185425 \times (q_{\text{x}} - 26)$ \\[6pt]
    \textbf{Fused \gls{bn} additive term $A_c$ affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.0339626707136631 \times (q_{\text{x}} + 10)$ \\[6pt]
    \textbf{Additive term $A_1$ quantized value:} \\[2pt]
    $-113$ \\[6pt]
  \end{center}
\end{layerbox}

\begin{layerbox}{Layer Type: Relu}{Layer Name: Activation No. 1}
  \begin{center}
    \textbf{Output affine dequantization formula:} \\[2pt]
    $x_{\text{q}} \approx 0.07379141449928284 \times (q_{\text{x}} + 128)$ \\[6pt]
  \end{center}
\end{layerbox}

Before diving into the calculations, let's examine the Quantization Specification Table\footnote{\url{https://ai.google.dev/edge/litert/models/quantization_spec}}. Below is the excerpt relevant to the \gls{tfop} \code{Conv2D}:

\begin{verbatim}
CONV_2D
  Input 0:
    data_type  : int8
    range      : [-128, 127]
    granularity: per-tensor
  Input 1 (Weight):
    data_type  : int8
    range      : [-127, 127]
    granularity: per-axis (dim = 0)
    restriction: zero_point = 0
  Input 2 (Bias):
    data_type  : int32
    range      : [int32_min, int32_max]
    granularity: per-axis
    restriction: (scale, zero_point) = (input0_scale*input1_scale[...], 0)
  Output 0:
    data_type  : int8
    range      : [-128, 127]
    granularity: per-tensor
\end{verbatim}

Using this information, we can retrace quantization calculations for a better understanding of model transformation. We start with the \code{input\_1} values — the model's input tensors — as observed in the \code{QuantizeLayer}. As expected, these range from 0 to 1 due to normalized input patches.

The input scale and zero-point can be calculated using equations \ref{eq:scale} and \ref{eq:zeropoint} from \secshortref{subsec:quantization}:

\begin{equation*}
\text{scale} = \frac{1 - 0}{127 - (-128)} = 0.003921568627450980
\end{equation*}

\begin{equation*}
\text{zero-point} = \text{round}\left( -128 - \frac{0}{\text{scale}} \right) = -128
\end{equation*}

The calculated values align with the Input affine dequantization formula for the Conv2D layer in the \gls{ptq} model. The minor precision difference at the 10th decimal place is expected due to \gls{float32} limits. This confirms the use of affine (asymmetric) quantization for \code{Input 0}, consistent with the table's range [-128,127][-128,127] and the absence of a zero-point restriction.

Moving to the Conv2D layer weights, the table specifies per-axis granularity with symmetric quantization (zero-point=0zero-point=0). Inspection of the 32 kernel value ranges (only Kernel No. 1 shown here) confirms symmetry around zero. This suggests that TensorFlow enforces clipping of weight extremes to fit symmetric ranges.

Netron reveals that Kernel No. 1 has the largest max value among the 32 kernels, yet it still doesn't match the actual tensor maximum (-0.4123-0.4123 to 0.544260.54426), indicating that TensorFlow may apply additional optimization — likely outlier-aware clipping or mean adjustment — since real values are not perfectly zero-mean but the quantization requires zero-point =0=0. A detailed investigation of this effect lies outside the scope of this work.

Using the clipped extrema, the symmetric quantization scale is:

\begin{equation*}
\text{scale} = \frac{0.544256329536438 - (-0.544256329536438)}{127 - (-127)} = 0.004285482909735732
\end{equation*}

This matches the Kernel No. 1 symmetric dequantization formula from the \gls{ptq} model table (within float precision limits).
Applying equation \ref{eq:dequantize}, the first quantized element of Kernel No. 1 can be approximately dequantized to:

\begin{equation*}
x_{q} \approx 0.004285482689738274 \cdot 8 = 0.034283861517906192
\end{equation*}

The resulting precision loss is small and can be quantified by absolute, relative, and \gls{lsb} errors:

\begin{itemize}
    \item $e_{\text{abs}} = 0.03428386151790619 - 0.032761260867118835 \approx 0.0015226006507873535$
    \item $e_{\text{rel}} = \frac{0.0015226006507873535}{0.032761260867118835} \times 100\% \approx 4.645\%$
    \item $e_{\text{lsb}} = \frac{0.0015226006507873535}{0.004285482689738274} \approx 0.355,\ \Delta = 0.004285482689738274$
\end{itemize}

Here, the \gls{lsb} error <0.5 is a good indicator: it suggests that scale boundaries are not exceeded and quantization was applied without mismatches, clipping errors, or encoding/decoding inconsistencies.

Bias quantization in Conv2D layers is particularly interesting: the specification shows no independent bias scale. Instead, the bias scale is derived from the product of the input scale and the corresponding per-channel weight scale. This avoids costly rescaling operations during inference, allowing bias addition to be performed directly in fixed-point arithmetic — an elegant speed-accuracy trade-off.

\begin{equation*}
\text{scale} = 0.003921568859368563 \cdot 0.004285482689738274 = 0.000016805815463440644535387373480262
\end{equation*}

\begin{equation*}
x_{q} \approx 0.000016805815903353505 \cdot 11801 = 0.198325433475474712505
\end{equation*}

The \gls{bn} \gls{tfop} is split into two simpler operations (Mul and Add) after quantization. This reflects the mathematical structure of batch normalization:

\begin{equation}
y = \gamma \cdot \frac{x - \mu_{\text{moving}}}{\sqrt{\sigma^2_{\text{moving}} + \epsilon}} + \beta
\label{eq:bn}
\end{equation}

\begin{equation}
M_c = \frac{\gamma_c}{\sqrt{\sigma^2_{\text{moving},c} + \epsilon}}, 
\quad
A_c = \beta_c - \frac{\gamma_c \cdot \mu_{\text{moving},c}}{\sqrt{\sigma^2_{\text{moving},c} + \epsilon}}
      = \beta_c - M_c \cdot \mu_{\text{moving},c}
\label{eq:bnquant}
\end{equation}

\begin{itemize}
    \item $x$ — input value to be normalized.
    \item $y$ — output value after batch normalization.
    \item $\mu_{\text{moving},c}$ — moving mean for channel $c$, computed during training as an exponential moving average of per-batch means; used in inference because batch statistics from a single sample are often unreliable.
    \item $\sigma^2_{\text{moving},c}$ — moving variance for channel $c$, computed during training as an exponential moving average of per-batch variances; also used in inference for the same reason as the moving mean.
    \item $\epsilon$ — small constant to avoid division by zero.
    \item $\gamma_c$ — learnable scale parameter for channel $c$.
    \item $\beta_c$ — learnable shift parameter for channel $c$.
    \item $M_c$ — per-channel multiplier used in quantized inference.
    \item $A_c$ — per-channel additive term used in quantized inference.
\end{itemize}

In quantized form, $\mu_{\text{moving}}$, $\sigma^2_{\text{moving}}$, $\gamma$, and $\beta$ are fused into per-channel multiplier $M_c$ and additive term $A_c$. Reverse-engineering from the quantized values recovers the original parameters with minimal error:

\begin{equation*}
\begin{gathered}
M_1 = 0.10698594152927399 \cdot (55 + 128) \approx 19.5784 \\
\sqrt{\sigma^2_{\text{moving},1} + \epsilon} = \sqrt{0.00221889466047287 + 0.001} \approx 0.0567353 \\
\hat{\gamma}_1 = M_1 \cdot \sqrt{\sigma^2_{\text{moving},1} + \epsilon} \approx 19.5784 \times 0.0567353 \approx 1.1107881 \\
\hat{\gamma}_1 = 1.1107881 \approx \gamma_1 = 1.110548973083496
\end{gathered}
\end{equation*}


\begin{equation*}
\begin{gathered}
A_1 = 0.0339626707136631 \cdot (-113 + 10) \approx -3.4941551 \\
\beta_1 = A_1 + M_1 \cdot \mu_{\text{moving},1} \approx -3.4941551 + 19.5784 \times 0.1726332 \approx -0.1159326 \\
\beta_1 = -0.1159326 \approx \beta_{1,\text{float}} = -0.11593257635831833
\end{gathered}
\end{equation*}

Finally, the Output affine dequantization formula is derived from the extrema of the Activation No. 1 layer:

\begin{equation*}
\text{scale} = \frac{18.816810607910156 - (-3.696099329375535 \mathrm{e}{-11})}{127 - (-128)} = 0.07379141414881222
\end{equation*}

\begin{equation*}
\text{zero-point} = \text{round}\left( -128 - \frac{-3.696099329375535 \mathrm{e}{-11}}{\text{scale}} \right) = -128
\end{equation*}

All subsequent quantization steps for \glspl{tfop} in the model follow the same general procedure, adhering to predefined constraints and reusing common underlying methods.

By analyzing the results, the following key quantization principles emerge:
\begin{itemize}
    \item After \gls{ptq}, the \code{Quantize Input} layer defines the input scale and zero-point based on observed extrema.
    \item Weights are typically quantized per-axis (per-channel) with symmetric ranges.
    \item Bias scales are derived from input and weight scales, enabling fixed-point computation without extra rescaling.
    \item Certain operations, such as BatchNorm, are decomposed into simpler arithmetic ops for inference efficiency.
    \item The precision loss is usually small, and \gls{lsb} error below $0.5$ indicates a well-matched scale.
\end{itemize}

In the \gls{qat} annotated model (left), the \texttt{QuantizeLayer} performs ``fake'' quantization --- quantizing and then immediately dequantizing tensors --- to simulate quantization effects during training.

After the quantized model is saved, the \gls{edgetpu} Compiler is invoked.

\subsection*{Edge TPU Compiling}

Since training may be performed either on a local Windows machine or on a Linux-based cloud GPU instance from Thunder Compute,
cross-platform automation of the pipeline is ensured by using helper functions and platform detection before invoking the \gls{edgetpu} compiler.
Because the \gls{edgetpu} compiler can only run on Debian-based Linux systems, the Windows Subsystem for Linux (WSL) is used when working on a Windows machine.

\todo{explain in hardware: edgetpu runtime, and maybe moev compiler from design to hardware...}

The \gls{edgetpu} Compiler reads the saved \code{.tflite} model, checks restriction fulfillment and compatibility with \gls{edgetpu} runtime,
before attempting to fuse all \glspl{tfop} into a single \code{edgetpu-custom-op}.
Below an example of console log is illustrated, which \gls{edgetpu} Compiler outputs during the process of compilation.

% Define a new style for Edge TPU compiler logs
\lstdefinestyle{compilerlog}{
  basicstyle=\normalsize\ttfamily, % can adjust \fontsize{12pt}{13.5pt}
  captionpos=b,                    % caption at the bottom
  frame=single,                     % box around the log
  breaklines=true,
  showstringspaces=false,
  columns=fullflexible
}

% Use the style for this specific log
\begin{lstlisting}[style=compilerlog,
    caption={Edge TPU compiler output for the quantized model},
    label={lst:edgetpu_compilation}]
Edge TPU Compiler version 16.0.384591198
Started a compilation timeout timer of 180 seconds.

Model compiled successfully in 898 ms.

Input model: dev/results/run_20250719_170647/quant.tflite
Input size: 1.90MiB
Output model: dev/results/run_20250719_170647/quant_edgetpu.tflite
Output size: 2.07MiB
On-chip memory used for caching model parameters: 1.90MiB
On-chip memory remaining for caching model parameters: 3.82MiB
Off-chip memory used for streaming uncached model parameters: 46.00KiB
Number of Edge TPU subgraphs: 1
Total number of operations: 53
Operation log: dev/results/run_20250719_170647/quant_edgetpu.log
See the operation log file for individual operation details.
Compilation child process completed within timeout period.
Compilation succeeded!
\end{lstlisting}

\todo{check lstlisting in preliminaries and anywhere else and adjust global settings, so I dont need to use custom compilerlog setting here locally}

The \gls{edgetpu} has $\sim8$ MiB of \gls{sram}, allowing it to cache model's parameter data. %\textasciitilde8~ 
It is furthermore specified\footnote{\url{https://coral.ai/docs/edgetpu/compiler/\#parameter-data-caching}},
that multiple models can be co-compiled and stored inside \gls{edgetpu} memory. This allows fast switching and inference, if it is required to use different models on one \gls{edgetpu}.
It is moreover the vice versa methodology possible --- segmenting a model for pipelining on multiple \glspl{edgetpu}. This could allow parallel inference or acceleration of larger models.
In the scope of this thesis, however, only one \gls{edgetpu} used for implementation work. Therefore ideally, but not necessary, the \gls{edgetpu} \gls{sram} has to be not overfilled,
enabling faster inference. The use of external memory can negatively affect speed performance, requiring to reload model's parameters.

From this concrete log example important information about memory usage can be backtraced. The model's executable is taking up approximately 8 - 1.90 - 3.82 = 2.28 MiB of \gls{edgetpu} cache.
While its parameters take 1.90 MiB of cache, leaving 3.82 MiB free, therefore storing an entire model successfully into \gls{edgetpu} \gls{sram}.
In !!!Evaluation!!! by comparing different models their memory usage will be considered too.
For this particular model, the 46.00 KiB of external memory was, however, still utilized, despite the availabiklity of remaining free \gls{edgetpu} \gls{sram} memory.
The reasons behind it could not be found in official documentation, potentionally requiring deeper investigation into compile process, which lies outside the scope of this thesis.

After a successfull compilation the \code{.log} file is saved along with the resulting ready-for-inference on \gls{edgetpu} model.
This log provides additional information about each \gls{tfop} status of attempt to map it to \gls{edgetpu},
as illustrated from a \code{.log} example on the right side of an <ref image>.

One way of confirming the full model compilation on the \gls{edgetpu} is to illustrate the resulting compiled model in Netron.
An example from it of a successfull mapping on every single one \gls{tfop} onto \gls{edgetpu} is illustrated on the left image side <improve this>.
In contrast to the models before compilation, e.g. illustrated on <ref image>, no additional structural information can be obtained from analyzing the graph.
This is a result of fusing all regular \glspl{tfop} into a single \code{edgetpu-custom-op}, deemed only to play on \gls{edgetpu}.
As it can be seen on the image, the input and output tensor shapes can be read off,
as well as additional information about input and output quantization scales and zero-points can be gathered in a similar way as in \secshortref{subsubsec:optquant}.
No more information provided.

\begin{minipage}[t]{0.26\textwidth}\vspace{0pt} % check line 552
    \centering
    % Scale the image to fit half page width while keeping aspect ratio
    \includegraphics[width=\linewidth,keepaspectratio]{files/EdgeTPUCompiledModel.pdf}
    \par\smallskip\footnotesize (a) Model structure diagram. % <-- add this
\end{minipage}%
\hspace{0.5cm plus 1fill} % if I use \hfill the overfull disappears 
\begin{minipage}[t]{0.74\textwidth}\vspace{0pt}
    \vspace*{\fill}        % push down from top
    \raggedright           % left-align text inside this block
    \small

    Edge TPU Compiler version 16.0.384591198\\
    Input: dev/results/run\_20250719\_170647/quant.tflite\\
    Output: dev/results/run\_20250719\_170647/quant\_edgetpu.tflite\\

    \vspace{0.5em}
    \begin{tabular}{@{}lrl@{}}
      Operator & Count & Status \\[0.5em]  % extra space after header row
      CONCATENATION & 3  & Mapped to Edge TPU \\
      LOGISTIC      & 1  & Mapped to Edge TPU \\
      ADD           & 14 & Mapped to Edge TPU \\
      MAX\_POOL\_2D & 3  & Mapped to Edge TPU \\
      MUL           & 14 & Mapped to Edge TPU \\
      CONV\_2D      & 15 & Mapped to Edge TPU \\
      TRANSPOSE\_CONV & 3 & Mapped to Edge TPU
    \end{tabular}

    \par\smallskip\centering\footnotesize (b) Edge TPU compilation summary table. % <-- add this
    \vspace*{\fill}        % push up from bottom -> vertical centering
\end{minipage}

\todo{improve description}

The final result is the \gls{edgetpu}-inference ready \code{.tflite} model file.

\subsection*{Technical Challenges}

The \gls{devboard} is running on a end of life version of Mendel Linux v...  <check and paste link>, which is the only one latest officialy supported software version for \gls{devboard}.
With the OS itself comes already pre-installed runtime library \code{libedgetpu} of version v16.0 released on <verify version and maybe find date>, exposing a low-level C++ \gls{api}
for direct interaction with \gls{edgetpu}. Consequently, this library version is the latest officially available in Coral repository for this Mendel Linux release.
Building the newest one as of today from source requires more complex process using Bazel and potentially breaking dependencies and necessitates debugging,
which lies outside the scope of this thesis.
The newest as of today \gls{edgetpu} Compiler version v16.0 was, however, used,
requiring runtime minimum runtime version v14.0\footnote{\url{https://coral.ai/docs/edgetpu/compiler/\#compiler-and-runtime-versions}},
which was fully compatible with \code{libedgetpu} v16.0.

\todo{explain it inHardware, rename Hardware to Edge Device or something, because explain there Hardware + libedgetpu + Compiler + tflite 2.5 on device}

At a very early development stages, it was, however, not clear, which is the latest supported \gls{tf} version for the provided runtime library and compiler,
as the related information cannot be found in official documents.
Hence, heuristically the newest \gls{tf} version v2.19.0 (at the timepoint this work was conducted) was utilized for the first tries of dataset pipeline building,
model architecting, training and conversion. No \gls{qat} annotation was performed, solely relying on \gls{ptq} before compiling the model for \gls{edgetpu} trial inference.
At a compile time, the following error from \gls{edgetpu} Compiler was encountered:

\begin{lstlisting}
Edge TPU Compiler version 16.0.384591198
ERROR: Didn't find op for builtin opcode 'TRANSPOSE_CONV' version '4'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?
\end{lstlisting}

To avoid this error, the first idea was to completely omit the use of \code{'TRANSPOSE\_CONV'} \gls{tfop} in the model, relying on a more simple \gls{tfop} combination for deconvolution:
\code{'UpSampling2D'} + \code{'Conv2D'}, which will affect the model accuracy, but at least could yield acceptable, especially for edge device inference, results. The other idea, however,
was given a try: the \gls{tf} version was downgraded to v2.5.0 <check if true>, which corresponds to, as mentioned in \secshortref{subsec:hardware},
\gls{tflite} library version on \gls{devboard}. The rationale behind this decision is to try the best version compatibility combination, taking the edge device as starting point.
After the downgrade the first successfull compilation and and inference was performed, with the results presented in \secshortref{sec:evaluation}.

Upon introduction of \gls{qat} annotated model layers, the \gls{tfmot} package has to be installed for it. The version used needs to be compatible with \gls{tf} version,
which at that point was v2.5.0. This necessitated the installation of \gls{tfmot} v0.6.0. At a \gls{tflite} conversion step, during \gls{ptq} the following error keep occuring:

\begin{lstlisting}
error: 'tfl.max_pool_2d' op quantization parameters violate the same scale constraint: !quant.uniform<i8:f32, 840332.86281612806:-128> vs. !quant.uniform<i8:f32, 0.027852464160498452:-127>
\end{lstlisting}

\todo{improve looks}

This error reffers to a \code{restriction: Input and outputs must all have same scale/zero\_point} for \code{MAX\_POOL\_2D} layer from a \gls{tflite} specification table,
mentioned in \secshortref{subsubsec:optquant}. As part of the debugging, it was made sure, that the input and output minimum and maximum value ranges were essentially identical,
with only mean distributions changing, what is normal for differently distributed sets of batches:

\noindent
\begin{minipage}[t]{0.48\textwidth}
\begin{lstlisting}[basicstyle=\ttfamily\small\centering, columns=fullflexible, keepspaces=true, frame=single]
      ---- INPUT TO MAXPOOL ----
      Shape: (1, 384, 384, 16)
      min:   0.0
      max:   3.411252021789551
      mean:  1.003772497177124
\end{lstlisting}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.48\textwidth}
\begin{lstlisting}[basicstyle=\ttfamily\small\centering, columns=fullflexible, keepspaces=true, frame=single]
      ---- OUTPUT OF MAXPOOL ----
      Shape: (1, 192, 192, 16)
      min:   0.0
      max:   3.411252021789551
      mean:  1.0165772438049316
\end{lstlisting}
\end{minipage}


Furthermore, the sanity check of the value ranges of different input batches was performed, confirming expected values ranging from 0 to $\sim5$,
without extremely large or small numbers as outliers, hence providing no reason for big scale values, such as \code{840332.86281612806} as can be gathered from error message.
Activation ranges from the preceding \code{Conv2D} layer were investigated with the help of Netron, showing no conspicuousness as well.
These investigations are suggesting the internal issue of this specific \gls{tfmot} version, as the input and output data are contradicting the error's message scale calculation.

\todo{explain somewhere at the beginning the importance of Technical Challenges Chapter, saying that it is important for practical reproducibility,
but if the reader wants pure implementation logic, theyre optional}

During the debugging process was furthermore identified,
that the similar issue was encountered even on a more later \gls{tfmot} version\footnote{\url{https://github.com/tensorflow/model-optimization/issues/1053}}.
This information combined with own investigations led to the idea of trying newer \gls{tfmot} version, where this bug was already fixed.
Concerns about its compatibility with the older \gls{tf} v2.5.0 were raised, since it was needed for \gls{tfop} v3 compatibility mentioned earlier.
It was decided to prceed, however, due to the absence of documentation of version incompatibility of newer \gls{tfmot} against older \gls{tf}.
The idea was to try multiple version combinations of these modules.

Picking a combination of newer \gls{tfmot} and \gls{tf} versions resulted in resolving the issue, without making architectural changes to the model.
This recursively confirmed the the error as internal in \gls{tf} or \gls{tfmot} implementation code, fixed in later releases.

The successfull conversion to \code{.tflite} model format was subsequently achieved. This process, executed internally by new versions of \gls{tf} and \gls{tfmot},
inserted, however, automatically the \code{Transpose} \gls{tfop} into model architecture. The \code{Transpose} \gls{tfop}, however, was used at no point in model architecting,
indicating automated intertnal optimization of \gls{qat} annotation by \gls{tfmot}. The reasons for this are unclear,
with assumption of necessity to transform at some point of the conversion the layer's outputs between two common shape conventions:
\code{[batch, H, W, C] (NHWC)} and \code{[batch, C, H, W] (NCHW)}. The inability of a certain party, involved in conversion process,
to handle either of these shapes may request the transformation,
which is handled with \code{Transpose} \gls{tfop} inserted before \code{TransposeConv} \gls{tfop}. Below the architecture of the same model is illustrated before (left)
and after \gls{edgetpu} compilation process (right).

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPartQuantEdgeTPUFail.pdf}
    \caption{Links}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    \includegraphics[height=12cm]{files/modelPartCustomEdgeTPUFail.pdf}
    \caption{Rechts}
  \end{subfigure}
  \caption{Zwei SVGs nebeneinander (als PDF)}
\end{figure}

The crucial console output part as well as \code{.log} file are provided, indicating the failed attempt to fuse multiple \glspl{tfop} into aspired \code{edgetpu-custom-op}.

\begin{lstlisting}[
  basicstyle=\ttfamily\small,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  breaklines=false,      % don't wrap—preserves column layout
  frame=single,
  xleftmargin=0pt,
  linewidth=\textwidth,  % fill the page width
  caption={Edge TPU compiler output for the quantized model},
  label={lst:edgetpu_compilation1},
  captionpos=b
]

Model successfully compiled but not all operations are supported
by the Edge TPU. A percentage of the model will instead run on the CPU,
which is slower. If possible, consider updating your model to use only
operations supported by the Edge TPU.
For details, visit g.co/coral/model-reqs.
Number of operations that will run on Edge TPU: 4
Number of operations that will run on CPU: 7
See the operation log file for individual operation details.

Logfile:
Edge TPU Compiler version 16.0.384591198
Input: quant.tflite
Output: quant_edgetpu.tflite

Operator             Count         Status
      
CONV_2D              2             More than one subgraph is not supported
CONV_2D              2             Mapped to Edge TPU
TRANSPOSE_CONV       1             Filter, bias, or other param is not
                                   constant at compile-time
LOGISTIC             1             More than one subgraph is not supported
ADD                  1             More than one subgraph is not supported
MAX_POOL_2D          1             Mapped to Edge TPU
CONCATENATION        1             More than one subgraph is not supported
TRANSPOSE            1             Tensor has unsupported rank
                                   (up to 3 innermost dimensions mapped)
QUANTIZE             1             Mapped to Edge TPU
\end{lstlisting}

\todo{maybe wrap it better, add or remove some info, ensure consistent style across all work}

This behaviour confirms the inability of multiple model's layers to meet the requirements defined in \secshortref{subsec:hardware},
as well as confirms the procedure of compilation process in official documentation\footnote{\url{https://coral.ai/docs/edgetpu/models-intro/\#compiling}}.
That states that the compiler will stop at a first unsopported by \gls{edgetpu} operation and delegate all subsequent operations for CPU execution,
even if after unsopported \gls{tfop} come the supported for \gls{edgetpu} execution operations. This behaviour is represented on the image below.

\begin{figure}[htbp]
  \centering
  % Choose either a width OR a height, or both with keepaspectratio.
  % Example A: set width only (aspect ratio preserved automatically)
  \includegraphics[width=0.7\linewidth]{files/compileSequenceEdgeTPU.png}

  % Example B (recommended): limit both max width & height, keep aspect ratio
  %\includegraphics[
  %  width=0.9\linewidth,
  %  height=0.5\textheight,
  %  keepaspectratio
  %]{images/example.png}

  \caption{Descriptive caption for the image (what the figure shows).}
  \label{fig:compileSeqEdgeTPU}
\end{figure}

The delegation of tensor operations to CPU will almost certainly drastically increase the inference time,
which is inappropriate in the scope of the work, aspiring to map the entire model on \gls{edgetpu}.

As the process of \code{Transpose} \gls{tfop} insertion is happening completely internally and automatically,
there is no possibility to affect this or to explicitly set the layer's parameters to be constant at compile-time and of shape <3, as required by \gls{edgetpu} Compiler.

There are again two ideas could be realised. Avoid the use of \code{TransposeConv} or to continue the search for compatible \gls{tf} and \gls{tfmot} versions.
By choosing the second idea, the one is caught between a rock and a hard place. On the one hand the \gls{tf} version has to be kept as low as possible for the best compatibility with
\gls{tflite} on the edge device and to avoid encountering the \code{TRANSPOSE\_CONV version 4} compiler error, described earlier. On the other hand the version of \gls{tf} and \gls{tfmot}
has to be high enough to contain the fixed bug of scale incompatibility, encountered earlier. With this in mind, the following approach was chosen: find the highest \gls{tf} version,
which is still utilizing the v3 of \code{TransposeConv} \gls{tfop}. This approach, however is highly relying on the assumption, that all other newer \glspl{tfop},
probably introduced in newer \gls{tf} version, will not encounter compiling problems, as well as be compatible with the older \gls{tflite} v2.5.0 on an inference device.
Because essentially, if the \gls{edgetpu} Compiler successfully fuses every \gls{tfop} into \code{edgetpu-custom-op},
the \gls{tflite} runtime v2.5.0 is required to only be able to parse and load the model until it is delegated over to \gls{edgetpu}.
So in this case, it is more relied on the compiler as the last instance to check the general compatibility before running inference.
Hence the certain compatibility unsureness is still existing, which presence has to be considered when using new (possibly custom) layer types.
But in the scope of this work to achieve the objective this assumption is deemed as sufficient.

Given the absence of the official documentation about such detailed information of \glspl{tfop},
the search for the compatible \code{TransposeConv} \gls{tfop} was only possible by investigating the contents of \code{register.cc} file,
located in \gls{tf} repository in the folder \code{tensorflow/tensorflow/lite/kernels}. It is important to mention,
that in different \gls{tf} versions this file can be present in a slightly different folders, but in a close vicinity to the provided path.
The newest \gls{tf} version with a \code{TransposeConv} v3 is found out to be v2.10.0. The corresponding lines of \code{register.cc} are:

\begin{lstlisting}
AddBuiltin(BuiltinOperator_TRANSPOSE_CONV, Register_TRANSPOSE_CONV(),
           /* min_version = */ 1,
           /* max_version = */ 3);
\end{lstlisting}

It turned out, that the newest \gls{tfmot} v0.8.0 is backward compatible with \gls{tf} v2.10.0, despite being tested against \gls{tf} 2.14.1,
as can be confirmed on official releases page\footnote{\url{https://github.com/tensorflow/model-optimization/releases}}.

The problem of such version incompatibilities is presumably stemming from introducing the newer version of \code{libedgetpu} runtime library,
while keeping the old \gls{tflite} version on a \gls{devboard}, without providing an official possibility to upgrade and even out the versions.
This, as proven here, could be misleading and confusing when the one faced with the choice of inter-compatibility of multiple required packages.
Especially when newer improved versions of some of these packages have become available in the meantime.

At the end, the version combination \gls{tf} v2.10.0 and \gls{tfmot} v0.8.0 resolved all previous errors and led to successfull compilation of model,
fully mapping all \glspl{tfop} to \gls{edgetpu}, as it is presented in <ref image here!>.
This setup was subsequently used later on during the full process of the pipeline utilization in the scope of this work.

\todo{requirements mention somewhere}

\section{Deployment}
\label{sec:deployment}

After training, conversion and compilation processes the \code{.tflite} model is deployed on \gls{edgetpu} to run inference.

At early development stage the most convenient deployment way with the help of python utilities was applied.
Provided by Google these libraries wrap all necessary boilerplate into a small amount of functions with a purpose to conveniently load the model into \gls{edgetpu},
pass the input tensor to it and collect the output tensor. A simplified example of full inference in python code is presented below for understanding the simplicity of
operating \gls{edgetpu} with provided heplers.

%\begin{minted}{python}
%from pycoral.utils.edgetpu import make_interpreter
%from pycoral.adapters import common
%
%# Load model
%interpreter = make_interpreter(model_path)
%interpreter.allocate_tensors()
%
%# Prepare input
%common.set_input(interpreter, input_tensor)
%
%# Inference
%interpreter.invoke()
%
%# Output
%output = common.output_tensor(interpreter, 0)
%\end{minted}

\todo{ENABLE Minted}

Utilizing the provided python modules the first inference results were obtained, they are presented along with the corresponding ground truth on the image below.

\begin{figure}[htbp]
    \centering
    % First image
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[height=5cm]{files/FIRSTInferenceResult.png}
        \caption{First image}
    \end{subfigure}
    \hfill
    % Second image
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[height=5cm]{files/FIRSTInferenceResultGT.png}
        \caption{Second image}
    \end{subfigure}
    \caption{Two images side by side}
    \label{fig:two-images}
\end{figure}

This single patch inference initial results became a milestone of this work and laid the foundation for the subsequent inference pipeline.

To simplify the operation of \gls{edgetpu} hardware so much, developers are providing two python libraries\footnote{\url{https://coral.ai/docs/edgetpu/tflite-python/}}:
PyCoral \gls{api} and \gls{tflite} \gls{api}, where PyCoral wraps the \gls{tflite} repetitive functions needed for inference into even simplier ones, as shown on the image above <REF>.

After successfull implementation of a single patch inference in python, it was decided to expand it into the full scene inference pipeline using C++.
The main rationale behind it lies in the extended control over inference process and its potential acceleration.
Furthermore python overhead functions to, for instance, read the \code{.TIF} images directly,
transform them and pass to \gls{edgetpu} in one go yields on one side teh convenience of implementing the complete pipeline inside one script.
On the other side, however, may lead to slowed process unwanted on embedded system inference. This logic applies to other necessary operations,
that potentially had to be performed in python. More common approach for embedded systems has to be utilized, transforming the input data
potentionally on other device, perform inference on edge device, transform output data on other device.
Additionall factor that led to implementation of C++ inference was the specific error,
encountered at the end of each and every Python inference. It was believed at the time,
that more control over inference sequence with the help of C++ functions will shed light on and help to get rid of that error.
More specific about it in \secshortref{subsec:tecchallDeployment}.

The \gls{devboard} Mendel Linux OS contains from installation almost every library needed for operating its \gls{edgetpu} module.
That holds true for Python libraries, such as PyCoral and \gls{tflite} \glspl{api}, as well as for hardware runtime library \code{libedgetpu}.
The essential \gls{tflite} C++ \gls{api} library is not included, though.
It is furthermore impossible to cross use the \gls{tflite} \gls{api} for Python by C++,
since the binary is compiled specifically for Python and in fact lacks the header files for C++.
On a chart below the general flow of running the model on \gls{edgetpu} on \gls{devboard} is represented,
highlighting the difference between inference implementation in Python and C++.

\begin{figure}[H]
\centering
\resizebox{\linewidth}{!}{%
\begin{tikzpicture}[
  >=Stealth, ->,
  box/.style={rectangle,draw,rounded corners,
              minimum width=1cm,minimum height=1cm,
              align=center,font=\normalsize}
]

% Shared start
\node[box] (model) {.tflite\\FlatBuffer Model};

% API branches
\node[box, right=0.5cm of model, yshift=2cm] (pyapi) {PyCoral \gls{api} (optional) or\\\gls{tflite} Python \gls{api}};
\node[box, right=0.5cm of model, yshift=-2cm] (cppapi) {\gls{tflite} C++ \gls{api}\\(headers \& library)};

% Convergence
\node[box, right=4cm of model] (interp) {\gls{tflite} Interpreter\\(C++ core)};

% Runtime + hardware
\node[box, right=1cm of interp] (runtime) {\gls{edgetpu} runtime driver\\\texttt{libedgetpu.so}};
\node[box, right=1cm of runtime] (hw) {\gls{edgetpu}\\Hardware Accelerator};

% Connections
\draw (model) -- (pyapi);
\draw (model) -- (cppapi);
\draw (pyapi) -- (interp);
\draw (cppapi) -- (interp);
\draw (interp) -- (runtime) -- (hw);

\end{tikzpicture}%
}
\caption{ADJUST}
\end{figure}

Since no pre-build \gls{tflite} C++ \gls{api} libraries can be found for \gls{devboard}, it had to be build from source.
The process is described in detail in \secshortref{subsec:tecchallDeployment}
and marks the third and the last technical implementation challenge milestone in the scope of this work.

After successfully incorporating the \gls{tflite} C++ \gls{api} library on \gls{devboard},
it was utilized to implement the first C++ inference inside the \code{inference.cpp} file. Following the success,
the folder reading feature was implemented on C++ to process multiple patches in a row.
These patches had to be stored as \code{.bin} files with known shape and type (\gls{float32}) of the data inside.
After inference the program writes output \code{.bin} files with predicted cloud masks in \gls{float32} format.
Quantization parameters of input and output are retrieved from model during the inference and used for quantized transformation of input and dequantization of the output tensor.
Later on, the stitching process was C++ implemented too.
By running the \code{stitch.cpp} file all \code{.bin} patches inside the input folder are gapless stitched into a single \code{.bim} scene.
This \code{.bin} scene can be then converted to \code{.png} file for visual inspection.
It is implemented to optionally be performed on \gls{devboard} with the help of Python script \code{convert.py}.
\todo{check all filenames}
It has to be mentioned, that for the sake of convenience the whole inference, stitching and conversion process could have been implemented as a single C++ executable.
It was kept, however, separate due to two reasons. Firstly allowing more fexibility and less overhead for later potentially performance critical implementation on the satellite.
Secondly, the persisting error, described in \secshortref{subsec:tecchallDeployment}, is not allowing to run any code immediately after inference process is done.

As final steps, concluding the inference implementation on \gls{devboard} were following improvements.
The SDCard with prepared \code{.bin} patches for each scene on it, is mounted on a \gls{devboard}. This allows to host the entire dataset of several GBs of size,
which would be impossible to do an a \gls{devboard}.
The stitching and conversion processes were unified under a single executable, with Pyhton function call for conversion after C++ \code{.bin} stitching.
During both inference and stitch-conversion pracesses the user is promted to pick one out of 20 test scenes for inference and/or conversion.
Furthermore, the Python process of conversion to \code{.png} is improved by eliminating the big output array being loaded in \gls{devboard}'s RAM memory,
by handling it in chunks, therefore enabling to convert bigger full resolution scenes.

The final version of inference, stitching and conversion implementation on a \gls{devboard} can be found in \code{git} under the tag \code{Deployment\_v1.0}.

\subsection*{Technical Challenges}
\label{subsec:tecchallDeployment}

The last significant implementation milestone in the scope of this work was to build the \gls{tflite} C++ \gls{api} library for the \gls{devboard} from source.
As can be seen on the chart <REF>, this library is crucial for implementation of pure C++ inference.

A guidance to carry out this task is very sparsely mentioned in the official documentation\footnote{\url{https://coral.ai/docs/edgetpu/tflite-cpp/\#build-your-project-with-libedgetpu}}.
Taking these recommendations into account it was clear, that the compatible version of \gls{tf} to a pre-installed on \gls{devboard} \code{libedgetpu} library had to be determined.
The version of installed \code{libedgetpu} was determined quickly as v16.0. But on the official releases website\footnote{\url{https://github.com/google-coral/libedgetpu/releases}},
can be found no mention of numerical verison convention, so the literal date when the symlink \code{libedgetpu.so.1} was modified was used to approximately determine the release version.
On the particular \gls{devboard} used in this work it was the 09.07.2021,
predating the official "Grouper" \code{libedgetpu} release\footnote{\url{https://github.com/google-coral/libedgetpu/tree/release-grouper}} on 26.07.2021 by two weeks.
Hence from the \code{workspace.bzl} of this exact release the exact \gls{tf} commit corresponding to the \gls{tf} release 2.5.0 as of 05.17.2021 was determined.

First build try was performed on a Windows machine, cross-compiling for \code{aarch64} Linux. After successfull compilation the wall was hit during the first inference test,
where it was run into low level incompatibility with \gls{devboard}'s atomic instructions. It became clear, that the library has to be build either by using Bazel with predefinition
of full \gls{devboard} configuration or directly on the \gls{devboard}. It was made a decision to stick with the second option. In order to do it, the SDCard was used to host large
\gls{tf} folder (of the specific v2.5.0 commit) and additionally provide RAM swap space, due to concerns,
that \gls{devboard} will run out of RAM memory during compilation process. It is important to mention,
that building was performed using the latest by the time of the work portable version of \code{CMake}. 

After successfully building the static \code{libtensorflow-lite.a} library, several additional necessary steps had to be performed and files gathered.
The unavoidable \code{edgetpu.h} header has to be downloaded\footnote{\url{https://github.com/google-coral/libedgetpu/blob/master/tflite/public/edgetpu.h}},
together with a compatible with the given build FlatBuffers library v1.12.0. Additionally every \gls{tflite} required static library had to be included.
The necessary include files are provided in git and can also be investigated in \code{MAKEFILE}.

After all compilator and linker issues were resolved and requirements satisfied, the successfull model loading and tensor allocation using C++ \gls{api}
marked the last technical milestone of this work.

\section{Evaluation}
\label{sec:evaluation}



\todo{Smooth transition to evaluation chapter! Very nice!}

}